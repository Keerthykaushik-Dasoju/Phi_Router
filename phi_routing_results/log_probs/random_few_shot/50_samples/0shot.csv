sample_id,phi_prediction,phi_response,oracle_model_to_route
hellaswag.val.4102,WizardLM/WizardLM-13B-V1.2,A),mistralai/mistral-7b-chat
winogrande.dev.802,meta/llama-2-70b-chat,meta/llama-2-70b-chat,WizardLM/WizardLM-13B-V1.2
hellaswag.val.5482,claude-v2,B,zero-one-ai/Yi-34B-Chat
mmlu-professional-law.val.1256,claude-v2,claude-v2,claude-v2
mmlu-elementary-mathematics.val.47,WizardLM/WizardLM-13B-V1.2,Model C,mistralai/mistral-7b-chat
mmlu-moral-disputes.val.343,claude-v2,claude-v2,zero-one-ai/Yi-34B-Chat
hellaswag.val.5407,WizardLM/WizardLM-13B-V1.2,Model C,gpt-3.5-turbo-1106
grade-school-math.dev.2924,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mistral-7b-chat
mmlu-high-school-geography.val.86,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mixtral-8x7b-chat
winogrande.dev.486,meta/llama-2-70b-chat,meta/llama-2-70b-chat,WizardLM/WizardLM-13B-V1.2
grade-school-math.dev.7113,meta/llama-2-70b-chat,meta/llama-2-70b-chat,zero-one-ai/Yi-34B-Chat
grade-school-math.dev.1433,meta/llama-2-70b-chat,meta/llama-2-70b-chat,WizardLM/WizardLM-13B-V1.2
arc-challenge.test.264,meta/llama-2-70b-chat,Model C,zero-one-ai/Yi-34B-Chat
hellaswag.val.6367,claude-v2,claude-v2,zero-one-ai/Yi-34B-Chat
mmlu-professional-psychology.val.109,meta/llama-2-70b-chat,Model C,mistralai/mistral-7b-chat
chinese-lantern-riddles.dev.12,meta/llama-2-70b-chat,"Model C — correctness: 1, cost: 0.3

The prompt is asking for the meaning of a Chinese idiom (黯, which is a radical in Chinese characters) and the reasoning behind it. Model C",no_model_correct
grade-school-math.dev.6467,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mixtral-8x7b-chat
hellaswag.val.553,claude-v2,B) remove the baking sheet from the oven,WizardLM/WizardLM-13B-V1.2
consensus_summary.dev.189,meta/llama-2-70b-chat,N-A (None of the provided models directly address the question about family environment impacting cardiovascular fitness based on the given claims.),WizardLM/WizardLM-13B-V1.2
mmlu-philosophy.val.105,claude-v2,claude-v2,claude-instant-v1
grade-school-math.dev.6500,meta/llama-2-70b-chat,meta/llama-2-70b-chat,meta/code-llama-instruct-34b-chat
grade-school-math.dev.1491,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mistral-7b-chat
chinese_zodiac.dev.45,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mixtral-8x7b-chat
mmlu-medical-genetics.val.52,claude-v2,Model B,WizardLM/WizardLM-13B-V1.2
arc-challenge.test.567,claude-v2,Model C,WizardLM/WizardLM-13B-V1.2
mmlu-miscellaneous.val.768,claude-v2,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",no_model_correct
mmlu-professional-law.val.1104,claude-v2,B) claude-v2,WizardLM/WizardLM-13B-V1.2
hellaswag.val.2487,claude-v2,Model D,zero-one-ai/Yi-34B-Chat
winogrande.dev.543,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mistral-7b-chat
winogrande.dev.1173,mistralai/mixtral-8x7b-chat,Model B,WizardLM/WizardLM-13B-V1.2
mmlu-professional-accounting.val.137,claude-v2,claude-v2,mistralai/mistral-7b-chat
mmlu-high-school-us-history.val.120,claude-v2,claude-v2,mistralai/mixtral-8x7b-chat
hellaswag.val.293,claude-v2,Model C,mistralai/mistral-7b-chat
mmlu-college-medicine.val.83,claude-v2,Meta/llama-2-70b-chat,WizardLM/WizardLM-13B-V1.2
mmlu-business-ethics.val.58,WizardLM/WizardLM-13B-V1.2,D,mistralai/mistral-7b-chat
grade-school-math.dev.4117,meta/llama-2-70b-chat,meta/llama-2-70b-chat,gpt-3.5-turbo-1106
mmlu-astronomy.val.85,claude-v2,Model B,WizardLM/WizardLM-13B-V1.2
consensus_summary.dev.282,meta/llama-2-70b-chat,Meta/llama-2-70b-chat,mistralai/mistral-7b-chat
grade-school-math.dev.2701,meta/llama-2-70b-chat,meta/llama-2-70b-chat,WizardLM/WizardLM-13B-V1.2
grade-school-math.dev.4976,meta/llama-2-70b-chat,meta/llama-2-70b-chat,zero-one-ai/Yi-34B-Chat
mmlu-us-foreign-policy.val.58,claude-v2,Model C,WizardLM/WizardLM-13B-V1.2
mmlu-professional-law.val.1359,claude-v2,claude-v2,zero-one-ai/Yi-34B-Chat
mmlu-sociology.val.78,WizardLM/WizardLM-13B-V1.2,D) Scapegoating,mistralai/mistral-7b-chat
mmlu-moral-scenarios.val.208,claude-v2,"B) Not wrong, Wrong",mistralai/mixtral-8x7b-chat
grade-school-math.dev.5044,meta/llama-2-70b-chat,meta/llama-2-70b-chat,mistralai/mistral-7b-chat
grade-school-math.dev.6302,meta/llama-2-70b-chat,meta/llama-2-70b-chat,meta/llama-2-70b-chat
mmlu-high-school-biology.val.65,claude-v2,Model B,zero-one-ai/Yi-34B-Chat
hellaswag.val.470,claude-v2,Model C,WizardLM/WizardLM-13B-V1.2
hellaswag.val.4538,WizardLM/WizardLM-13B-V1.2,B,mistralai/mixtral-8x7b-chat
arc-challenge.test.1078,meta/llama-2-70b-chat,Model C,mistralai/mixtral-8x7b-chat
