sample_id,phi_prediction,phi_correctness,phi_cost,WizardLM/WizardLM-13B-V1.2_correctness,WizardLM/WizardLM-13B-V1.2_cost,claude-instant-v1_correctness,claude-instant-v1_cost,claude-v1_correctness,claude-v1_cost,claude-v2_correctness,claude-v2_cost,gpt-3.5-turbo-1106_correctness,gpt-3.5-turbo-1106_cost,gpt-4-1106-preview_correctness,gpt-4-1106-preview_cost,meta/code-llama-instruct-34b-chat_correctness,meta/code-llama-instruct-34b-chat_cost,meta/llama-2-70b-chat_correctness,meta/llama-2-70b-chat_cost,mistralai/mistral-7b-chat_correctness,mistralai/mistral-7b-chat_cost,mistralai/mixtral-8x7b-chat_correctness,mistralai/mixtral-8x7b-chat_cost,zero-one-ai/Yi-34B-Chat_correctness,zero-one-ai/Yi-34B-Chat_cost
hellaswag.val.4102,WizardLM/WizardLM-13B-V1.2,1.0,8.13e-05,1.0,8.13e-05,0.0,0.0002176,1.0,0.002176,1.0,0.002176,0.0,0.0002729999999999,1.0,0.00275,0.0,0.0002102959999999,1.0,0.0002439,1.0,5.420000000000001e-05,1.0,0.0001626,1.0,0.000216
winogrande.dev.802,WizardLM/WizardLM-13B-V1.2,1.0,1.4699999999999998e-05,1.0,1.4699999999999998e-05,0.0,4.24e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,0.0,5.1e-05,1.0,0.0005,0.0,3.7248e-05,1.0,4.410000000000001e-05,0.0,9.8e-06,0.0,2.94e-05,1.0,3.84e-05
hellaswag.val.5482,WizardLM/WizardLM-13B-V1.2,0.0,6.45e-05,0.0,6.45e-05,1.0,0.0001728,1.0,0.001728,0.0,0.001752,1.0,0.000217,1.0,0.00216,0.0,0.00016684,0.0,0.0001935,0.0,4.3e-05,0.0,0.000129,1.0,0.0001712
mmlu-professional-law.val.1256,WizardLM/WizardLM-13B-V1.2,0.0,8.34e-05,0.0,8.34e-05,0.0,0.0002232,0.0,0.002232,1.0,0.002232,0.0,0.000278,0.0,0.00279,0.0,0.000215728,0.0,0.0002502,0.0,5.56e-05,0.0,0.0001668,0.0,0.0002224
mmlu-elementary-mathematics.val.47,WizardLM/WizardLM-13B-V1.2,1.0,2.07e-05,1.0,2.07e-05,0.0,5.6e-05,0.0,0.00056,0.0,0.00056,0.0,7.1e-05,1.0,0.0007,0.0,5.354400000000001e-05,0.0,6.21e-05,1.0,1.38e-05,0.0,4.14e-05,0.0,5.52e-05
mmlu-moral-disputes.val.343,WizardLM/WizardLM-13B-V1.2,0.0,3.9e-05,0.0,3.9e-05,1.0,0.0001048,1.0,0.0010479999999999,1.0,0.0010479999999999,1.0,0.00013,1.0,0.00131,0.0,0.00010088,0.0,0.000117,0.0,2.6e-05,0.0,7.8e-05,1.0,0.0001032
hellaswag.val.5407,WizardLM/WizardLM-13B-V1.2,0.0,9.3e-05,0.0,9.3e-05,0.0,0.000252,0.0,0.002496,0.0,0.002496,1.0,0.0003109999999999,1.0,0.00312,0.0,0.000241336,0.0,0.0002799,0.0,6.22e-05,0.0,0.0001866,0.0,0.000248
grade-school-math.dev.2924,WizardLM/WizardLM-13B-V1.2,0.75,0.0001452,0.75,0.0001452,0.75,0.0004696,0.75,0.004312,0.75,0.005056,0.75,0.000448,0.75,0.00839,0.25,0.00030652,0.75,0.0003132,0.75,8.26e-05,0.75,0.0002352,0.75,0.0003352
mmlu-high-school-geography.val.86,WizardLM/WizardLM-13B-V1.2,0.0,3.06e-05,0.0,3.06e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000105,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
winogrande.dev.486,WizardLM/WizardLM-13B-V1.2,1.0,1.4399999999999998e-05,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,1.0,0.000392,0.0,5e-05,1.0,0.00052,0.0,3.7248e-05,1.0,4.32e-05,0.0,9.6e-06,0.0,2.8799999999999995e-05,0.0,3.7600000000000006e-05
grade-school-math.dev.7113,WizardLM/WizardLM-13B-V1.2,0.25,0.0001841999999999,0.25,0.0001841999999999,0.25,0.000876,0.75,0.00552,0.75,0.006744,0.5,0.000543,0.5,0.00927,0.25,0.0003298,0.25,0.0006156,0.25,0.00013,0.25,0.000303,0.75,0.0005552
grade-school-math.dev.1433,WizardLM/WizardLM-13B-V1.2,0.75,0.0001695,0.75,0.0001695,0.75,0.0005463999999999,0.75,0.005704,0.75,0.0061119999999999,0.75,0.000539,0.5,0.00671,0.25,0.000227368,0.75,0.0003528,0.25,8.460000000000001e-05,0.25,0.0002718,0.75,0.0004
arc-challenge.test.264,WizardLM/WizardLM-13B-V1.2,0.0,2.4e-05,0.0,2.4e-05,0.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,8.2e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,0.0,4.8e-05,1.0,6.320000000000002e-05
hellaswag.val.6367,WizardLM/WizardLM-13B-V1.2,0.0,7.83e-05,0.0,7.83e-05,0.0,0.0002096,0.0,0.002096,0.0,0.002096,0.0,0.000263,0.0,0.00265,0.0,0.000202536,0.0,0.000234,0.0,5.220000000000001e-05,0.0,0.0001566,1.0,0.000208
mmlu-professional-psychology.val.109,WizardLM/WizardLM-13B-V1.2,0.0,3.15e-05,0.0,3.15e-05,0.0,8.480000000000001e-05,0.0,0.000848,0.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,1.0,2.1e-05,1.0,6.3e-05,0.0,8.400000000000001e-05
chinese-lantern-riddles.dev.12,WizardLM/WizardLM-13B-V1.2,0.0,7.649999999999999e-05,0.0,7.649999999999999e-05,0.0,0.0003799999999999,0.0,0.006296,0.0,0.004376,0.0,0.000236,0.0,0.0103,0.0,0.000180032,0.0,0.0002466,0.0,3.86e-05,0.0,0.0001968,0.0,0.0002
grade-school-math.dev.6467,WizardLM/WizardLM-13B-V1.2,0.25,0.0001779,0.25,0.0001779,0.75,0.0005415999999999,0.75,0.004504,0.25,0.007192,0.5,0.000456,0.75,0.00962,0.25,0.000396536,0.25,0.0003798,0.25,9.26e-05,0.75,0.0002076,0.25,0.0004384
hellaswag.val.553,WizardLM/WizardLM-13B-V1.2,1.0,3.15e-05,1.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.000107,1.0,0.00109,0.0,8.148e-05,1.0,9.45e-05,0.0,2.1e-05,0.0,6.3e-05,1.0,8.320000000000002e-05
consensus_summary.dev.189,meta/llama-2-70b-chat,0.75,0.0002277,1.0,5.58e-05,0.5,0.0001456,0.5,0.001456,0.75,0.002608,0.5,0.0001799999999999,0.5,0.00182,0.75,0.000150544,0.75,0.0002277,0.75,3.88e-05,0.75,0.0001193999999999,0.5,0.0001424
mmlu-philosophy.val.105,WizardLM/WizardLM-13B-V1.2,0.0,3.3e-05,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,0.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,0.0,8.8e-05
grade-school-math.dev.6500,WizardLM/WizardLM-13B-V1.2,0.5,0.0001250999999999,0.5,0.0001250999999999,0.75,0.0004776,0.75,0.00384,0.5,0.005448,0.5,0.000399,0.5,0.00639,0.75,0.000266944,0.5,0.0003024,0.25,0.0001028,0.5,0.0002243999999999,0.75,0.000376
grade-school-math.dev.1491,WizardLM/WizardLM-13B-V1.2,0.75,0.0001308,0.75,0.0001308,0.75,0.0003488,0.75,0.003752,0.75,0.004472,0.75,0.000376,0.75,0.0066999999999999,0.75,0.000242112,0.75,0.0003195,0.75,8.560000000000001e-05,0.75,0.0002112,0.75,0.00038
chinese_zodiac.dev.45,WizardLM/WizardLM-13B-V1.2,0.0,3.33e-05,0.0,3.33e-05,0.0,9.2e-05,1.0,0.00092,0.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.00010864,0.0,0.0001008,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,8.88e-05
mmlu-medical-genetics.val.52,WizardLM/WizardLM-13B-V1.2,1.0,3.24e-05,1.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
arc-challenge.test.567,WizardLM/WizardLM-13B-V1.2,1.0,2.52e-05,1.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.6e-05,1.0,0.00085,0.0,6.5184e-05,1.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
mmlu-miscellaneous.val.768,WizardLM/WizardLM-13B-V1.2,0.0,2.4e-05,0.0,2.4e-05,0.0,6.480000000000002e-05,0.0,0.000648,0.0,0.000648,0.0,7.999999999999999e-05,0.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,0.0,4.8e-05,0.0,6.400000000000001e-05
mmlu-professional-law.val.1104,WizardLM/WizardLM-13B-V1.2,1.0,6.81e-05,1.0,6.81e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.0018239999999999,1.0,0.000227,1.0,0.00228,0.0,0.0001761519999999,0.0,0.0002042999999999,0.0,4.5400000000000006e-05,1.0,0.0001362,0.0,0.0001816
hellaswag.val.2487,WizardLM/WizardLM-13B-V1.2,0.0,2.97e-05,0.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,1.0,8.91e-05,0.0,1.98e-05,0.0,5.94e-05,1.0,7.840000000000001e-05
winogrande.dev.543,WizardLM/WizardLM-13B-V1.2,0.0,1.5e-05,0.0,1.5e-05,0.0,4.08e-05,0.0,0.000408,0.0,0.000408,1.0,5.2e-05,0.0,0.00054,1.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,1.0,3e-05,0.0,3.92e-05
winogrande.dev.1173,WizardLM/WizardLM-13B-V1.2,1.0,1.59e-05,1.0,1.59e-05,0.0,4.32e-05,0.0,0.000432,1.0,0.000432,1.0,5.5e-05,1.0,0.00057,0.0,4.1128e-05,1.0,4.77e-05,0.0,1.06e-05,0.0,3.18e-05,0.0,4.24e-05
mmlu-professional-accounting.val.137,WizardLM/WizardLM-13B-V1.2,0.0,2.94e-05,0.0,2.94e-05,0.0,7.920000000000001e-05,0.0,0.000792,0.0,0.000792,0.0,9.8e-05,1.0,0.00102,0.0,7.604800000000001e-05,0.0,8.82e-05,1.0,1.96e-05,1.0,5.88e-05,0.0,7.760000000000002e-05
mmlu-high-school-us-history.val.120,WizardLM/WizardLM-13B-V1.2,0.0,0.0001137,0.0,0.0001137,0.0,0.000304,1.0,0.00304,1.0,0.00304,1.0,0.000379,1.0,0.0038,0.0,0.000294104,0.0,0.0003411,0.0,7.58e-05,1.0,0.0002274,1.0,0.0003032
hellaswag.val.293,WizardLM/WizardLM-13B-V1.2,0.0,3.63e-05,0.0,3.63e-05,0.0,9.76e-05,0.0,0.000976,0.0,0.000976,0.0,0.000121,0.0,0.00122,1.0,9.3896e-05,0.0,0.000108,1.0,2.42e-05,0.0,7.259999999999999e-05,0.0,9.6e-05
mmlu-college-medicine.val.83,WizardLM/WizardLM-13B-V1.2,1.0,2.85e-05,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-business-ethics.val.58,WizardLM/WizardLM-13B-V1.2,1.0,5.22e-05,1.0,5.22e-05,0.0,0.00014,0.0,0.0014,0.0,0.0014,0.0,0.000174,1.0,0.00178,0.0,0.000135024,0.0,0.0001566,1.0,3.48e-05,0.0,0.0001044,1.0,0.0001392
grade-school-math.dev.4117,WizardLM/WizardLM-13B-V1.2,0.25,0.0001574999999999,0.25,0.0001574999999999,0.75,0.0005935999999999,0.25,0.006704,0.25,0.00548,0.75,0.000299,0.75,0.01261,0.25,0.000304968,0.25,0.0003015,0.25,9.1e-05,0.25,0.0002718,0.25,0.0003224
mmlu-astronomy.val.85,WizardLM/WizardLM-13B-V1.2,1.0,4.26e-05,1.0,4.26e-05,1.0,0.0001144,1.0,0.0011439999999999,1.0,0.0011439999999999,1.0,0.0001419999999999,1.0,0.00143,0.0,0.000110192,0.0,0.0001278,0.0,2.84e-05,1.0,8.52e-05,1.0,0.0001136
consensus_summary.dev.282,WizardLM/WizardLM-13B-V1.2,0.75,0.0001215,0.75,0.0001215,0.5,0.0006256,0.25,0.001816,0.25,0.004408,0.75,0.000403,0.75,0.00524,0.75,0.000285568,0.75,0.000432,0.75,6.48e-05,0.75,0.0001812,0.75,0.0002424
grade-school-math.dev.2701,WizardLM/WizardLM-13B-V1.2,0.75,0.0001977,0.75,0.0001977,0.25,0.000736,0.5,0.005104,0.75,0.005464,0.25,0.00068,0.75,0.00761,0.25,0.000369376,0.25,0.0003789,0.25,0.0001112,0.75,0.000267,0.25,0.0004576
grade-school-math.dev.4976,WizardLM/WizardLM-13B-V1.2,0.25,0.000156,0.25,0.000156,0.75,0.0005304,0.25,0.006504,0.5,0.008424,0.75,0.000561,0.75,0.00996,0.25,0.000340664,0.25,0.0003528,0.25,9.26e-05,0.25,0.0002741999999999,0.75,0.0004696
mmlu-us-foreign-policy.val.58,WizardLM/WizardLM-13B-V1.2,1.0,2.7e-05,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,0.0,7.120000000000001e-05
mmlu-professional-law.val.1359,WizardLM/WizardLM-13B-V1.2,0.0,7.649999999999999e-05,0.0,7.649999999999999e-05,1.0,0.0002048,1.0,0.002048,1.0,0.002048,1.0,0.000255,1.0,0.00256,0.0,0.00019788,0.0,0.0002295,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-sociology.val.78,gpt-4-1106-preview,1.0,0.0008799999999999,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-moral-scenarios.val.208,WizardLM/WizardLM-13B-V1.2,0.0,4.2e-05,0.0,4.2e-05,1.0,0.0001128,1.0,0.0011279999999999,1.0,0.0011279999999999,0.0,0.00014,1.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,1.0,8.4e-05,1.0,0.0001112
grade-school-math.dev.5044,WizardLM/WizardLM-13B-V1.2,0.75,0.0001467,0.75,0.0001467,0.75,0.0004464,0.75,0.0038399999999999,0.75,0.005592,0.75,0.000542,0.5,0.00711,0.75,0.00029876,0.75,0.0003123,0.75,8.560000000000001e-05,0.75,0.0002399999999999,0.75,0.0003472
grade-school-math.dev.6302,WizardLM/WizardLM-13B-V1.2,0.25,0.0001944,0.25,0.0001944,0.75,0.0005552,0.75,0.004064,0.75,0.005672,0.75,0.000569,0.75,0.00748,0.25,0.000354632,0.75,0.0003438,0.25,9.48e-05,0.25,0.0002718,0.75,0.0004024
mmlu-high-school-biology.val.65,WizardLM/WizardLM-13B-V1.2,0.0,2.58e-05,0.0,2.58e-05,0.0,6.960000000000001e-05,0.0,0.000696,0.0,0.000696,0.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,0.0,5.16e-05,1.0,6.88e-05
hellaswag.val.470,WizardLM/WizardLM-13B-V1.2,1.0,3.51e-05,1.0,3.51e-05,1.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000119,1.0,0.00118,0.0,9.0792e-05,1.0,0.0001053,0.0,2.34e-05,0.0,7.02e-05,1.0,9.36e-05
hellaswag.val.4538,WizardLM/WizardLM-13B-V1.2,0.0,7.439999999999999e-05,0.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00249,0.0,0.000192448,1.0,0.0002223,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
arc-challenge.test.1078,WizardLM/WizardLM-13B-V1.2,0.0,3.3600000000000004e-05,0.0,3.3600000000000004e-05,0.0,9.04e-05,0.0,0.000904,0.0,0.000904,1.0,0.000114,1.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,0.0,8.88e-05
