sample_id,phi_prediction,phi_correctness,phi_cost,WizardLM/WizardLM-13B-V1.2_correctness,WizardLM/WizardLM-13B-V1.2_cost,claude-instant-v1_correctness,claude-instant-v1_cost,claude-v1_correctness,claude-v1_cost,claude-v2_correctness,claude-v2_cost,gpt-3.5-turbo-1106_correctness,gpt-3.5-turbo-1106_cost,gpt-4-1106-preview_correctness,gpt-4-1106-preview_cost,meta/code-llama-instruct-34b-chat_correctness,meta/code-llama-instruct-34b-chat_cost,meta/llama-2-70b-chat_correctness,meta/llama-2-70b-chat_cost,mistralai/mistral-7b-chat_correctness,mistralai/mistral-7b-chat_cost,mistralai/mixtral-8x7b-chat_correctness,mistralai/mixtral-8x7b-chat_cost,zero-one-ai/Yi-34B-Chat_correctness,zero-one-ai/Yi-34B-Chat_cost
hellaswag.val.4102,A),0.0,0.0,1.0,8.13e-05,0.0,0.0002176,1.0,0.002176,1.0,0.002176,0.0,0.0002729999999999,1.0,0.00275,0.0,0.0002102959999999,1.0,0.0002439,1.0,5.420000000000001e-05,1.0,0.0001626,1.0,0.000216
winogrande.dev.802,meta/llama-2-70b-chat,1.0,4.410000000000001e-05,1.0,1.4699999999999998e-05,0.0,4.24e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,0.0,5.1e-05,1.0,0.0005,0.0,3.7248e-05,1.0,4.410000000000001e-05,0.0,9.8e-06,0.0,2.94e-05,1.0,3.84e-05
hellaswag.val.5482,B,0.0,0.0,0.0,6.45e-05,1.0,0.0001728,1.0,0.001728,0.0,0.001752,1.0,0.000217,1.0,0.00216,0.0,0.00016684,0.0,0.0001935,0.0,4.3e-05,0.0,0.000129,1.0,0.0001712
mmlu-professional-law.val.1256,claude-v2,1.0,0.002232,0.0,8.34e-05,0.0,0.0002232,0.0,0.002232,1.0,0.002232,0.0,0.000278,0.0,0.00279,0.0,0.000215728,0.0,0.0002502,0.0,5.56e-05,0.0,0.0001668,0.0,0.0002224
mmlu-elementary-mathematics.val.47,Model C,0.0,0.0,1.0,2.07e-05,0.0,5.6e-05,0.0,0.00056,0.0,0.00056,0.0,7.1e-05,1.0,0.0007,0.0,5.354400000000001e-05,0.0,6.21e-05,1.0,1.38e-05,0.0,4.14e-05,0.0,5.52e-05
mmlu-moral-disputes.val.343,claude-v2,1.0,0.0010479999999999,0.0,3.9e-05,1.0,0.0001048,1.0,0.0010479999999999,1.0,0.0010479999999999,1.0,0.00013,1.0,0.00131,0.0,0.00010088,0.0,0.000117,0.0,2.6e-05,0.0,7.8e-05,1.0,0.0001032
hellaswag.val.5407,Model C,0.0,0.0,0.0,9.3e-05,0.0,0.000252,0.0,0.002496,0.0,0.002496,1.0,0.0003109999999999,1.0,0.00312,0.0,0.000241336,0.0,0.0002799,0.0,6.22e-05,0.0,0.0001866,0.0,0.000248
grade-school-math.dev.2924,meta/llama-2-70b-chat,0.75,0.0003132,0.75,0.0001452,0.75,0.0004696,0.75,0.004312,0.75,0.005056,0.75,0.000448,0.75,0.00839,0.25,0.00030652,0.75,0.0003132,0.75,8.26e-05,0.75,0.0002352,0.75,0.0003352
mmlu-high-school-geography.val.86,meta/llama-2-70b-chat,0.0,9.27e-05,0.0,3.06e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000105,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
winogrande.dev.486,meta/llama-2-70b-chat,1.0,4.32e-05,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,1.0,0.000392,0.0,5e-05,1.0,0.00052,0.0,3.7248e-05,1.0,4.32e-05,0.0,9.6e-06,0.0,2.8799999999999995e-05,0.0,3.7600000000000006e-05
grade-school-math.dev.7113,meta/llama-2-70b-chat,0.25,0.0006156,0.25,0.0001841999999999,0.25,0.000876,0.75,0.00552,0.75,0.006744,0.5,0.000543,0.5,0.00927,0.25,0.0003298,0.25,0.0006156,0.25,0.00013,0.25,0.000303,0.75,0.0005552
grade-school-math.dev.1433,meta/llama-2-70b-chat,0.75,0.0003528,0.75,0.0001695,0.75,0.0005463999999999,0.75,0.005704,0.75,0.0061119999999999,0.75,0.000539,0.5,0.00671,0.25,0.000227368,0.75,0.0003528,0.25,8.460000000000001e-05,0.25,0.0002718,0.75,0.0004
arc-challenge.test.264,Model C,0.0,0.0,0.0,2.4e-05,0.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,8.2e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,0.0,4.8e-05,1.0,6.320000000000002e-05
hellaswag.val.6367,claude-v2,0.0,0.002096,0.0,7.83e-05,0.0,0.0002096,0.0,0.002096,0.0,0.002096,0.0,0.000263,0.0,0.00265,0.0,0.000202536,0.0,0.000234,0.0,5.220000000000001e-05,0.0,0.0001566,1.0,0.000208
mmlu-professional-psychology.val.109,Model C,0.0,0.0,0.0,3.15e-05,0.0,8.480000000000001e-05,0.0,0.000848,0.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,1.0,2.1e-05,1.0,6.3e-05,0.0,8.400000000000001e-05
chinese-lantern-riddles.dev.12,"Model C — correctness: 1, cost: 0.3

The prompt is asking for the meaning of a Chinese idiom (黯, which is a radical in Chinese characters) and the reasoning behind it. Model C",0.0,0.0,0.0,7.649999999999999e-05,0.0,0.0003799999999999,0.0,0.006296,0.0,0.004376,0.0,0.000236,0.0,0.0103,0.0,0.000180032,0.0,0.0002466,0.0,3.86e-05,0.0,0.0001968,0.0,0.0002
grade-school-math.dev.6467,meta/llama-2-70b-chat,0.25,0.0003798,0.25,0.0001779,0.75,0.0005415999999999,0.75,0.004504,0.25,0.007192,0.5,0.000456,0.75,0.00962,0.25,0.000396536,0.25,0.0003798,0.25,9.26e-05,0.75,0.0002076,0.25,0.0004384
hellaswag.val.553,B) remove the baking sheet from the oven,0.0,0.0,1.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.000107,1.0,0.00109,0.0,8.148e-05,1.0,9.45e-05,0.0,2.1e-05,0.0,6.3e-05,1.0,8.320000000000002e-05
consensus_summary.dev.189,N-A (None of the provided models directly address the question about family environment impacting cardiovascular fitness based on the given claims.),0.0,0.0,1.0,5.58e-05,0.5,0.0001456,0.5,0.001456,0.75,0.002608,0.5,0.0001799999999999,0.5,0.00182,0.75,0.000150544,0.75,0.0002277,0.75,3.88e-05,0.75,0.0001193999999999,0.5,0.0001424
mmlu-philosophy.val.105,claude-v2,1.0,0.000888,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,0.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,0.0,8.8e-05
grade-school-math.dev.6500,meta/llama-2-70b-chat,0.5,0.0003024,0.5,0.0001250999999999,0.75,0.0004776,0.75,0.00384,0.5,0.005448,0.5,0.000399,0.5,0.00639,0.75,0.000266944,0.5,0.0003024,0.25,0.0001028,0.5,0.0002243999999999,0.75,0.000376
grade-school-math.dev.1491,meta/llama-2-70b-chat,0.75,0.0003195,0.75,0.0001308,0.75,0.0003488,0.75,0.003752,0.75,0.004472,0.75,0.000376,0.75,0.0066999999999999,0.75,0.000242112,0.75,0.0003195,0.75,8.560000000000001e-05,0.75,0.0002112,0.75,0.00038
chinese_zodiac.dev.45,meta/llama-2-70b-chat,0.0,0.0001008,0.0,3.33e-05,0.0,9.2e-05,1.0,0.00092,0.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.00010864,0.0,0.0001008,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,8.88e-05
mmlu-medical-genetics.val.52,Model B,0.0,0.0,1.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
arc-challenge.test.567,Model C,0.0,0.0,1.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.6e-05,1.0,0.00085,0.0,6.5184e-05,1.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
mmlu-miscellaneous.val.768,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,0.0,2.4e-05,0.0,6.480000000000002e-05,0.0,0.000648,0.0,0.000648,0.0,7.999999999999999e-05,0.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,0.0,4.8e-05,0.0,6.400000000000001e-05
mmlu-professional-law.val.1104,B) claude-v2,0.0,0.0,1.0,6.81e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.0018239999999999,1.0,0.000227,1.0,0.00228,0.0,0.0001761519999999,0.0,0.0002042999999999,0.0,4.5400000000000006e-05,1.0,0.0001362,0.0,0.0001816
hellaswag.val.2487,Model D,0.0,0.0,0.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,1.0,8.91e-05,0.0,1.98e-05,0.0,5.94e-05,1.0,7.840000000000001e-05
winogrande.dev.543,meta/llama-2-70b-chat,0.0,4.5e-05,0.0,1.5e-05,0.0,4.08e-05,0.0,0.000408,0.0,0.000408,1.0,5.2e-05,0.0,0.00054,1.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,1.0,3e-05,0.0,3.92e-05
winogrande.dev.1173,Model B,0.0,0.0,1.0,1.59e-05,0.0,4.32e-05,0.0,0.000432,1.0,0.000432,1.0,5.5e-05,1.0,0.00057,0.0,4.1128e-05,1.0,4.77e-05,0.0,1.06e-05,0.0,3.18e-05,0.0,4.24e-05
mmlu-professional-accounting.val.137,claude-v2,0.0,0.000792,0.0,2.94e-05,0.0,7.920000000000001e-05,0.0,0.000792,0.0,0.000792,0.0,9.8e-05,1.0,0.00102,0.0,7.604800000000001e-05,0.0,8.82e-05,1.0,1.96e-05,1.0,5.88e-05,0.0,7.760000000000002e-05
mmlu-high-school-us-history.val.120,claude-v2,1.0,0.00304,0.0,0.0001137,0.0,0.000304,1.0,0.00304,1.0,0.00304,1.0,0.000379,1.0,0.0038,0.0,0.000294104,0.0,0.0003411,0.0,7.58e-05,1.0,0.0002274,1.0,0.0003032
hellaswag.val.293,Model C,0.0,0.0,0.0,3.63e-05,0.0,9.76e-05,0.0,0.000976,0.0,0.000976,0.0,0.000121,0.0,0.00122,1.0,9.3896e-05,0.0,0.000108,1.0,2.42e-05,0.0,7.259999999999999e-05,0.0,9.6e-05
mmlu-college-medicine.val.83,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-business-ethics.val.58,D,0.0,0.0,1.0,5.22e-05,0.0,0.00014,0.0,0.0014,0.0,0.0014,0.0,0.000174,1.0,0.00178,0.0,0.000135024,0.0,0.0001566,1.0,3.48e-05,0.0,0.0001044,1.0,0.0001392
grade-school-math.dev.4117,meta/llama-2-70b-chat,0.25,0.0003015,0.25,0.0001574999999999,0.75,0.0005935999999999,0.25,0.006704,0.25,0.00548,0.75,0.000299,0.75,0.01261,0.25,0.000304968,0.25,0.0003015,0.25,9.1e-05,0.25,0.0002718,0.25,0.0003224
mmlu-astronomy.val.85,Model B,0.0,0.0,1.0,4.26e-05,1.0,0.0001144,1.0,0.0011439999999999,1.0,0.0011439999999999,1.0,0.0001419999999999,1.0,0.00143,0.0,0.000110192,0.0,0.0001278,0.0,2.84e-05,1.0,8.52e-05,1.0,0.0001136
consensus_summary.dev.282,Meta/llama-2-70b-chat,0.0,0.0,0.75,0.0001215,0.5,0.0006256,0.25,0.001816,0.25,0.004408,0.75,0.000403,0.75,0.00524,0.75,0.000285568,0.75,0.000432,0.75,6.48e-05,0.75,0.0001812,0.75,0.0002424
grade-school-math.dev.2701,meta/llama-2-70b-chat,0.25,0.0003789,0.75,0.0001977,0.25,0.000736,0.5,0.005104,0.75,0.005464,0.25,0.00068,0.75,0.00761,0.25,0.000369376,0.25,0.0003789,0.25,0.0001112,0.75,0.000267,0.25,0.0004576
grade-school-math.dev.4976,meta/llama-2-70b-chat,0.25,0.0003528,0.25,0.000156,0.75,0.0005304,0.25,0.006504,0.5,0.008424,0.75,0.000561,0.75,0.00996,0.25,0.000340664,0.25,0.0003528,0.25,9.26e-05,0.25,0.0002741999999999,0.75,0.0004696
mmlu-us-foreign-policy.val.58,Model C,0.0,0.0,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,0.0,7.120000000000001e-05
mmlu-professional-law.val.1359,claude-v2,1.0,0.002048,0.0,7.649999999999999e-05,1.0,0.0002048,1.0,0.002048,1.0,0.002048,1.0,0.000255,1.0,0.00256,0.0,0.00019788,0.0,0.0002295,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-sociology.val.78,D) Scapegoating,0.0,0.0,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-moral-scenarios.val.208,"B) Not wrong, Wrong",0.0,0.0,0.0,4.2e-05,1.0,0.0001128,1.0,0.0011279999999999,1.0,0.0011279999999999,0.0,0.00014,1.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,1.0,8.4e-05,1.0,0.0001112
grade-school-math.dev.5044,meta/llama-2-70b-chat,0.75,0.0003123,0.75,0.0001467,0.75,0.0004464,0.75,0.0038399999999999,0.75,0.005592,0.75,0.000542,0.5,0.00711,0.75,0.00029876,0.75,0.0003123,0.75,8.560000000000001e-05,0.75,0.0002399999999999,0.75,0.0003472
grade-school-math.dev.6302,meta/llama-2-70b-chat,0.75,0.0003438,0.25,0.0001944,0.75,0.0005552,0.75,0.004064,0.75,0.005672,0.75,0.000569,0.75,0.00748,0.25,0.000354632,0.75,0.0003438,0.25,9.48e-05,0.25,0.0002718,0.75,0.0004024
mmlu-high-school-biology.val.65,Model B,0.0,0.0,0.0,2.58e-05,0.0,6.960000000000001e-05,0.0,0.000696,0.0,0.000696,0.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,0.0,5.16e-05,1.0,6.88e-05
hellaswag.val.470,Model C,0.0,0.0,1.0,3.51e-05,1.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000119,1.0,0.00118,0.0,9.0792e-05,1.0,0.0001053,0.0,2.34e-05,0.0,7.02e-05,1.0,9.36e-05
hellaswag.val.4538,B,0.0,0.0,0.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00249,0.0,0.000192448,1.0,0.0002223,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
arc-challenge.test.1078,Model C,0.0,0.0,0.0,3.3600000000000004e-05,0.0,9.04e-05,0.0,0.000904,0.0,0.000904,1.0,0.000114,1.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,0.0,8.88e-05
arc-challenge.test.570,Model C,0.0,0.0,1.0,2.49e-05,1.0,6.720000000000001e-05,1.0,0.000672,1.0,0.000672,0.0,8.5e-05,1.0,0.0008399999999999,0.0,6.4408e-05,1.0,7.470000000000001e-05,0.0,1.66e-05,1.0,4.98e-05,1.0,6.560000000000001e-05
mmlu-college-mathematics.val.77,Model C (meta/llama-2-70b-chat),0.0,0.0,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
mmlu-moral-disputes.val.225,B,0.0,0.0,1.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
mmlu-jurisprudence.val.72,Model C,0.0,0.0,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,0.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
winogrande.dev.604,Model B,0.0,0.0,0.0,1.6800000000000002e-05,0.0,4.56e-05,1.0,0.0004559999999999,1.0,0.0004559999999999,0.0,5.8e-05,1.0,0.0006,0.0,4.3456000000000005e-05,1.0,5.0400000000000005e-05,0.0,1.12e-05,0.0,3.3600000000000004e-05,0.0,4.4e-05
mmlu-high-school-computer-science.val.70,Model C,0.0,0.0,1.0,3.39e-05,0.0,9.120000000000002e-05,0.0,0.000912,0.0,0.000912,0.0,0.000113,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,1.0,2.2600000000000004e-05,0.0,6.72e-05,1.0,9.040000000000002e-05
grade-school-math.dev.6230,meta/llama-2-70b-chat,0.25,0.0003474,0.25,0.0001629,0.25,0.000548,0.75,0.004736,0.75,0.005312,0.75,0.000477,0.75,0.0066099999999999,0.75,0.000313504,0.25,0.0003474,0.25,0.0001062,0.75,0.000264,0.75,0.000316
hellaswag.val.4042,D) claude-v2,0.0,0.0,0.0,9.03e-05,1.0,0.0002416,1.0,0.002416,1.0,0.002416,1.0,0.000303,1.0,0.00302,0.0,0.000233576,1.0,0.0002709,0.0,6.0200000000000006e-05,0.0,0.0001806,1.0,0.00024
winogrande.dev.953,meta/llama-2-70b-chat,0.0,5.0400000000000005e-05,0.0,1.6800000000000002e-05,1.0,4.56e-05,1.0,0.0004559999999999,1.0,0.0004559999999999,1.0,5.8e-05,1.0,0.0006,0.0,4.4232000000000006e-05,0.0,5.0400000000000005e-05,1.0,1.12e-05,1.0,3.3600000000000004e-05,1.0,4.4e-05
hellaswag.val.8239,D,0.0,0.0,0.0,7.89e-05,0.0,0.0002144,0.0,0.00212,0.0,0.002144,1.0,0.000266,1.0,0.00265,0.0,0.000204864,0.0,0.0002367,0.0,5.280000000000001e-05,0.0,0.0001584,0.0,0.0002104
hellaswag.val.9774,D) claude-v2,0.0,0.0,0.0,6.84e-05,1.0,0.0001856,0.0,0.0018319999999999,1.0,0.0018319999999999,1.0,0.0002299999999999,1.0,0.00229,0.0,0.000176928,1.0,0.0002043,0.0,4.56e-05,1.0,0.0001368,1.0,0.0001816
abstract2title.test.247,meta/llama-2-70b-chat,1.0,0.0002538,1.0,4.83e-05,1.0,0.0001608,1.0,0.00192,1.0,0.001872,1.0,0.000154,1.0,0.0021,1.0,0.00012028,1.0,0.0002538,1.0,2.82e-05,1.0,0.0001338,1.0,0.000112
mmlu-high-school-world-history.val.9,meta/llama-2-70b-chat,0.0,0.0004959,1.0,0.0001652999999999,1.0,0.0004416,1.0,0.004416,1.0,0.004416,1.0,0.000551,1.0,0.00552,0.0,0.000427576,0.0,0.0004959,0.0,0.0001102,1.0,0.0003305999999999,1.0,0.00044
mmlu-professional-law.val.554,claude-v2,0.0,0.002168,0.0,8.099999999999999e-05,0.0,0.0002168,0.0,0.002168,0.0,0.002168,0.0,0.00027,0.0,0.00271,0.0,0.00020952,0.0,0.000243,1.0,5.4000000000000005e-05,0.0,0.0001619999999999,0.0,0.0002152
mmlu-professional-law.val.423,claude-v2,1.0,0.001928,0.0,7.199999999999999e-05,0.0,0.0001928,0.0,0.001928,1.0,0.001928,0.0,0.00024,0.0,0.00241,0.0,0.00018624,0.0,0.000216,0.0,4.8e-05,0.0,0.0001439999999999,1.0,0.0001911999999999
hellaswag.val.3654,A,0.0,0.0,1.0,8.519999999999998e-05,1.0,0.0002288,0.0,0.002288,1.0,0.002288,0.0,0.000287,1.0,0.00289,1.0,0.00022116,1.0,0.0002565,1.0,5.7e-05,1.0,0.0001709999999999,1.0,0.0002272
chinese_zodiac.dev.383,meta/llama-2-70b-chat,0.0,0.0001215,0.0,3.75e-05,0.0,9.2e-05,1.0,0.00092,0.0,0.00092,0.0,0.000113,1.0,0.00115,0.0,0.000113296,0.0,0.0001215,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,9.44e-05
arc-challenge.test.408,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 1, cost: 0.6
Model D",0.0,0.0,0.0,3.3e-05,0.0,8.88e-05,0.0,0.000888,0.0,0.000888,0.0,0.000112,1.0,0.00111,1.0,8.536000000000001e-05,0.0,9.9e-05,1.0,2.2e-05,0.0,6.6e-05,0.0,8.72e-05
mmlu-professional-accounting.val.52,Model C,0.0,0.0,0.0,3.69e-05,0.0,9.92e-05,0.0,0.000992,0.0,0.000992,0.0,0.000123,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001107,1.0,2.46e-05,1.0,7.379999999999999e-05,0.0,9.84e-05
grade-school-math.dev.611,meta/llama-2-70b-chat,0.75,0.0003995999999999,0.25,0.0001307999999999,0.25,0.0002608,0.75,0.005464,0.75,0.005416,0.75,0.000483,0.75,0.00917,0.25,0.00034144,0.75,0.0003995999999999,0.25,8.740000000000001e-05,0.75,0.0002316,0.75,0.0003464
grade-school-math.dev.3476,meta/llama-2-70b-chat,0.75,0.0003015,0.75,0.0001233,0.75,0.0004368,0.5,0.003864,0.5,0.004776,0.25,0.000283,0.5,0.00531,0.75,0.000261512,0.75,0.0003015,0.75,7.48e-05,0.75,0.0002279999999999,0.75,0.0003288
mmlu-professional-law.val.1449,D,0.0,0.0,0.0,5.52e-05,1.0,0.0001488,1.0,0.001488,1.0,0.001488,0.0,0.000185,1.0,0.00186,0.0,0.00014356,0.0,0.0001665,0.0,3.7000000000000005e-05,1.0,0.000111,1.0,0.0001472
mmlu-world-religions.val.7,B,0.0,0.0,0.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
hellaswag.val.7887,D) claude-v2,0.0,0.0,0.0,8.01e-05,1.0,0.0002144,0.0,0.002144,1.0,0.002144,0.0,0.000269,1.0,0.00268,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
mmlu-econometrics.val.103,C) claude-v2,0.0,0.0,1.0,4.14e-05,0.0,0.0001112,0.0,0.001112,0.0,0.001112,0.0,0.000138,0.0,0.00139,0.0,0.000106312,0.0,0.0001241999999999,0.0,2.7600000000000003e-05,0.0,8.28e-05,0.0,0.0001104
hellaswag.val.4426,D,0.0,0.0,0.0,6.989999999999999e-05,1.0,0.0001872,0.0,0.001872,1.0,0.001872,1.0,0.000233,1.0,0.00234,0.0,0.000180808,1.0,0.0002088,0.0,4.660000000000001e-05,0.0,0.0001397999999999,1.0,0.0001856
hellaswag.val.4339,C),0.0,0.0,0.0,8.819999999999999e-05,1.0,0.000236,1.0,0.00236,1.0,0.00236,1.0,0.000296,1.0,0.00298,0.0,0.000228144,1.0,0.0002646,0.0,5.8800000000000006e-05,0.0,0.0001763999999999,1.0,0.0002344
hellaswag.val.4793,D,0.0,0.0,1.0,7.47e-05,1.0,0.0002024,1.0,0.002,1.0,0.002,1.0,0.000251,1.0,0.00253,1.0,0.0001932239999999,1.0,0.0002232,1.0,4.980000000000001e-05,1.0,0.0001494,1.0,0.0001984
mmlu-professional-accounting.val.170,Model C,0.0,0.0,0.0,3.6e-05,1.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.0001199999999999,1.0,0.00124,0.0,9.312e-05,0.0,0.000108,0.0,2.4e-05,0.0,7.2e-05,1.0,9.52e-05
hellaswag.val.1480,claude-v2,0.0,0.000712,0.0,2.64e-05,0.0,7.12e-05,1.0,0.000712,0.0,0.000712,1.0,9e-05,1.0,0.00089,0.0,6.828800000000001e-05,1.0,7.83e-05,0.0,1.7599999999999998e-05,0.0,5.28e-05,1.0,6.96e-05
winogrande.dev.745,meta/llama-2-70b-chat,0.0,4.770000000000001e-05,0.0,1.59e-05,0.0,4.4e-05,1.0,0.0004399999999999,0.0,0.0004399999999999,0.0,5.6e-05,0.0,0.0005499999999999,1.0,4.1904e-05,0.0,4.770000000000001e-05,1.0,1.08e-05,1.0,3.24e-05,0.0,4.32e-05
mmlu-professional-psychology.val.468,Model D,0.0,0.0,1.0,3.93e-05,0.0,0.0001056,1.0,0.0010559999999999,0.0,0.0010559999999999,0.0,0.0001309999999999,1.0,0.00132,0.0,0.000101656,0.0,0.0001179,0.0,2.62e-05,0.0,7.86e-05,0.0,0.0001048
chinese_zodiac.dev.134,meta/llama-2-70b-chat,0.0,0.0001152,0.0,3.3600000000000004e-05,0.0,9.28e-05,1.0,0.000928,0.0,0.000928,0.0,0.000114,0.0,0.00116,0.0,0.000107864,0.0,0.0001152,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,9.28e-05
mmlu-professional-law.val.814,claude-v2,1.0,0.00284,0.0,0.0001061999999999,1.0,0.000284,1.0,0.00284,1.0,0.00284,1.0,0.000356,1.0,0.00355,0.0,0.0002747039999999,0.0,0.0003186,0.0,7.08e-05,1.0,0.0002123999999999,1.0,0.0002824
mtbench.dev.3,"Correct choice: claude-v2

This model is chosen because it is designed to handle complex language tasks, including rephrasing and incorporating literary devices like metaphors and similes. The ""v2"" version suggests an",0.0,0.0,0.9,0.0003209999999999,1.0,0.0016776,0.7,0.018216,1.0,0.012768,1.0,0.0009029999999999,1.0,0.03033,0.9,0.000466376,0.9,0.0009198,0.4,0.0001424,1.0,0.0006575999999999,1.0,0.0006768
mmlu-miscellaneous.val.781,Model B,0.0,0.0,1.0,1.95e-05,1.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.7e-05,1.0,0.00066,0.0,5.044e-05,0.0,5.8500000000000006e-05,0.0,1.3e-05,1.0,3.9e-05,1.0,5.12e-05
hellaswag.val.3409,claude-v2,1.0,0.002112,0.0,7.89e-05,0.0,0.0002136,0.0,0.002112,1.0,0.002112,1.0,0.000265,1.0,0.00264,0.0,0.000204088,1.0,0.0002358,0.0,5.260000000000001e-05,0.0,0.0001578,1.0,0.0002096
mmlu-high-school-biology.val.298,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00084,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
grade-school-math.dev.7313,meta/llama-2-70b-chat,0.25,0.0002988,0.25,0.0001539,0.75,0.0006224,0.75,0.005096,0.75,0.0068,0.75,0.0004799999999999,0.75,0.00835,0.75,0.0002669439999999,0.25,0.0002988,0.25,8.280000000000001e-05,0.25,0.0002838,0.25,0.0004032
mmlu-high-school-physics.val.147,claude-v2,0.0,0.0012239999999999,0.0,4.56e-05,1.0,0.0001224,0.0,0.0012239999999999,0.0,0.0012239999999999,0.0,0.0001519999999999,0.0,0.00153,0.0,0.000117952,0.0,0.0001368,0.0,3.04e-05,0.0,9.12e-05,0.0,0.0001216
grade-school-math.dev.4322,meta/llama-2-70b-chat,0.75,0.0004005,0.75,0.0001427999999999,0.75,0.0007032,0.75,0.004536,0.75,0.004944,0.75,0.000609,0.75,0.0089699999999999,0.75,0.000381792,0.75,0.0004005,0.75,9.98e-05,0.75,0.0002663999999999,0.75,0.0003808
hellaswag.val.2307,Model C,0.0,0.0,0.0,2.82e-05,0.0,7.600000000000002e-05,0.0,0.00076,1.0,0.00076,1.0,9.6e-05,1.0,0.00095,0.0,7.2944e-05,0.0,8.46e-05,0.0,1.8800000000000003e-05,0.0,5.64e-05,1.0,7.440000000000002e-05
hellaswag.val.3508,B,0.0,0.0,0.0,8.16e-05,1.0,0.0002208,1.0,0.002184,1.0,0.002184,0.0,0.000274,1.0,0.00276,0.0,0.000211072,1.0,0.0002439,0.0,5.44e-05,0.0,0.0001632,1.0,0.0002167999999999
mmlu-abstract-algebra.val.98,Model C,0.0,0.0,0.0,2.3400000000000003e-05,1.0,6.32e-05,0.0,0.000632,1.0,0.000632,0.0,7.8e-05,0.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.240000000000001e-05
grade-school-math.dev.2300,meta/llama-2-70b-chat,0.25,0.0003987,0.25,0.0001827,0.25,0.000644,0.5,0.00728,0.75,0.008552,0.75,0.000673,0.75,0.01078,0.25,0.00040352,0.25,0.0003987,0.5,0.0001036,0.75,0.000411,0.25,0.0002408
hellaswag.val.3270,Model C,0.0,0.0,0.0,7.23e-05,1.0,0.0001944,1.0,0.001944,1.0,0.001944,1.0,0.000244,1.0,0.00246,0.0,0.000187792,1.0,0.0002169,0.0,4.84e-05,0.0,0.0001452,1.0,0.0001928
winogrande.dev.594,Model B,0.0,0.0,0.0,1.59e-05,0.0,4.32e-05,0.0,0.000432,0.0,0.000432,0.0,5.5e-05,0.0,0.00057,0.0,4.0352e-05,0.0,4.77e-05,1.0,1.06e-05,1.0,3.18e-05,0.0,4.16e-05
mmlu-jurisprudence.val.56,meta/llama-2-70b-chat,0.0,0.0001431,1.0,4.77e-05,1.0,0.000128,0.0,0.0012799999999999,1.0,0.0012799999999999,1.0,0.000159,0.0,0.0016,0.0,0.000123384,0.0,0.0001431,1.0,3.180000000000001e-05,1.0,9.54e-05,1.0,0.0001264
grade-school-math.dev.5533,meta/llama-2-70b-chat,0.5,0.0003429,0.5,0.0001437,0.25,0.000608,0.25,0.004616,0.5,0.005336,0.5,0.000501,0.5,0.00739,0.25,0.000346872,0.5,0.0003429,0.0,7.620000000000001e-05,0.75,0.0002184,0.75,0.0003808
hellaswag.val.4438,D,0.0,0.0,0.0,6.569999999999998e-05,0.0,0.0001768,0.0,0.001768,0.0,0.001768,1.0,0.00022,1.0,0.00221,0.0,0.00017072,1.0,0.000198,0.0,4.4000000000000006e-05,0.0,0.0001319999999999,0.0,0.0001752
mbpp.dev.401,meta/llama-2-70b-chat,0.0,0.0004347,0.0,9.78e-05,0.0,0.0006456,0.0,0.004656,1.0,0.00576,1.0,0.000309,1.0,0.02001,0.0,0.0002304719999999,0.0,0.0004347,0.0,5.76e-05,1.0,0.0001715999999999,0.0,0.000152
winogrande.dev.1148,meta/llama-2-70b-chat,0.0,4.5e-05,0.0,1.5e-05,0.0,4.08e-05,0.0,0.000408,0.0,0.000408,0.0,5.2e-05,1.0,0.00051,1.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,0.0,3e-05,0.0,4e-05
mmlu-jurisprudence.val.95,meta/llama-2-70b-chat,0.0,9.27e-05,1.0,3.09e-05,0.0,8.320000000000002e-05,0.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,0.0,8.240000000000001e-05
grade-school-math.dev.4923,meta/llama-2-70b-chat,0.25,0.000378,0.75,0.0001668,0.75,0.0005888,0.75,0.00572,0.75,0.006032,0.25,0.000616,0.25,0.01126,0.25,0.000391104,0.25,0.000378,0.25,0.000115,0.25,0.0003426,0.25,0.0002552
hellaswag.val.1084,B) claude-v2,0.0,0.0,1.0,3.51e-05,1.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000117,1.0,0.00118,0.0,9.0792e-05,1.0,0.0001043999999999,0.0,2.34e-05,1.0,7.02e-05,1.0,9.28e-05
hellaswag.val.734,Model D,0.0,0.0,0.0,3.57e-05,1.0,9.600000000000002e-05,1.0,0.00096,1.0,0.00096,1.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,1.0,0.0001061999999999,0.0,2.3800000000000003e-05,1.0,7.14e-05,1.0,9.440000000000002e-05
grade-school-math.dev.5655,meta/llama-2-70b-chat,0.25,0.0003771,0.25,0.0001610999999999,0.75,0.000608,0.75,0.005576,0.75,0.006992,0.75,0.000445,0.75,0.00844,0.25,0.000326696,0.25,0.0003771,1.0,6.48e-05,0.75,0.0002766,0.25,0.000244
mmlu-high-school-psychology.val.201,A) humanistic,0.0,0.0,1.0,2.25e-05,1.0,6.08e-05,1.0,0.000608,1.0,0.000608,1.0,7.5e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,1.0,1.5e-05,1.0,4.5e-05,1.0,5.92e-05
mmlu-college-biology.val.105,Model C,0.0,0.0,0.0,2.31e-05,0.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
mmlu-professional-psychology.val.461,B) Cognitive Assessment System,0.0,0.0,1.0,3.12e-05,0.0,8.400000000000001e-05,1.0,0.00084,1.0,0.00084,1.0,0.000104,1.0,0.00108,0.0,8.0704e-05,0.0,9.36e-05,0.0,2.08e-05,1.0,6.24e-05,1.0,8.32e-05
mmlu-college-computer-science.val.54,Model C,0.0,0.0,1.0,3.93e-05,0.0,0.0001056,0.0,0.0010559999999999,0.0,0.0010559999999999,1.0,0.0001309999999999,1.0,0.00132,0.0,0.000101656,0.0,0.0001179,1.0,2.62e-05,1.0,7.86e-05,0.0,0.0001048
mbpp.dev.175,meta/llama-2-70b-chat,0.0,0.0003933,0.0,0.0001365,1.0,0.0005584,1.0,0.005128,1.0,0.005368,1.0,0.000565,1.0,0.0137899999999999,1.0,0.000241336,0.0,0.0003933,0.0,6.48e-05,0.0,0.0002538,0.0,0.0002968
hellaswag.val.4597,A) claude-v2,0.0,0.0,1.0,6.84e-05,1.0,0.0001832,1.0,0.0018319999999999,1.0,0.0018319999999999,0.0,0.0002299999999999,1.0,0.00232,1.0,0.000176928,1.0,0.0002043,1.0,4.56e-05,1.0,0.0001368,1.0,0.0001816
hellaswag.val.7788,D,0.0,0.0,1.0,8.25e-05,1.0,0.000224,1.0,0.002216,1.0,0.002216,1.0,0.000278,1.0,0.0028,0.0,0.0002141759999999,1.0,0.0002483999999999,0.0,5.520000000000001e-05,1.0,0.0001656,1.0,0.00022
mmlu-professional-law.val.11,claude-v2,1.0,0.0041839999999999,1.0,0.0001565999999999,1.0,0.0004184,1.0,0.0041839999999999,1.0,0.0041839999999999,1.0,0.000522,1.0,0.0052299999999999,0.0,0.000405072,0.0,0.0004698,0.0,0.0001043999999999,1.0,0.0003131999999999,1.0,0.0004168
mmlu-prehistory.val.156,claude-v2,1.0,0.000832,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.240000000000001e-05
mmlu-professional-law.val.1172,Model C) claude-v2,0.0,0.0,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,0.0,4.56e-05,0.0,6e-05
hellaswag.val.4942,B,0.0,0.0,1.0,7.68e-05,1.0,0.0002088,0.0,0.002064,0.0,0.002064,0.0,0.000259,1.0,0.00261,1.0,0.0001994319999999,0.0,0.0002304,1.0,5.14e-05,1.0,0.0001542,1.0,0.0002048
hellaswag.val.4474,D,0.0,0.0,0.0,7.469999999999999e-05,0.0,0.0002032,0.0,0.002008,1.0,0.002008,1.0,0.000252,1.0,0.00251,0.0,0.000194,1.0,0.0002241,0.0,5e-05,0.0,0.00015,1.0,0.0001992
hellaswag.val.7170,B,0.0,0.0,1.0,7.110000000000001e-05,1.0,0.0001904,1.0,0.0019039999999999,1.0,0.0019039999999999,1.0,0.0002389999999999,1.0,0.00241,0.0,0.000183912,1.0,0.0002123999999999,0.0,4.74e-05,0.0,0.0001422,1.0,0.0001896
grade-school-math.dev.658,meta/llama-2-70b-chat,0.25,0.0004212,0.25,0.0002028,0.25,0.0007208,0.75,0.005912,0.25,0.010256,0.75,0.000561,0.25,0.01078,0.25,0.000334456,0.25,0.0004212,0.75,9.9e-05,1.0,0.0001998,0.25,0.0004544
mmlu-high-school-microeconomics.val.235,Model C,0.0,0.0,0.0,3.78e-05,1.0,0.0001016,1.0,0.001016,1.0,0.001016,1.0,0.000126,1.0,0.00127,0.0,9.7776e-05,0.0,0.0001134,0.0,2.52e-05,1.0,7.56e-05,1.0,0.0001
mmlu-high-school-geography.val.31,Model C) claude-v2,0.0,0.0,0.0,2.22e-05,1.0,6e-05,0.0,0.0006,1.0,0.0006,0.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,0.0,4.44e-05,0.0,5.92e-05
grade-school-math.dev.2234,meta/llama-2-70b-chat,0.25,0.0004176,0.75,0.0001688999999999,0.25,0.0006384,0.25,0.005808,0.5,0.00708,0.25,0.000641,0.75,0.01029,0.25,0.000327472,0.25,0.0004176,0.25,0.0001146,0.5,0.0003048,0.75,0.0006384
mmlu-professional-psychology.val.391,Model C,0.0,0.0,0.0,4.32e-05,0.0,0.000116,0.0,0.00116,0.0,0.00116,0.0,0.000144,0.0,0.00145,0.0,0.000111744,0.0,0.0001295999999999,1.0,2.88e-05,0.0,8.64e-05,0.0,0.0001152
mmlu-professional-law.val.278,meta/llama-2-70b-chat,0.0,0.0002267999999999,0.0,7.56e-05,1.0,0.0002024,1.0,0.002024,0.0,0.002024,0.0,0.000252,0.0,0.00253,0.0,0.000195552,0.0,0.0002267999999999,0.0,5.0400000000000005e-05,0.0,0.0001512,0.0,0.0002016
grade-school-math.dev.3135,meta/llama-2-70b-chat,0.5,0.0004293,0.75,0.0001632,0.75,0.0006256,0.75,0.005728,0.75,0.005824,0.75,0.0006349999999999,0.5,0.00878,0.25,0.000379464,0.5,0.0004293,0.75,8.5e-05,0.75,0.0002724,0.5,0.0004128
grade-school-math.dev.1252,meta/llama-2-70b-chat,0.75,0.0003447,0.0,0.0001640999999999,0.75,0.0005776,0.75,0.004408,0.5,0.005128,0.75,0.000549,0.75,0.00845,0.25,0.000262288,0.75,0.0003447,0.75,9.26e-05,0.75,0.0002615999999999,0.75,0.000388
mmlu-college-computer-science.val.97,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 1, cost: 0.7
Model D",0.0,0.0,1.0,4.08e-05,1.0,0.0001096,1.0,0.001096,0.0,0.001096,1.0,0.000136,1.0,0.0014,0.0,0.000105536,0.0,0.0001224,1.0,2.72e-05,1.0,8.159999999999999e-05,1.0,0.0001088
hellaswag.val.9610,Model B,0.0,0.0,0.0,7.86e-05,0.0,0.0002128,0.0,0.002104,0.0,0.002104,0.0,0.0002619999999999,1.0,0.00266,0.0,0.000203312,0.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
abstract2title.test.243,meta/llama-2-70b-chat,1.0,0.0003968999999999,1.0,5.97e-05,1.0,0.0002264,1.0,0.002,1.0,0.00224,1.0,0.000215,1.0,0.00247,1.0,0.000145888,1.0,0.0003968999999999,1.0,3.78e-05,1.0,0.0001188,1.0,0.0001568
grade-school-math.dev.563,meta/llama-2-70b-chat,0.75,0.0003618,0.75,0.0001395,0.25,0.0007335999999999,0.75,0.004528,0.75,0.005776,0.75,0.000512,0.75,0.0064399999999999,0.75,0.000320488,0.75,0.0003618,0.75,8.02e-05,0.75,0.0002292,0.25,0.0002368
mmlu-nutrition.val.153,D,0.0,0.0,0.0,5.1e-05,1.0,0.0001368,1.0,0.0013679999999999,1.0,0.0013679999999999,1.0,0.0001699999999999,1.0,0.00171,0.0,0.0001319199999999,0.0,0.000153,0.0,3.4000000000000007e-05,1.0,0.000102,1.0,0.000136
hellaswag.val.6632,A,0.0,0.0,1.0,8.37e-05,0.0,0.0002264,0.0,0.00224,0.0,0.00224,0.0,0.000279,1.0,0.0028,1.0,0.0002165039999999,1.0,0.0002511,1.0,5.580000000000001e-05,1.0,0.0001674,1.0,0.0002224
mmlu-professional-law.val.559,meta/llama-2-70b-chat,0.0,0.0002448,0.0,8.16e-05,1.0,0.0002184,1.0,0.002184,1.0,0.002184,0.0,0.000272,1.0,0.00273,0.0,0.000211072,0.0,0.0002448,0.0,5.44e-05,0.0,0.0001632,0.0,0.0002167999999999
arc-challenge.test.779,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,1.0,1.95e-05,1.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.5e-05,1.0,0.00069,1.0,5.044e-05,0.0,5.8500000000000006e-05,1.0,1.3e-05,1.0,3.9e-05,1.0,5.12e-05
grade-school-math.dev.2424,meta/llama-2-70b-chat,0.75,0.0003474,0.25,0.0001428,0.75,0.0005208,0.75,0.00432,0.75,0.00504,0.75,0.000541,0.5,0.00717,0.25,0.000339888,0.75,0.0003474,0.25,9.68e-05,0.75,0.0002706,0.75,0.000332
grade-school-math.dev.3418,meta/llama-2-70b-chat,0.75,0.0003717,0.75,0.0001791,0.25,0.0006399999999999,0.75,0.00448,0.75,0.006712,0.75,0.00051,0.5,0.0100699999999999,0.25,0.000331352,0.75,0.0003717,0.25,6.02e-05,0.75,0.0002502,0.25,0.0002376
grade-school-math.dev.6003,meta/llama-2-70b-chat,0.25,0.0004536,0.25,0.0001854,0.5,0.000616,0.25,0.005824,0.75,0.007312,0.25,0.000653,0.5,0.01244,0.25,0.000423696,0.25,0.0004536,0.25,0.0001138,0.25,0.0003095999999999,0.25,0.0006896
grade-school-math.dev.7387,meta/llama-2-70b-chat,0.75,0.0004374,0.25,0.000174,0.75,0.0005608,0.75,0.004768,0.75,0.005488,0.5,0.000494,0.75,0.00764,0.75,0.000313504,0.75,0.0004374,0.75,7.26e-05,0.75,0.0002652,0.75,0.0004056
mmlu-high-school-psychology.val.109,Model D) Mania,0.0,0.0,0.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
arc-challenge.test.39,Model C,0.0,0.0,1.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000105,1.0,0.00107,0.0,7.992800000000001e-05,1.0,9.27e-05,1.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
hellaswag.val.3974,D,0.0,0.0,0.0,7.89e-05,0.0,0.0002136,0.0,0.002112,0.0,0.002112,0.0,0.000265,1.0,0.00264,0.0,0.000204088,0.0,0.0002367,0.0,5.260000000000001e-05,0.0,0.0001578,0.0,0.0002096
grade-school-math.dev.807,meta/llama-2-70b-chat,0.75,0.0003294,0.25,0.0001671,0.75,0.000484,0.75,0.005752,0.75,0.00544,0.75,0.0004399999999999,0.75,0.00668,0.75,0.000270824,0.75,0.0003294,0.75,7.04e-05,0.25,0.0002322,0.75,0.0003424
mmlu-philosophy.val.7,Model C,0.0,0.0,0.0,3.33e-05,1.0,8.960000000000001e-05,1.0,0.000896,1.0,0.000896,1.0,0.000111,1.0,0.00112,0.0,8.6136e-05,0.0,9.990000000000002e-05,0.0,2.22e-05,0.0,6.659999999999999e-05,1.0,8.88e-05
chinese_idioms.dev.12,llama-2-70b-chat,0.0,0.0,0.25,4.68e-05,0.0,7.439999999999999e-05,0.25,0.00132,0.0,0.002424,0.25,0.0001189999999999,0.25,0.00738,0.25,0.000155976,0.25,0.0001566,0.25,2.36e-05,0.25,0.0001578,0.75,0.0001272
mmlu-professional-law.val.914,B,0.0,0.0,0.0,6.3e-05,0.0,0.0001688,0.0,0.0016879999999999,0.0,0.0016879999999999,0.0,0.0002099999999999,0.0,0.00211,0.0,0.00016296,0.0,0.000189,0.0,4.2e-05,1.0,0.000126,0.0,0.000168
hellaswag.val.2843,Model C,0.0,0.0,0.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00094,0.0,6.984e-05,0.0,8.01e-05,1.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
hellaswag.val.5896,C) claude-v2,0.0,0.0,0.0,8.4e-05,0.0,0.0002272,0.0,0.002248,0.0,0.002248,1.0,0.00028,1.0,0.00281,0.0,0.00021728,0.0,0.000252,0.0,5.6000000000000006e-05,0.0,0.000168,1.0,0.0002232
mmlu-electrical-engineering.val.87,Model B,0.0,0.0,0.0,2.7e-05,0.0,7.280000000000001e-05,0.0,0.000728,0.0,0.000728,0.0,8.999999999999999e-05,0.0,0.00094,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,0.0,7.2e-05
mtbench-math.dev.12,meta/llama-2-70b-chat,0.5,0.0004491,0.1,0.0002505,0.1,0.0015048,0.1,0.0073919999999999,0.1,0.008472,0.1,0.000761,1.0,0.0151499999999999,0.1,0.000198656,0.5,0.0004491,0.1,7.16e-05,0.1,0.0002298,0.1,0.0004048
arc-challenge.test.978,Model B,0.0,0.0,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,9e-05,1.0,0.00089,0.0,6.828800000000001e-05,1.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
hellaswag.val.8506,C) claude-v2,0.0,0.0,0.0,8.819999999999999e-05,0.0,0.000236,0.0,0.00236,1.0,0.00236,0.0,0.000296,1.0,0.00298,0.0,0.000228144,1.0,0.0002646,0.0,5.8800000000000006e-05,0.0,0.0001763999999999,1.0,0.0002344
grade-school-math.dev.2075,meta/llama-2-70b-chat,0.25,0.0004238999999999,0.25,0.0001664999999999,0.25,0.0008344,0.25,0.0068559999999999,0.75,0.006664,0.25,0.0007589999999999,0.75,0.01529,0.25,0.000374032,0.25,0.0004238999999999,0.75,0.000102,0.25,0.0002826,0.25,0.000652
mmlu-high-school-us-history.val.137,meta/llama-2-70b-chat,0.0,0.0002961,1.0,9.87e-05,1.0,0.000264,1.0,0.00264,1.0,0.00264,1.0,0.000329,1.0,0.0033,0.0,0.000255304,0.0,0.0002961,1.0,6.58e-05,1.0,0.0001974,1.0,0.0002624
mmlu-prehistory.val.48,Model C) meta/llama-2-70b-chat,0.0,0.0,0.0,3.09e-05,0.0,8.320000000000002e-05,0.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,0.0,6.18e-05,1.0,8.240000000000001e-05
mmlu-high-school-microeconomics.val.49,claude-v2,1.0,0.000864,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,1.0,2.14e-05,1.0,6.42e-05,1.0,8.56e-05
mmlu-elementary-mathematics.val.366,Model D,0.0,0.0,0.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.8e-05
grade-school-math.dev.5500,meta/llama-2-70b-chat,0.75,0.0004077,0.25,0.000174,0.25,0.0006136,0.75,0.005896,0.5,0.008632,0.75,0.000674,0.5,0.0092299999999999,0.25,0.00031428,0.75,0.0004077,0.5,0.0001056,0.5,0.0002742,0.25,0.0004112
mmlu-high-school-geography.val.2,Model D,0.0,0.0,0.0,2.13e-05,1.0,5.76e-05,1.0,0.000576,1.0,0.000576,1.0,7.099999999999999e-05,1.0,0.0007199999999999,0.0,5.5096e-05,0.0,6.390000000000001e-05,0.0,1.42e-05,1.0,4.26e-05,1.0,5.6e-05
grade-school-math.dev.2770,meta/llama-2-70b-chat,0.25,0.0003492,0.25,0.0001844999999999,0.25,0.0007344,0.25,0.005232,0.25,0.00648,0.25,0.000552,0.25,0.00954,0.25,0.000273152,0.25,0.0003492,0.25,0.0001058,0.25,0.00024,0.25,0.0003816
hellaswag.val.1826,Model C,0.0,0.0,0.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9e-05,0.0,2.02e-05,0.0,6.06e-05,1.0,8e-05
mmlu-high-school-statistics.val.68,C,0.0,0.0,0.0,5.46e-05,0.0,0.0001464,1.0,0.001464,0.0,0.001464,1.0,0.000182,1.0,0.0018599999999999,0.0,0.000141232,0.0,0.0001638,1.0,3.64e-05,1.0,0.0001092,0.0,0.0001448
grade-school-math.dev.4470,meta/llama-2-70b-chat,0.75,0.0003087,0.75,0.0001311,0.75,0.0004568,0.5,0.00212,0.75,0.005528,0.5,0.000294,0.75,0.00559,0.75,0.0002328,0.75,0.0003087,0.75,6.780000000000001e-05,0.75,0.0002004,0.5,0.0003128
winogrande.dev.571,Model C - claude-v2,0.0,0.0,0.0,1.4699999999999998e-05,0.0,4e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,0.0,5.1e-05,0.0,0.0005,0.0,3.8024e-05,1.0,4.410000000000001e-05,0.0,9.8e-06,0.0,2.94e-05,0.0,3.84e-05
mmlu-anatomy.val.97,Model B,0.0,0.0,0.0,3.99e-05,1.0,0.0001072,1.0,0.001072,1.0,0.001072,0.0,0.000133,0.0,0.00134,0.0,0.000103208,0.0,0.0001197,0.0,2.6600000000000003e-05,0.0,7.98e-05,1.0,0.0001064
mmlu-professional-law.val.884,A,0.0,0.0,0.0,7.89e-05,0.0,0.0002112,0.0,0.002112,0.0,0.002112,0.0,0.000263,1.0,0.00264,0.0,0.000204088,0.0,0.0002367,1.0,5.260000000000001e-05,1.0,0.0001578,1.0,0.0002096
mmlu-management.val.7,Model C) Referent,0.0,0.0,1.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,1.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
mmlu-professional-law.val.1232,D) claude-v2,0.0,0.0,0.0,8.04e-05,0.0,0.0002152,0.0,0.002152,0.0,0.002152,1.0,0.000268,1.0,0.00269,0.0,0.0002079679999999,0.0,0.0002412,1.0,5.360000000000001e-05,1.0,0.0001608,1.0,0.0002144
hellaswag.val.27,claude-v2,1.0,0.00092,0.0,3.4200000000000005e-05,1.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.000116,1.0,0.00115,0.0,8.846400000000001e-05,1.0,0.0001016999999999,0.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
mmlu-conceptual-physics.val.129,Model D,0.0,0.0,0.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,0.0,7.7e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,1.0,4.5e-05,0.0,6e-05
grade-school-math.dev.51,meta/llama-2-70b-chat,0.75,0.0003924,0.5,0.0001626,0.75,0.0006575999999999,0.75,0.004632,0.75,0.00564,0.75,0.00065,0.5,0.01017,0.75,0.000346872,0.75,0.0003924,0.25,6.120000000000001e-05,0.75,0.0002958,0.75,0.000456
hellaswag.val.6993,C,0.0,0.0,0.0,7.769999999999999e-05,1.0,0.000208,0.0,0.00208,1.0,0.00208,1.0,0.000261,1.0,0.00263,0.0,0.000200984,1.0,0.0002322,0.0,5.1800000000000005e-05,0.0,0.0001553999999999,1.0,0.0002064
mmlu-professional-psychology.val.553,claude-v2,1.0,0.001,1.0,3.72e-05,1.0,0.0001,1.0,0.001,1.0,0.001,1.0,0.000124,1.0,0.00125,0.0,9.6224e-05,0.0,0.0001116,1.0,2.4800000000000003e-05,1.0,7.44e-05,1.0,9.92e-05
arc-challenge.test.97,C,0.0,0.0,0.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.7e-05,1.0,0.00086,0.0,6.596e-05,1.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
grade-school-math.dev.4366,meta/llama-2-70b-chat,0.75,0.0005840999999999,0.5,0.0001422,0.75,0.0006056,0.75,0.0048319999999999,0.5,0.006056,0.75,0.000439,0.75,0.0067899999999999,0.25,0.000269272,0.75,0.0005840999999999,0.5,7.6e-05,0.5,0.0002657999999999,0.5,0.0003528
mmlu-anatomy.val.42,Model C - Femur,0.0,0.0,0.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
grade-school-math.dev.6315,meta/llama-2-70b-chat,0.0,0.0004437,0.75,0.0001458,0.0,0.0002856,0.75,0.003744,0.0,0.002688,0.0,0.000377,0.75,0.00624,0.75,0.000245992,0.0,0.0004437,0.25,5.9600000000000005e-05,0.25,0.0001938,0.25,0.0002464
mmlu-professional-law.val.1039,claude-v2,0.0,0.00208,1.0,7.769999999999999e-05,0.0,0.000208,0.0,0.00208,0.0,0.00208,1.0,0.000259,1.0,0.0026,0.0,0.000200984,0.0,0.0002331,1.0,5.1800000000000005e-05,1.0,0.0001547999999999,0.0,0.0002064
mbpp.dev.298,meta/llama-2-70b-chat,0.0,0.0002655,0.0,8.340000000000001e-05,0.0,0.0005408,0.0,0.00728,0.0,0.006512,0.0,0.000491,0.0,0.01249,1.0,0.00017072,0.0,0.0002655,0.0,5.86e-05,0.0,0.0001656,0.0,0.0003168
grade-school-math.dev.2817,meta/llama-2-70b-chat,0.25,0.0004419,0.25,0.0001725,0.75,0.0006896,0.75,0.005456,0.75,0.006032,0.25,0.000645,0.75,0.01129,0.25,0.000441544,0.25,0.0004419,0.75,8.74e-05,0.25,0.0002592,0.25,0.0004776
grade-school-math.dev.4728,meta/llama-2-70b-chat,0.75,0.0003888,0.25,0.0001479,0.75,0.0005104,0.75,0.00508,0.75,0.005944,0.75,0.000518,0.75,0.00761,0.5,0.00030652,0.75,0.0003888,0.25,7.82e-05,0.25,0.0002441999999999,0.75,0.0003456
mmlu-formal-logic.val.34,A) WizardLM/WizardLM-13B-V1.2,0.0,0.0,0.0,2.64e-05,0.0,7.12e-05,0.0,0.000712,0.0,0.000712,0.0,9e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,0.0,6.96e-05
mmlu-professional-law.val.534,claude-v2,1.0,0.001984,0.0,7.41e-05,1.0,0.0001984,1.0,0.001984,1.0,0.001984,1.0,0.000247,0.0,0.00248,0.0,0.000191672,0.0,0.0002222999999999,0.0,4.94e-05,1.0,0.0001482,1.0,0.0001967999999999
hellaswag.val.2353,D) meta/llama-2-70b-chat,0.0,0.0,0.0,3.78e-05,0.0,0.0001016,1.0,0.001016,1.0,0.001016,1.0,0.000126,1.0,0.0013,0.0,9.7776e-05,0.0,0.0001125,1.0,2.52e-05,1.0,7.56e-05,1.0,0.0001
winogrande.dev.465,Model B,0.0,0.0,0.0,1.4699999999999998e-05,0.0,4e-05,0.0,0.0003999999999999,0.0,0.0003999999999999,1.0,4.9e-05,1.0,0.00053,1.0,3.8024e-05,0.0,4.410000000000001e-05,1.0,9.8e-06,1.0,2.94e-05,1.0,3.84e-05
mmlu-college-physics.val.88,Model C - claude-v2,0.0,0.0,0.0,3.75e-05,0.0,0.0001008,0.0,0.001008,0.0,0.001008,0.0,0.000125,0.0,0.00126,0.0,9.7e-05,0.0,0.0001125,1.0,2.5e-05,0.0,7.5e-05,0.0,0.0001
mmlu-professional-psychology.val.135,Model C,0.0,0.0,0.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.840000000000001e-05
hellaswag.val.3462,Model C,0.0,0.0,0.0,6.689999999999999e-05,0.0,0.0001824,0.0,0.0018,0.0,0.001824,0.0,0.000226,0.0,0.00228,0.0,0.0001738239999999,1.0,0.0002007,0.0,4.480000000000001e-05,0.0,0.0001344,0.0,0.0001784
grade-school-math.dev.96,meta/llama-2-70b-chat,0.5,0.0003924,0.75,0.0001578,0.5,0.0006655999999999,0.75,0.0049759999999999,0.75,0.005648,0.75,0.0005139999999999,0.75,0.00748,0.5,0.000328248,0.5,0.0003924,0.25,7.72e-05,0.75,0.0003017999999999,0.5,0.0003888
grade-school-math.dev.678,meta/llama-2-70b-chat,0.75,0.0003591,0.75,0.0001725,0.75,0.0005424,0.75,0.0047519999999999,0.75,0.005664,0.75,0.0005,0.75,0.00855,0.25,0.000369376,0.75,0.0003591,0.75,8.82e-05,0.75,0.0002826,0.25,0.0005216
hellaswag.val.1304,Model C,0.0,0.0,0.0,2.85e-05,0.0,7.680000000000001e-05,1.0,0.000768,0.0,0.000768,0.0,9.5e-05,0.0,0.00096,0.0,7.372e-05,1.0,8.55e-05,0.0,1.9e-05,0.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-high-school-statistics.val.142,D) gpt-4-1106-preview,0.0,0.0,0.0,5.88e-05,0.0,0.0001576,0.0,0.001576,0.0,0.001576,0.0,0.000196,0.0,0.00197,0.0,0.000152096,0.0,0.0001763999999999,1.0,3.92e-05,0.0,0.0001176,0.0,0.0001568
hellaswag.val.460,claude-v2,0.0,0.000712,0.0,2.64e-05,0.0,7.12e-05,1.0,0.000712,0.0,0.000712,0.0,9e-05,0.0,0.00089,0.0,6.828800000000001e-05,0.0,7.83e-05,1.0,1.7599999999999998e-05,0.0,5.28e-05,0.0,6.96e-05
arc-challenge.test.626,Model C,0.0,0.0,1.0,3.8700000000000006e-05,1.0,0.000104,1.0,0.00104,1.0,0.00104,1.0,0.0001309999999999,1.0,0.0013,0.0,0.000100104,1.0,0.0001161,0.0,2.58e-05,1.0,7.740000000000001e-05,1.0,0.0001024
mmlu-security-studies.val.87,B,0.0,0.0,1.0,7.38e-05,1.0,0.0001976,0.0,0.001976,0.0,0.001976,1.0,0.000246,1.0,0.00247,0.0,0.0001908959999999,0.0,0.0002214,0.0,4.920000000000001e-05,1.0,0.0001476,0.0,0.0001968
mmlu-high-school-mathematics.val.134,Model C - claude-v2,0.0,0.0,0.0,3.75e-05,0.0,0.0001008,0.0,0.001008,0.0,0.001008,0.0,0.000127,0.0,0.00129,0.0,9.7e-05,0.0,0.0001125,1.0,2.5e-05,0.0,7.5e-05,0.0,9.92e-05
mmlu-moral-scenarios.val.813,"C) Wrong, Not wrong",0.0,0.0,0.0,4.5e-05,0.0,0.0001208,1.0,0.0012079999999999,0.0,0.0012079999999999,0.0,0.0001519999999999,1.0,0.00151,0.0,0.0001164,0.0,0.000135,0.0,3e-05,1.0,9e-05,0.0,0.0001192
hellaswag.val.9828,Model C,0.0,0.0,1.0,8.52e-05,1.0,0.0002304,1.0,0.00228,1.0,0.00228,0.0,0.000286,1.0,0.00285,1.0,0.0002203839999999,0.0,0.0002556,1.0,5.680000000000001e-05,1.0,0.0001704,1.0,0.0002264
hellaswag.val.8423,C,0.0,0.0,0.0,7.8e-05,1.0,0.0002112,1.0,0.002088,1.0,0.002088,1.0,0.0002619999999999,1.0,0.00261,0.0,0.00020176,1.0,0.000234,0.0,5.2e-05,0.0,0.000156,1.0,0.0002072
grade-school-math.dev.2902,meta/llama-2-70b-chat,0.75,0.000396,0.75,0.0001574999999999,0.75,0.0005824,0.75,0.004816,0.75,0.005416,0.5,0.000493,0.75,0.00893,0.75,0.000349976,0.75,0.000396,0.75,8.86e-05,0.75,0.0002357999999999,0.25,0.0002416
abstract2title.test.216,meta/llama-2-70b-chat,1.0,0.0004302,1.0,0.0001155,1.0,0.000368,1.0,0.003584,1.0,0.003776,1.0,0.000401,1.0,0.00451,1.0,0.00029488,1.0,0.0004302,1.0,7.64e-05,1.0,0.0002334,1.0,0.0003088
mmlu-high-school-chemistry.val.129,Model C,0.0,0.0,0.0,6.36e-05,0.0,0.0001704,0.0,0.001704,0.0,0.001704,0.0,0.000212,0.0,0.00213,0.0,0.0001645119999999,0.0,0.0001908,0.0,4.24e-05,0.0,0.0001272,0.0,0.0001687999999999
mmlu-professional-law.val.109,C) meta/llama-2-70b-chat,0.0,0.0,1.0,0.0001004999999999,1.0,0.0002688,1.0,0.002688,1.0,0.002688,1.0,0.000335,1.0,0.00336,0.0,0.00025996,0.0,0.0003014999999999,0.0,6.7e-05,1.0,0.0002009999999999,1.0,0.0002672
mmlu-moral-disputes.val.305,B) claude-v2,0.0,0.0,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,1.0,0.00122,0.0,9.3896e-05,0.0,0.0001089,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
mmlu-professional-law.val.980,claude-v2,0.0,0.001856,0.0,6.93e-05,0.0,0.0001856,0.0,0.001856,0.0,0.001856,1.0,0.000231,0.0,0.00235,0.0,0.000179256,0.0,0.0002078999999999,0.0,4.6200000000000005e-05,0.0,0.0001386,0.0,0.0001848
mmlu-professional-law.val.1362,B,0.0,0.0,0.0,6.51e-05,0.0,0.0001744,0.0,0.0017439999999999,0.0,0.0017439999999999,0.0,0.000217,0.0,0.00218,0.0,0.0001683919999999,0.0,0.0001952999999999,0.0,4.340000000000001e-05,0.0,0.0001302,0.0,0.0001736
hellaswag.val.8338,C,0.0,0.0,0.0,7.89e-05,1.0,0.0002136,0.0,0.002112,1.0,0.002112,0.0,0.000265,1.0,0.00267,0.0,0.000204088,0.0,0.0002358,0.0,5.260000000000001e-05,0.0,0.0001578,1.0,0.0002096
hellaswag.val.5347,B,0.0,0.0,0.0,7.56e-05,1.0,0.0002024,1.0,0.002024,1.0,0.002024,1.0,0.000254,1.0,0.00256,0.0,0.000195552,1.0,0.0002267999999999,0.0,5.0400000000000005e-05,1.0,0.0001512,1.0,0.0002008
hellaswag.val.6754,C) claude-v2,0.0,0.0,0.0,7.529999999999999e-05,1.0,0.0002024,1.0,0.002024,1.0,0.002024,1.0,0.000254,1.0,0.00256,0.0,0.000195552,1.0,0.0002259,0.0,5.0400000000000005e-05,0.0,0.0001512,0.0,0.0002008
hellaswag.val.8525,Model C,0.0,0.0,0.0,7.409999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00249,0.0,0.000192448,0.0,0.0002223,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
mmlu-college-medicine.val.170,Meta/llama-2-70b-chat,0.0,0.0,0.0,3.45e-05,0.0,9.28e-05,0.0,0.000928,0.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,0.0,0.0001035,0.0,2.3e-05,0.0,6.9e-05,0.0,9.2e-05
grade-school-math.dev.2835,meta/llama-2-70b-chat,0.25,0.0003933,0.25,0.0001338,0.75,0.0007256,0.75,0.007568,0.75,0.006056,0.75,0.000678,0.75,0.0102399999999999,0.25,0.000347648,0.25,0.0003933,0.25,9.7e-05,0.75,0.0003048,0.25,0.0004816
grade-school-math.dev.5780,meta/llama-2-70b-chat,0.75,0.0003006,0.75,0.0001203,0.75,0.0004824,0.75,0.004344,0.75,0.006144,0.5,0.000477,0.5,0.00573,0.75,0.000270824,0.75,0.0003006,0.75,8.36e-05,0.75,0.0002268,0.75,0.00036
grade-school-math.dev.4642,meta/llama-2-70b-chat,0.75,0.0003357,0.75,0.0001296,0.75,0.0004912,0.75,0.003688,0.75,0.005248,0.5,0.000446,0.5,0.0059,0.25,0.000316608,0.75,0.0003357,0.5,7.88e-05,0.75,0.0001914,0.75,0.00034
mmlu-virology.val.49,D) Deep pyro sequencing (NGS),0.0,0.0,1.0,2.49e-05,0.0,6.720000000000001e-05,0.0,0.000672,0.0,0.000672,0.0,8.3e-05,0.0,0.0008399999999999,0.0,6.4408e-05,0.0,7.470000000000001e-05,1.0,1.66e-05,0.0,4.98e-05,0.0,6.64e-05
mmlu-elementary-mathematics.val.74,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 1, cost: 0.9
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.5e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.36e-05
mmlu-philosophy.val.61,Model C (meta/llama-2-70b-chat),0.0,0.0,0.0,2.07e-05,1.0,5.6e-05,1.0,0.00056,1.0,0.00056,1.0,6.9e-05,1.0,0.0007,0.0,5.354400000000001e-05,0.0,6.21e-05,0.0,1.38e-05,1.0,4.14e-05,1.0,5.44e-05
hellaswag.val.5476,B,0.0,0.0,0.0,8.699999999999999e-05,1.0,0.0002328,1.0,0.002328,1.0,0.002328,1.0,0.00029,1.0,0.00291,0.0,0.00022504,0.0,0.0002601,0.0,5.78e-05,1.0,0.0001739999999999,1.0,0.0002312
hellaswag.val.19,claude-v2,1.0,0.0011359999999999,0.0,4.2e-05,1.0,0.0001136,1.0,0.0011359999999999,1.0,0.0011359999999999,0.0,0.0001409999999999,1.0,0.00142,0.0,0.000109416,1.0,0.0001269,0.0,2.82e-05,1.0,8.46e-05,0.0,0.000112
mmlu-professional-law.val.160,meta/llama-2-70b-chat,0.0,0.0001467,1.0,4.89e-05,1.0,0.0001312,1.0,0.001312,0.0,0.001312,1.0,0.000165,1.0,0.00167,0.0,0.0001264879999999,0.0,0.0001467,1.0,3.2600000000000006e-05,1.0,9.78e-05,1.0,0.0001296
mmlu-moral-scenarios.val.244,D) WizardLM-13B-V1.2,0.0,0.0,0.0,4.02e-05,1.0,0.000108,0.0,0.00108,1.0,0.00108,0.0,0.000136,1.0,0.00138,0.0,0.000103984,0.0,0.0001206,0.0,2.68e-05,0.0,8.04e-05,0.0,0.0001072
hellaswag.val.4468,Model D,0.0,0.0,0.0,8.85e-05,0.0,0.0002368,0.0,0.002368,0.0,0.002368,1.0,0.000297,1.0,0.00296,0.0,0.0002289199999999,0.0,0.0002655,0.0,5.9e-05,0.0,0.000177,0.0,0.0002352
hellaswag.val.8495,C) claude-v2,0.0,0.0,0.0,8.549999999999999e-05,1.0,0.0002288,0.0,0.002288,1.0,0.002288,1.0,0.000285,0.0,0.00286,0.0,0.00022116,1.0,0.0002565,0.0,5.7e-05,0.0,0.0001709999999999,1.0,0.0002272
grade-school-math.dev.5874,meta/llama-2-70b-chat,0.25,0.0003843,0.75,0.0001458,0.75,0.0004792,0.75,0.00388,0.75,0.00448,0.75,0.000385,0.75,0.0068899999999999,0.75,0.000303416,0.25,0.0003843,0.75,8.520000000000001e-05,0.75,0.0002658,0.75,0.0003592
hellaswag.val.3984,B,0.0,0.0,0.0,7.26e-05,1.0,0.0001944,1.0,0.001944,1.0,0.001944,1.0,0.000242,1.0,0.00243,0.0,0.000187792,1.0,0.0002169,0.0,4.84e-05,0.0,0.0001452,1.0,0.0001928
grade-school-math.dev.2374,meta/llama-2-70b-chat,0.25,0.0004752,0.5,0.0001782,0.25,0.0006536,0.75,0.0062,0.5,0.0071839999999999,0.75,0.00083,0.5,0.01201,0.25,0.000363944,0.25,0.0004752,0.25,8.620000000000001e-05,0.25,0.0003005999999999,0.25,0.0005528
mmlu-clinical-knowledge.val.44,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 0, cost: 0.1
Model D",0.0,0.0,1.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,1.0,2.02e-05,1.0,6.06e-05,1.0,8.08e-05
winogrande.dev.432,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Correct choice: Model A",0.0,0.0,1.0,1.41e-05,1.0,3.84e-05,1.0,0.000384,0.0,0.000384,0.0,4.9000000000000005e-05,1.0,0.00048,1.0,3.6472000000000006e-05,0.0,4.2300000000000005e-05,1.0,9.4e-06,1.0,2.82e-05,1.0,3.68e-05
mmlu-professional-law.val.46,claude-v2,0.0,0.003128,0.0,0.000117,0.0,0.0003128,0.0,0.003128,0.0,0.003128,0.0,0.00039,0.0,0.00391,0.0,0.00030264,0.0,0.0003509999999999,0.0,7.8e-05,0.0,0.000234,1.0,0.0003112
mmlu-high-school-psychology.val.250,claude-v2,1.0,0.000728,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,1.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
mmlu-prehistory.val.62,C,0.0,0.0,1.0,5.4e-05,1.0,0.0001448,1.0,0.0014479999999999,1.0,0.0014479999999999,1.0,0.0001799999999999,1.0,0.00181,0.0,0.00013968,0.0,0.000162,0.0,3.600000000000001e-05,1.0,0.000108,1.0,0.0001432
grade-school-math.dev.1518,meta/llama-2-70b-chat,0.25,0.0003672,0.75,0.0001661999999999,0.25,0.0006304,0.25,0.005128,0.75,0.006736,0.75,0.0004089999999999,0.75,0.01286,0.75,0.00032592,0.25,0.0003672,0.25,8.54e-05,0.5,0.0002484,0.25,0.000424
hellaswag.val.1461,claude-v2,1.0,0.000736,1.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.00095,1.0,7.0616e-05,0.0,8.190000000000001e-05,1.0,1.82e-05,0.0,5.46e-05,1.0,7.200000000000002e-05
mmlu-jurisprudence.val.83,Model C,0.0,0.0,0.0,2.97e-05,0.0,8e-05,0.0,0.0008,1.0,0.0008,0.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,0.0,1.98e-05,0.0,5.94e-05,0.0,7.840000000000001e-05
hellaswag.val.7492,C) claude-v2,0.0,0.0,1.0,8.01e-05,1.0,0.0002152,0.0,0.002152,1.0,0.002152,0.0,0.00027,1.0,0.00272,0.0,0.0002079679999999,1.0,0.0002412,1.0,5.360000000000001e-05,1.0,0.0001608,1.0,0.0002136
chinese_shi_jing.test.25,meta/llama-2-70b-chat,0.0,0.0001116,0.0,2.46e-05,0.0,8.72e-05,0.0,0.000752,0.0,0.000704,0.0,9.6e-05,0.0,0.00094,0.0,7.527200000000001e-05,0.0,0.0001116,0.0,1.56e-05,0.0,4.74e-05,0.0,6.64e-05
winogrande.dev.652,Model B,0.0,0.0,0.0,1.53e-05,1.0,4.16e-05,1.0,0.000416,1.0,0.000416,1.0,5.3e-05,1.0,0.00055,1.0,3.9576e-05,0.0,4.59e-05,1.0,1.02e-05,1.0,3.06e-05,1.0,4e-05
winogrande.dev.348,Model B,0.0,0.0,0.0,1.47e-05,1.0,4.08e-05,1.0,0.000408,1.0,0.000408,1.0,5.2e-05,1.0,0.00054,1.0,3.880000000000001e-05,0.0,4.41e-05,1.0,1e-05,1.0,3e-05,1.0,3.92e-05
chinese_zodiac.dev.109,meta/llama-2-70b-chat,0.0,0.0001143,0.0,3.33e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,0.0,0.000113,0.0,0.00115,0.0,9.4672e-05,0.0,0.0001143,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,8.960000000000001e-05
hellaswag.val.3822,Model D,0.0,0.0,1.0,6.659999999999999e-05,1.0,0.0001816,0.0,0.001792,1.0,0.001792,1.0,0.000225,1.0,0.00224,0.0,0.000173048,0.0,0.0001998,0.0,4.460000000000001e-05,1.0,0.0001338,1.0,0.0001776
hellaswag.val.8245,Model B,0.0,0.0,1.0,6.659999999999999e-05,1.0,0.0001792,1.0,0.001792,1.0,0.001792,1.0,0.000223,1.0,0.00227,0.0,0.000173048,1.0,0.0001998,0.0,4.460000000000001e-05,1.0,0.0001338,1.0,0.0001776
mmlu-professional-law.val.1415,meta/llama-2-70b-chat,0.0,0.0001368,1.0,4.56e-05,1.0,0.0001224,1.0,0.0012239999999999,1.0,0.0012239999999999,1.0,0.0001539999999999,1.0,0.00156,0.0,0.000117952,0.0,0.0001368,1.0,3.04e-05,1.0,9.12e-05,1.0,0.0001208
hellaswag.val.1965,claude-v2,1.0,0.00072,0.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,9.1e-05,1.0,0.0009,0.0,6.9064e-05,0.0,7.919999999999999e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
hellaswag.val.3410,A,0.0,0.0,1.0,7.439999999999999e-05,1.0,0.0002016,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00252,1.0,0.000192448,1.0,0.0002223,1.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
hellaswag.val.3048,Model C,0.0,0.0,0.0,3e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9e-05,0.0,2.02e-05,0.0,6.06e-05,0.0,8e-05
arc-challenge.test.367,Model C (2 m/s),0.0,0.0,1.0,2.19e-05,0.0,5.92e-05,1.0,0.000592,0.0,0.000592,0.0,7.3e-05,0.0,0.00074,1.0,5.6648e-05,1.0,6.57e-05,1.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
mmlu-logical-fallacies.val.34,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.31e-05,0.0,6.24e-05,1.0,0.000624,0.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,0.0,6.16e-05
hellaswag.val.6678,claude-v2,0.0,0.002144,0.0,7.979999999999999e-05,1.0,0.0002144,1.0,0.002144,0.0,0.002144,1.0,0.000269,1.0,0.00268,1.0,0.000207192,1.0,0.0002402999999999,1.0,5.34e-05,1.0,0.0001602,1.0,0.0002128
mmlu-high-school-world-history.val.8,claude-v2,1.0,0.003008,1.0,0.0001125,1.0,0.0003008,1.0,0.003008,1.0,0.003008,1.0,0.000375,1.0,0.00376,0.0,0.000291,0.0,0.0003374999999999,1.0,7.500000000000001e-05,1.0,0.000225,1.0,0.0002992
mmlu-professional-medicine.val.180,D) CT scan of the chest,0.0,0.0,0.0,6.12e-05,0.0,0.000164,0.0,0.00164,0.0,0.00164,0.0,0.000206,1.0,0.00208,0.0,0.000158304,0.0,0.0001836,1.0,4.080000000000001e-05,1.0,0.0001224,0.0,0.0001624
grade-school-math.dev.7182,meta/llama-2-70b-chat,0.25,0.0003645,0.25,0.0001581,0.25,0.0005528,0.25,0.005888,0.25,0.007016,0.25,0.000541,0.25,0.00832,0.25,0.000313504,0.25,0.0003645,0.25,0.0005562,0.25,0.0002274,0.25,0.0003528
bias_detection.dev.239,"meta/llama-2-70b-chat

Reasoning: This model is best suited because it is designed for understanding and classifying complex information, such as distinguishing between fact, opinion, claim, data, quote",0.0,0.0,0.0,0.0001152,0.0,0.0003672,0.0,0.00468,0.0,0.006024,0.0,0.0004039999999999,0.0,0.01164,0.0,0.000262288,0.0,0.0004473,0.0,6.220000000000001e-05,0.0,0.0002568,0.0,0.000356
grade-school-math.dev.4583,meta/llama-2-70b-chat,0.75,0.0004904999999999,0.25,0.000192,0.25,0.0006704,0.75,0.00596,0.25,0.005552,0.75,0.000757,0.75,0.01132,0.25,0.000428352,0.75,0.0004904999999999,0.75,0.000114,0.25,0.0003306,0.75,0.0005232
mmlu-conceptual-physics.val.193,Model B,0.0,0.0,1.0,2.13e-05,1.0,5.76e-05,1.0,0.000576,1.0,0.000576,1.0,7.099999999999999e-05,1.0,0.0007199999999999,0.0,5.5096e-05,0.0,6.390000000000001e-05,0.0,1.42e-05,1.0,4.26e-05,1.0,5.68e-05
winogrande.dev.1059,Model B,0.0,0.0,0.0,1.41e-05,1.0,3.84e-05,1.0,0.000384,1.0,0.000384,0.0,4.9000000000000005e-05,1.0,0.00048,0.0,3.6472000000000006e-05,1.0,4.2300000000000005e-05,0.0,9.4e-06,0.0,2.82e-05,0.0,3.68e-05
mmlu-high-school-physics.val.105,Model C,0.0,0.0,0.0,4.74e-05,1.0,0.0001272,1.0,0.001272,1.0,0.001272,1.0,0.000158,1.0,0.00159,0.0,0.000122608,0.0,0.0001422,0.0,3.160000000000001e-05,1.0,9.48e-05,1.0,0.0001256
mmlu-moral-disputes.val.73,B) consequentialist theory,0.0,0.0,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
mbpp.dev.123,meta/llama-2-70b-chat,0.0,0.0005049,0.0,0.0001460999999999,1.0,0.0008784,1.0,0.0088799999999999,1.0,0.0101999999999999,1.0,0.000719,1.0,0.01842,0.0,0.000176928,0.0,0.0005049,0.0,9.18e-05,1.0,0.0002795999999999,0.0,0.0004616
mmlu-miscellaneous.val.64,claude-v2,1.0,0.000784,0.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,0.0,9.9e-05,1.0,0.00098,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.76e-05
mmlu-professional-law.val.101,meta/llama-2-70b-chat,0.0,0.0001791,0.0,5.97e-05,1.0,0.00016,0.0,0.0015999999999999,1.0,0.0015999999999999,1.0,0.0001989999999999,0.0,0.002,0.0,0.000154424,0.0,0.0001791,0.0,3.980000000000001e-05,0.0,0.0001193999999999,0.0,0.0001584
hellaswag.val.4837,claude-v2,1.0,0.002032,0.0,7.59e-05,1.0,0.0002032,1.0,0.002032,1.0,0.002032,1.0,0.0002549999999999,1.0,0.00257,0.0,0.000196328,1.0,0.0002268,0.0,5.06e-05,0.0,0.0001518,1.0,0.0002016
hellaswag.val.429,claude-v2,0.0,0.000944,0.0,3.51e-05,0.0,9.44e-05,0.0,0.000944,0.0,0.000944,0.0,0.000117,0.0,0.00118,1.0,9.0792e-05,0.0,0.0001043999999999,1.0,2.34e-05,1.0,7.02e-05,0.0,9.28e-05
hellaswag.val.9856,B,0.0,0.0,1.0,7.74e-05,1.0,0.0002072,1.0,0.002072,1.0,0.002072,0.0,0.000258,1.0,0.00262,1.0,0.000200208,1.0,0.0002313,1.0,5.160000000000001e-05,1.0,0.0001548,1.0,0.0002056
hellaswag.val.5030,B,0.0,0.0,0.0,7.5e-05,1.0,0.0002032,0.0,0.002008,1.0,0.002008,1.0,0.000252,1.0,0.00254,0.0,0.000194,1.0,0.0002241,0.0,5e-05,0.0,0.00015,1.0,0.0001992
mmlu-security-studies.val.129,D) meta/llama-2-70b-chat,0.0,0.0,1.0,5.34e-05,1.0,0.0001432,1.0,0.001432,1.0,0.001432,1.0,0.000178,1.0,0.00179,0.0,0.0001381279999999,0.0,0.0001602,0.0,3.5600000000000005e-05,1.0,0.0001068,1.0,0.0001416
hellaswag.val.1447,B,0.0,0.0,1.0,6.03e-05,1.0,0.0001616,1.0,0.001616,1.0,0.001616,1.0,0.000203,1.0,0.00202,0.0,0.000155976,1.0,0.0001799999999999,0.0,4.020000000000001e-05,1.0,0.0001205999999999,1.0,0.00016
grade-school-math.dev.3918,meta/llama-2-70b-chat,0.25,0.0004293,0.25,0.0001923,0.5,0.0007208,0.75,0.005696,0.5,0.00464,0.5,0.000639,0.5,0.00853,0.25,0.000365496,0.25,0.0004293,0.25,0.0001206,0.25,0.0003131999999999,0.25,0.0004408
arc-challenge.test.948,Model B,0.0,0.0,0.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,0.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,1.0,9e-05,0.0,2.02e-05,1.0,6.06e-05,1.0,8e-05
mmlu-philosophy.val.121,Model D,0.0,0.0,1.0,3.27e-05,1.0,8.800000000000001e-05,1.0,0.00088,0.0,0.00088,1.0,0.0001089999999999,1.0,0.0011,0.0,8.4584e-05,0.0,9.81e-05,1.0,2.18e-05,1.0,6.54e-05,1.0,8.72e-05
mmlu-sociology.val.58,Model C,0.0,0.0,1.0,2.82e-05,1.0,7.600000000000002e-05,1.0,0.00076,1.0,0.00076,1.0,9.4e-05,1.0,0.00095,0.0,7.2944e-05,0.0,8.46e-05,0.0,1.8800000000000003e-05,1.0,5.64e-05,1.0,7.440000000000002e-05
mbpp.dev.360,meta/llama-2-70b-chat,0.0,0.0002943,0.0,5.879999999999999e-05,0.0,0.0004328,0.0,0.005192,0.0,0.005576,0.0,0.000161,1.0,0.01357,0.0,0.00026384,0.0,0.0002943,0.0,4.100000000000001e-05,0.0,0.0002682,0.0,0.0003632
mmlu-professional-law.val.288,meta/llama-2-70b-chat,0.0,0.0002583,1.0,8.61e-05,0.0,0.0002304,0.0,0.002304,1.0,0.002304,1.0,0.000287,0.0,0.00288,0.0,0.000222712,0.0,0.0002583,0.0,5.7400000000000006e-05,0.0,0.0001722,1.0,0.0002296
mmlu-professional-law.val.657,meta/llama-2-70b-chat,0.0,0.0003096,0.0,0.0001032,1.0,0.000276,1.0,0.00276,1.0,0.00276,1.0,0.000344,1.0,0.00345,0.0,0.0002669439999999,0.0,0.0003096,0.0,6.88e-05,0.0,0.0002064,1.0,0.0002744
hellaswag.val.1326,B,0.0,0.0,1.0,3.96e-05,1.0,0.0001064,0.0,0.0010639999999999,1.0,0.0010639999999999,1.0,0.0001319999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001178999999999,0.0,2.64e-05,1.0,7.92e-05,0.0,0.0001048
hellaswag.val.3344,Model D,0.0,0.0,0.0,8.699999999999999e-05,0.0,0.0002352,0.0,0.002328,1.0,0.002328,1.0,0.00029,1.0,0.00291,0.0,0.00022504,1.0,0.0002601,0.0,5.800000000000001e-05,0.0,0.0001739999999999,1.0,0.0002312
mmlu-conceptual-physics.val.65,Model D,0.0,0.0,0.0,2.61e-05,0.0,7.04e-05,0.0,0.000704,0.0,0.000704,1.0,8.7e-05,0.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,0.0,5.22e-05,1.0,6.88e-05
winogrande.dev.121,Model B,0.0,0.0,1.0,1.7100000000000002e-05,0.0,4.64e-05,0.0,0.000464,1.0,0.000464,0.0,5.9e-05,1.0,0.00061,0.0,4.4232e-05,1.0,5.04e-05,0.0,1.14e-05,0.0,3.4200000000000005e-05,1.0,4.56e-05
grade-school-math.dev.1140,meta/llama-2-70b-chat,0.25,0.0003357,0.25,0.0001674,0.75,0.0007176,0.75,0.005592,0.75,0.007512,0.25,0.000531,0.75,0.01221,0.25,0.000273928,0.25,0.0003357,0.25,9.78e-05,0.25,0.000285,0.25,0.0004288
grade-school-math.dev.1567,meta/llama-2-70b-chat,0.25,0.0004284,0.25,0.0001539,0.75,0.0007447999999999,0.75,0.006248,0.75,0.006176,0.75,0.000638,0.75,0.01411,0.25,0.00041904,0.25,0.0004284,0.25,0.0001012,0.25,0.000381,0.25,0.0004424
mmlu-professional-law.val.1350,claude-v2,0.0,0.001584,0.0,5.91e-05,0.0,0.0001584,0.0,0.001584,0.0,0.001584,0.0,0.000197,0.0,0.00198,0.0,0.0001528719999999,0.0,0.0001773,0.0,3.94e-05,1.0,0.0001182,0.0,0.0001568
hellaswag.val.4342,claude-v2,1.0,0.001856,1.0,6.93e-05,0.0,0.000188,1.0,0.001856,1.0,0.001856,0.0,0.000233,1.0,0.00235,1.0,0.000179256,1.0,0.0002078999999999,1.0,4.6200000000000005e-05,1.0,0.0001386,1.0,0.000184
hellaswag.val.4889,B,0.0,0.0,0.0,8.31e-05,0.0,0.0002224,0.0,0.002224,0.0,0.002224,1.0,0.000279,0.0,0.00281,0.0,0.000214952,0.0,0.0002493,0.0,5.5400000000000005e-05,0.0,0.0001662,0.0,0.0002208
winogrande.dev.706,Model B,0.0,0.0,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,1.0,0.000392,1.0,5e-05,1.0,0.00049,0.0,3.7248e-05,1.0,4.23e-05,0.0,9.6e-06,0.0,2.8799999999999995e-05,1.0,3.8400000000000005e-05
arc-challenge.test.2,Model C,0.0,0.0,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00102,1.0,7.604800000000001e-05,1.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
mmlu-international-law.val.65,claude-v2,0.0,0.001112,1.0,4.14e-05,0.0,0.0001112,0.0,0.001112,0.0,0.001112,0.0,0.000138,1.0,0.00139,0.0,0.000107088,0.0,0.0001241999999999,1.0,2.7600000000000003e-05,1.0,8.28e-05,1.0,0.0001104
mmlu-security-studies.val.218,C,0.0,0.0,0.0,8.91e-05,1.0,0.0002384,1.0,0.002384,1.0,0.002384,1.0,0.000297,1.0,0.00298,0.0,0.000230472,0.0,0.0002673,0.0,5.94e-05,1.0,0.0001782,1.0,0.0002367999999999
mmlu-professional-medicine.val.44,meta/llama-2-70b-chat,0.0,0.0001521,0.0,5.07e-05,1.0,0.000136,0.0,0.0013599999999999,1.0,0.0013599999999999,0.0,0.000169,1.0,0.0017,0.0,0.000131144,0.0,0.0001521,0.0,3.38e-05,0.0,0.0001014,0.0,0.0001344
hellaswag.val.1102,claude-v2,1.0,0.000728,1.0,2.67e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,1.0,6.984e-05,1.0,8.1e-05,1.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
abstract2title.test.62,meta/llama-2-70b-chat,1.0,0.0002862,1.0,5.58e-05,1.0,0.000172,1.0,0.0021279999999999,1.0,0.0020559999999999,1.0,0.000195,1.0,0.00245,1.0,0.000135024,1.0,0.0002862,1.0,3.48e-05,1.0,0.0001133999999999,1.0,0.000148
mmlu-marketing.val.100,C) Pay per click (PPC),0.0,0.0,1.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
hellaswag.val.1894,D) meta/llama-2-70b-chat,0.0,0.0,0.0,3.3e-05,0.0,8.960000000000001e-05,0.0,0.000896,0.0,0.000896,1.0,0.000111,1.0,0.00112,0.0,8.6136e-05,0.0,9.990000000000002e-05,0.0,2.22e-05,0.0,6.659999999999999e-05,0.0,8.800000000000001e-05
grade-school-math.dev.2479,meta/llama-2-70b-chat,0.75,0.0003663,0.75,0.0001584,0.75,0.0005903999999999,0.75,0.004392,0.75,0.005472,0.75,0.000552,0.75,0.00804,0.25,0.00041516,0.75,0.0003663,0.75,7.98e-05,0.25,0.000276,0.75,0.0004032
mmlu-professional-law.val.564,claude-v2,1.0,0.002416,0.0,9.03e-05,0.0,0.0002416,1.0,0.002416,1.0,0.002416,1.0,0.000301,0.0,0.00302,0.0,0.000233576,0.0,0.0002709,0.0,6.0200000000000006e-05,0.0,0.0001806,0.0,0.00024
mmlu-moral-scenarios.val.760,"C) Not wrong, Wrong",0.0,0.0,1.0,4.38e-05,1.0,0.0001176,1.0,0.001176,1.0,0.001176,1.0,0.000146,1.0,0.00147,0.0,0.000113296,0.0,0.0001314,0.0,2.92e-05,0.0,8.759999999999999e-05,1.0,0.000116
abstract2title.test.105,meta/llama-2-70b-chat,1.0,0.0004158,1.0,6.69e-05,1.0,0.0002175999999999,1.0,0.002008,1.0,0.002416,1.0,0.00022,1.0,0.00272,1.0,0.000168392,1.0,0.0004158,1.0,4.08e-05,1.0,0.0001326,1.0,0.0001631999999999
hellaswag.val.9502,A or B,0.0,0.0,0.0,7.35e-05,0.0,0.0001976,0.0,0.001976,0.0,0.001976,0.0,0.000248,0.0,0.0025,0.0,0.0001908959999999,0.0,0.0002214,0.0,4.920000000000001e-05,0.0,0.0001476,0.0,0.000196
mmlu-security-studies.val.156,B,0.0,0.0,1.0,6.9e-05,1.0,0.0001848,1.0,0.001848,0.0,0.001848,1.0,0.00023,1.0,0.00231,0.0,0.0001784799999999,0.0,0.000207,0.0,4.600000000000001e-05,1.0,0.000138,1.0,0.000184
mmlu-moral-scenarios.val.626,"B) Not wrong, Wrong",0.0,0.0,1.0,4.05e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,1.0,0.000135,1.0,0.00139,0.0,0.0001047599999999,0.0,0.0001215,0.0,2.7e-05,1.0,8.1e-05,1.0,0.0001072
hellaswag.val.6255,B,0.0,0.0,0.0,6.78e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.001848,1.0,0.000227,1.0,0.00228,0.0,0.0001761519999999,1.0,0.0002033999999999,0.0,4.5400000000000006e-05,0.0,0.0001362,1.0,0.0001808
mmlu-high-school-world-history.val.173,meta/llama-2-70b-chat,0.0,0.0004986,0.0,0.0001661999999999,1.0,0.000444,1.0,0.0044399999999999,1.0,0.0044399999999999,1.0,0.000554,1.0,0.0055499999999999,0.0,0.000429904,0.0,0.0004986,0.0,0.0001108,1.0,0.0003323999999999,1.0,0.0004424
mmlu-miscellaneous.val.637,claude-v2,0.0,0.0006,1.0,2.22e-05,0.0,6e-05,0.0,0.0006,0.0,0.0006,0.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,1.0,1.48e-05,0.0,4.44e-05,0.0,5.92e-05
hellaswag.val.4051,C) claude-v2,0.0,0.0,0.0,7.38e-05,1.0,0.0001976,1.0,0.001976,1.0,0.001976,1.0,0.000248,1.0,0.0025,0.0,0.0001908959999999,0.0,0.0002214,0.0,4.920000000000001e-05,0.0,0.0001476,1.0,0.000196
grade-school-math.dev.4084,meta/llama-2-70b-chat,0.25,0.0004581,0.75,0.00015,0.75,0.0005055999999999,0.75,0.004456,0.5,0.005464,0.75,0.000447,0.75,0.00677,0.75,0.000460944,0.25,0.0004581,0.25,8.9e-05,0.5,0.000261,0.25,0.0002256
mmlu-machine-learning.val.27,Model C,0.0,0.0,0.0,2.22e-05,0.0,6e-05,0.0,0.0006,0.0,0.0006,0.0,7.4e-05,0.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,0.0,4.44e-05,0.0,5.92e-05
hellaswag.val.7254,Model C,0.0,0.0,0.0,7.56e-05,1.0,0.0002032,0.0,0.002032,0.0,0.002032,0.0,0.0002549999999999,1.0,0.00257,0.0,0.000196328,0.0,0.0002268,0.0,5.06e-05,0.0,0.0001518,0.0,0.0002016
mmlu-clinical-knowledge.val.119,"Model A - correctness: 1, cost: 0.8",0.0,0.0,1.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.0011899999999999,0.0,8.924e-05,0.0,0.0001035,0.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
mmlu-moral-disputes.val.332,Model C,0.0,0.0,0.0,3.3600000000000004e-05,1.0,9.04e-05,0.0,0.000904,0.0,0.000904,1.0,0.000112,0.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,1.0,2.24e-05,0.0,6.720000000000001e-05,1.0,8.96e-05
mmlu-high-school-government-and-politics.val.24,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
hellaswag.val.2290,Model D,0.0,0.0,0.0,3.69e-05,1.0,9.92e-05,0.0,0.000992,0.0,0.000992,1.0,0.000125,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001098,0.0,2.46e-05,0.0,7.379999999999999e-05,0.0,9.76e-05
grade-school-math.dev.7422,meta/llama-2-70b-chat,0.25,0.0005013,0.25,0.0001716,0.75,0.000856,0.25,0.005968,0.75,0.005944,0.75,0.000698,0.75,0.0096499999999999,0.5,0.000523024,0.25,0.0005013,0.25,8.999999999999999e-05,0.25,0.0002904,0.75,0.0005464
mmlu-abstract-algebra.val.99,C) meta/llama-2-70b-chat,0.0,0.0,0.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.9e-05,0.0,0.00098,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,0.0,5.8200000000000005e-05,1.0,7.76e-05
mmlu-clinical-knowledge.val.171,Model D - Alcohol,0.0,0.0,1.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.9e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
mmlu-human-sexuality.val.103,Model D,0.0,0.0,0.0,4.17e-05,0.0,0.000112,0.0,0.00112,0.0,0.001144,1.0,0.000139,1.0,0.0014,0.0,0.000107864,0.0,0.0001250999999999,0.0,2.78e-05,1.0,8.340000000000001e-05,0.0,0.0001112
hellaswag.val.7714,Model C,0.0,0.0,0.0,7.95e-05,0.0,0.0002152,0.0,0.002128,0.0,0.002128,1.0,0.000267,1.0,0.00266,0.0,0.00020564,0.0,0.0002385,0.0,5.300000000000001e-05,0.0,0.000159,1.0,0.0002112
hellaswag.val.5396,C) meta/llama-2-70b-chat,0.0,0.0,1.0,7.859999999999999e-05,1.0,0.0002112,1.0,0.002112,0.0,0.002112,0.0,0.000265,1.0,0.00267,1.0,0.000204088,0.0,0.0002358,1.0,5.260000000000001e-05,1.0,0.0001578,1.0,0.0002096
hellaswag.val.3840,C,0.0,0.0,0.0,8.4e-05,1.0,0.000228,1.0,0.002256,0.0,0.00228,1.0,0.000283,1.0,0.00285,0.0,0.000218056,1.0,0.000252,0.0,5.62e-05,0.0,0.0001686,1.0,0.000224
mmlu-professional-law.val.1034,meta/llama-2-70b-chat,0.0,0.0002736,0.0,9.12e-05,0.0,0.000244,0.0,0.00244,0.0,0.00244,0.0,0.000304,0.0,0.00305,0.0,0.000235904,0.0,0.0002736,0.0,6.080000000000001e-05,0.0,0.0001817999999999,0.0,0.0002432
grade-school-math.dev.2592,meta/llama-2-70b-chat,0.75,0.0004122,0.5,0.0001569,0.75,0.0006152,0.75,0.004904,0.75,0.006512,0.75,0.000632,0.75,0.01045,0.25,0.0003608399999999,0.75,0.0004122,0.25,0.0001078,0.25,0.000279,0.75,0.0004888
arc-challenge.test.815,Model C,0.0,0.0,1.0,1.89e-05,1.0,5.12e-05,1.0,0.000512,1.0,0.000512,1.0,6.3e-05,1.0,0.00067,0.0,4.8888e-05,1.0,5.58e-05,0.0,1.26e-05,0.0,3.78e-05,1.0,4.9600000000000006e-05
chinese_zodiac.dev.122,meta/llama-2-70b-chat,0.0,0.0001152,0.0,3.3600000000000004e-05,0.0,9.28e-05,0.0,0.000928,0.0,0.000928,1.0,0.000114,1.0,0.00116,0.0,0.0001156239999999,0.0,0.0001152,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,9.28e-05
mmlu-high-school-statistics.val.54,Model C,0.0,0.0,0.0,3.06e-05,0.0,8.240000000000001e-05,1.0,0.000824,1.0,0.000824,0.0,0.000102,1.0,0.00103,0.0,7.9152e-05,0.0,9.18e-05,0.0,2.04e-05,0.0,6.12e-05,1.0,8.16e-05
mmlu-high-school-geography.val.170,B) claude-v2,0.0,0.0,1.0,2.3400000000000003e-05,1.0,6.32e-05,1.0,0.000632,1.0,0.000632,1.0,8e-05,1.0,0.00082,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,1.0,6.16e-05
mmlu-professional-law.val.328,claude-v2,0.0,0.003784,0.0,0.0001416,0.0,0.0003784,0.0,0.003784,0.0,0.003784,0.0,0.000472,0.0,0.00473,0.0,0.000366272,0.0,0.0004247999999999,0.0,9.44e-05,0.0,0.0002832,1.0,0.0003768
mmlu-miscellaneous.val.468,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.25e-05,1.0,6.08e-05,1.0,0.000608,1.0,0.000608,1.0,7.5e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,0.0,6e-05
mmlu-high-school-us-history.val.58,meta/llama-2-70b-chat,0.0,0.0003222,1.0,0.0001074,1.0,0.0002872,1.0,0.002872,0.0,0.002872,1.0,0.000358,1.0,0.00362,0.0,0.000277808,0.0,0.0003222,1.0,7.16e-05,1.0,0.0002148,1.0,0.0002864
grade-school-math.dev.641,meta/llama-2-70b-chat,0.5,0.0003509999999999,0.5,0.0001491,0.75,0.0005488,0.75,0.004264,0.5,0.005488,0.75,0.000504,0.5,0.00689,0.25,0.00029488,0.5,0.0003509999999999,0.25,6.52e-05,0.5,0.0002772,0.5,0.0002464
hellaswag.val.4185,D,0.0,0.0,0.0,8.309999999999999e-05,0.0,0.0002256,0.0,0.002232,0.0,0.002232,1.0,0.00028,1.0,0.00279,0.0,0.000215728,0.0,0.0002493,0.0,5.56e-05,0.0,0.0001668,1.0,0.0002216
hellaswag.val.5494,Model C,0.0,0.0,1.0,6.57e-05,1.0,0.0001784,0.0,0.0017599999999999,1.0,0.0017599999999999,0.0,0.0002209999999999,1.0,0.0022,0.0,0.0001699439999999,0.0,0.0001962,0.0,4.380000000000001e-05,0.0,0.0001314,1.0,0.0001744
mmlu-international-law.val.107,Model B,0.0,0.0,0.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,0.0,0.0001035,0.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
mmlu-high-school-biology.val.122,Model A,0.0,0.0,1.0,4.23e-05,1.0,0.0001136,1.0,0.0011359999999999,0.0,0.0011359999999999,1.0,0.0001409999999999,1.0,0.00142,0.0,0.000109416,0.0,0.0001269,1.0,2.82e-05,1.0,8.46e-05,1.0,0.0001128
hellaswag.val.4973,B,0.0,0.0,1.0,6.269999999999999e-05,0.0,0.0001712,1.0,0.0016879999999999,1.0,0.0016879999999999,0.0,0.0002119999999999,1.0,0.00214,1.0,0.00016296,1.0,0.0001881,1.0,4.2e-05,0.0,0.000126,1.0,0.0001672
grade-school-math.dev.2819,meta/llama-2-70b-chat,0.75,0.0003635999999999,0.25,0.0001437,0.75,0.0004672,0.5,0.004648,0.75,0.005632,0.5,0.000432,0.75,0.0070399999999999,0.25,0.000325144,0.75,0.0003635999999999,0.25,8.26e-05,0.25,0.0002885999999999,0.75,0.000408
chinese_zodiac.dev.195,meta/llama-2-70b-chat,0.0,0.0001314,0.0,3.3600000000000004e-05,0.0,9.28e-05,0.0,0.000928,1.0,0.000928,1.0,0.000114,1.0,0.00116,0.0,0.000114072,0.0,0.0001314,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,9.04e-05
hellaswag.val.3986,D) claude-v2,0.0,0.0,0.0,7.92e-05,0.0,0.0002144,1.0,0.00212,1.0,0.00212,1.0,0.000266,1.0,0.00268,0.0,0.000204864,0.0,0.0002376,0.0,5.280000000000001e-05,0.0,0.0001584,1.0,0.0002104
grade-school-math.dev.3734,meta/llama-2-70b-chat,0.25,0.0003717,0.25,0.0001971,0.25,0.0006944,0.25,0.00608,0.25,0.006704,0.75,0.000752,0.5,0.01081,0.25,0.00030264,0.25,0.0003717,0.25,0.0001154,0.75,0.000324,0.25,0.0004664
grade-school-math.dev.4541,meta/llama-2-70b-chat,0.5,0.0003447,0.25,0.0001305,0.25,0.0004288,0.75,0.004816,0.75,0.0052,0.75,0.0004669999999999,0.5,0.00698,0.75,0.000289448,0.5,0.0003447,0.5,7.2e-05,0.25,0.0002208,0.5,0.0002296
grade-school-math.dev.2309,meta/llama-2-70b-chat,0.25,0.0005454,0.25,0.000192,0.25,0.000824,0.5,0.00692,0.5,0.008936,0.75,0.000739,0.5,0.01234,0.25,0.000385672,0.25,0.0005454,0.25,0.0001218,0.25,0.000345,0.25,0.0004344
grade-school-math.dev.5653,meta/llama-2-70b-chat,0.75,0.0003267,0.75,0.0001293,0.75,0.0004848,0.75,0.0036239999999999,0.75,0.00408,0.75,0.000344,0.5,0.00516,0.5,0.000273152,0.75,0.0003267,0.75,6.1e-05,0.0,0.0001853999999999,0.75,0.000344
grade-school-math.dev.2905,meta/llama-2-70b-chat,0.75,0.0004175999999999,0.75,0.0001508999999999,0.5,0.0007312,0.75,0.00496,0.75,0.006208,0.5,0.000476,0.75,0.00785,0.25,0.000389552,0.75,0.0004175999999999,0.25,7.92e-05,0.75,0.0002322,0.75,0.000412
consensus_summary.dev.233,Model C,0.0,0.0,0.75,6.840000000000001e-05,0.5,0.0001632,1.0,0.0013679999999999,0.5,0.0018,0.5,0.000187,0.75,0.0030299999999999,0.75,0.00018624,0.75,0.0001836,0.75,3.540000000000001e-05,0.75,0.0001284,0.5,0.0001336
mmlu-professional-law.val.1171,claude-v2,1.0,0.001648,1.0,6.149999999999999e-05,1.0,0.0001648,1.0,0.001648,1.0,0.001648,1.0,0.000205,1.0,0.00206,0.0,0.0001590799999999,0.0,0.0001845,0.0,4.100000000000001e-05,1.0,0.0001229999999999,1.0,0.0001632
mmlu-college-mathematics.val.64,Model C) meta/llama-2-70b-chat,0.0,0.0,0.0,2.4e-05,1.0,6.480000000000002e-05,0.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.400000000000001e-05
mtbench-reference.dev.3,Model C (meta/llama-2-70b-chat),0.0,0.0,0.1,0.0002663999999999,0.1,0.000952,0.1,0.01864,0.1,0.012304,0.1,0.000973,1.0,0.0305,0.2,0.000676672,0.1,0.0009495,0.1,9.54e-05,0.1,0.0005543999999999,0.1,0.0012136
chinese_zodiac.dev.373,meta/llama-2-70b-chat,0.0,0.0001251,0.0,3.33e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.000111744,0.0,0.0001251,0.0,2.22e-05,0.0,6.659999999999999e-05,0.0,9.44e-05
grade-school-math.dev.2702,meta/llama-2-70b-chat,0.75,0.0004653,0.5,0.0001494,0.75,0.0006544,0.75,0.0050799999999999,0.75,0.0056799999999999,0.75,0.000538,0.75,0.00758,0.25,0.000294104,0.75,0.0004653,0.75,8.72e-05,0.75,0.0002172,0.5,0.0003599999999999
mmlu-high-school-microeconomics.val.138,Model C,0.0,0.0,0.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.640000000000001e-05
mmlu-anatomy.val.6,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.49e-05,0.0,6.720000000000001e-05,1.0,0.000672,1.0,0.000672,1.0,8.3e-05,1.0,0.00087,0.0,6.4408e-05,0.0,7.470000000000001e-05,1.0,1.66e-05,1.0,4.98e-05,1.0,6.560000000000001e-05
mmlu-prehistory.val.36,B,0.0,0.0,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00101,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.76e-05
arc-challenge.val.282,Model C,0.0,0.0,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,1.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
grade-school-math.dev.885,meta/llama-2-70b-chat,0.25,0.0003815999999999,0.5,0.0001598999999999,0.25,0.0007544,0.5,0.0049759999999999,0.5,0.006224,0.25,0.000552,0.5,0.00754,0.5,0.00035308,0.25,0.0003815999999999,0.25,9.96e-05,0.75,0.0002226,0.5,0.0004336
hellaswag.val.5934,B,0.0,0.0,0.0,8.519999999999998e-05,0.0,0.0002288,1.0,0.002288,1.0,0.002288,1.0,0.000287,1.0,0.00289,0.0,0.00022116,1.0,0.0002565,0.0,5.7e-05,1.0,0.0001709999999999,1.0,0.0002272
hellaswag.val.4548,claude-v2,0.0,0.001888,1.0,7.049999999999999e-05,1.0,0.0001888,1.0,0.001888,0.0,0.001888,0.0,0.000237,1.0,0.00239,1.0,0.0001823599999999,1.0,0.0002115,1.0,4.7e-05,1.0,0.0001409999999999,1.0,0.0001872
mmlu-professional-accounting.val.245,Llama-2-70b-chat,0.0,0.0,0.0,5.16e-05,0.0,0.0001384,0.0,0.001384,0.0,0.001384,0.0,0.000174,1.0,0.0017599999999999,0.0,0.000133472,0.0,0.0001548,1.0,3.44e-05,0.0,0.0001032,0.0,0.0001376
arc-challenge.test.250,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,1.0,1.95e-05,1.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.5e-05,1.0,0.00069,0.0,5.044e-05,0.0,5.8500000000000006e-05,1.0,1.3e-05,1.0,3.9e-05,1.0,5.12e-05
grade-school-math.dev.5595,meta/llama-2-70b-chat,0.75,0.0003329999999999,0.5,0.0001442999999999,0.75,0.0005768,0.75,0.00644,0.75,0.00644,0.5,0.000447,0.75,0.00733,0.25,0.000239008,0.75,0.0003329999999999,0.25,9.18e-05,0.75,0.0002598,0.5,0.0002272
grade-school-math.dev.6217,meta/llama-2-70b-chat,0.5,0.0004122,0.25,0.0001401,0.5,0.0006664,0.5,0.00652,0.5,0.006808,0.5,0.000429,0.5,0.00908,0.25,0.000405848,0.5,0.0004122,0.25,0.000114,0.5,0.0002742,0.5,0.0004664
hellaswag.val.9121,claude-v2,1.0,0.001936,0.0,7.230000000000001e-05,0.0,0.000196,0.0,0.001936,1.0,0.001936,0.0,0.000243,0.0,0.00245,0.0,0.000187016,0.0,0.000216,0.0,4.8200000000000006e-05,0.0,0.0001446,1.0,0.000192
mmlu-professional-law.val.1326,B,0.0,0.0,1.0,8.280000000000001e-05,1.0,0.0002216,1.0,0.002216,1.0,0.002216,1.0,0.000276,1.0,0.00277,0.0,0.0002141759999999,0.0,0.0002483999999999,0.0,5.520000000000001e-05,1.0,0.0001656,1.0,0.00022
grade-school-math.dev.1245,meta/llama-2-70b-chat,0.75,0.0004095,0.25,0.0001886999999999,0.75,0.0005232,0.75,0.00528,0.75,0.006408,0.5,0.000501,0.5,0.00927,0.75,0.000326696,0.75,0.0004095,0.5,7.999999999999999e-05,0.25,0.0002909999999999,0.75,0.0003599999999999
hellaswag.val.3167,claude-v2,1.0,0.000776,1.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.001,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,0.0,5.76e-05,1.0,7.6e-05
hellaswag.val.1383,C) claude-v2,0.0,0.0,1.0,5.58e-05,0.0,0.0001496,0.0,0.001496,0.0,0.00152,0.0,0.000188,0.0,0.00187,0.0,0.0001443359999999,0.0,0.0001664999999999,0.0,3.720000000000001e-05,0.0,0.0001115999999999,0.0,0.000148
mmlu-professional-law.val.407,claude-v2,1.0,0.00232,1.0,8.669999999999999e-05,1.0,0.000232,1.0,0.00232,1.0,0.00232,1.0,0.000289,1.0,0.0029,0.0,0.000224264,0.0,0.0002601,0.0,5.780000000000001e-05,1.0,0.0001733999999999,1.0,0.0002304
mmlu-professional-law.val.344,meta/llama-2-70b-chat,0.0,0.0002259,0.0,7.53e-05,1.0,0.0002016,1.0,0.002016,1.0,0.002016,0.0,0.000251,1.0,0.00252,0.0,0.000194776,0.0,0.0002259,0.0,5.020000000000001e-05,0.0,0.0001506,1.0,0.0002
grade-school-math.dev.7009,meta/llama-2-70b-chat,0.25,0.0004365,0.25,0.0001305,0.75,0.0006127999999999,0.75,0.005096,0.75,0.00548,0.75,0.000441,0.75,0.00919,0.25,0.00035696,0.25,0.0004365,0.25,9.02e-05,0.75,0.000264,0.25,0.000248
mmlu-professional-law.val.1067,claude-v2,0.0,0.002672,0.0,9.99e-05,0.0,0.0002672,0.0,0.002672,0.0,0.002672,0.0,0.000333,0.0,0.00334,0.0,0.0002584079999999,0.0,0.0002997,0.0,6.659999999999999e-05,0.0,0.0001998,0.0,0.0002656
mmlu-professional-law.val.115,claude-v2,0.0,0.002168,0.0,8.099999999999999e-05,1.0,0.0002168,1.0,0.002168,0.0,0.002168,1.0,0.00027,1.0,0.00271,0.0,0.00020952,0.0,0.000243,0.0,5.4000000000000005e-05,1.0,0.0001619999999999,0.0,0.0002152
hellaswag.val.2555,claude-v2,1.0,0.00072,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,1.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,0.0,7.040000000000002e-05
mmlu-professional-law.val.1229,meta/llama-2-70b-chat,0.0,0.0002295,0.0,7.649999999999999e-05,0.0,0.0002048,1.0,0.002048,1.0,0.002048,0.0,0.000255,1.0,0.00256,0.0,0.00019788,0.0,0.0002295,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-global-facts.val.54,Model C,0.0,0.0,1.0,2.4e-05,0.0,6.480000000000002e-05,1.0,0.000648,0.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00084,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,0.0,6.400000000000001e-05
hellaswag.val.4243,B,0.0,0.0,0.0,7.62e-05,1.0,0.000204,1.0,0.00204,1.0,0.00204,1.0,0.000256,1.0,0.00258,0.0,0.0001971039999999,1.0,0.0002277,0.0,5.080000000000001e-05,0.0,0.0001524,0.0,0.0002024
mmlu-elementary-mathematics.val.70,Model C (meta/llama-2-70b-chat),0.0,0.0,0.0,3.12e-05,1.0,8.400000000000001e-05,0.0,0.00084,1.0,0.00084,0.0,0.000104,0.0,0.00108,0.0,8.0704e-05,0.0,9.36e-05,0.0,2.08e-05,0.0,6.24e-05,0.0,8.32e-05
mmlu-high-school-government-and-politics.val.176,Model D,0.0,0.0,1.0,3.3600000000000004e-05,1.0,9.04e-05,1.0,0.000904,0.0,0.000904,1.0,0.000112,1.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,1.0,8.88e-05
grade-school-math.dev.6163,meta/llama-2-70b-chat,0.25,0.0003807,0.25,0.0001293,0.75,0.000608,0.75,0.00524,0.25,0.005648,0.25,0.00066,0.75,0.01627,0.25,0.000291776,0.25,0.0003807,0.75,8.16e-05,0.75,0.0002502,0.25,0.0003648
mmlu-miscellaneous.val.505,Model C,0.0,0.0,0.0,1.98e-05,0.0,5.36e-05,0.0,0.000536,0.0,0.000536,0.0,6.599999999999999e-05,0.0,0.00067,0.0,5.1216000000000006e-05,0.0,5.940000000000001e-05,0.0,1.32e-05,1.0,3.96e-05,0.0,5.2e-05
hellaswag.val.941,B) claude-v2,0.0,0.0,1.0,4.17e-05,1.0,0.0001128,1.0,0.0011279999999999,1.0,0.0011279999999999,1.0,0.0001419999999999,1.0,0.00141,0.0,0.00010864,1.0,0.0001251,0.0,2.8e-05,1.0,8.4e-05,1.0,0.0001112
mmlu-professional-law.val.269,claude-v2,0.0,0.002592,1.0,9.69e-05,1.0,0.0002592,0.0,0.002592,0.0,0.002592,1.0,0.000323,1.0,0.00324,0.0,0.000250648,0.0,0.0002907,0.0,6.46e-05,1.0,0.0001938,1.0,0.0002584
winogrande.dev.853,mistralai/mixtral-8x7b-chat,1.0,2.94e-05,1.0,1.44e-05,0.0,4e-05,0.0,0.0003999999999999,0.0,0.0003999999999999,1.0,5.1e-05,1.0,0.0005,1.0,3.8024e-05,0.0,4.3200000000000007e-05,1.0,9.8e-06,1.0,2.94e-05,0.0,3.92e-05
winogrande.dev.1135,meta/llama-2-70b-chat,0.0,4.14e-05,1.0,1.3799999999999998e-05,1.0,3.76e-05,1.0,0.000376,1.0,0.000376,1.0,4.8e-05,1.0,0.0005,0.0,3.492e-05,0.0,4.14e-05,1.0,9.2e-06,1.0,2.76e-05,1.0,3.6e-05
mmlu-human-aging.val.76,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.96e-05
grade-school-math.dev.3693,meta/llama-2-70b-chat,0.75,0.0003636,0.75,0.0001326,0.75,0.000576,0.75,0.004248,0.75,0.0056159999999999,0.75,0.000482,0.75,0.00624,0.75,0.00030652,0.75,0.0003636,0.25,8.58e-05,0.75,0.0002309999999999,0.75,0.0004008
winogrande.dev.116,Model B,0.0,0.0,0.0,1.62e-05,1.0,4.4e-05,1.0,0.0004399999999999,1.0,0.0004399999999999,1.0,5.6e-05,1.0,0.0005499999999999,0.0,4.1904e-05,1.0,4.860000000000001e-05,0.0,1.08e-05,1.0,3.24e-05,1.0,4.32e-05
hellaswag.val.9273,D) claude-v2,0.0,0.0,0.0,6.9e-05,0.0,0.0001872,0.0,0.001848,0.0,0.001848,1.0,0.000232,1.0,0.00231,0.0,0.0001784799999999,0.0,0.0002061,0.0,4.600000000000001e-05,0.0,0.000138,1.0,0.0001832
chinese_homonym.dev.17,Model C - claude-v2,0.0,0.0,0.0,2.31e-05,0.0,6.32e-05,1.0,0.000632,0.0,0.000632,0.0,7.8e-05,0.0,0.00079,0.0,5.9752e-05,0.0,0.0002079,0.0,1.54e-05,1.0,6.48e-05,0.0,0.0001152
mmlu-business-ethics.val.48,Model C,0.0,0.0,1.0,3.6e-05,0.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.0001199999999999,1.0,0.00121,0.0,9.312e-05,0.0,0.000108,0.0,2.4e-05,1.0,7.2e-05,1.0,9.52e-05
hellaswag.val.3058,claude-v2,1.0,0.000744,1.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,1.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,0.0,7.280000000000001e-05
grade-school-math.dev.6695,meta/llama-2-70b-chat,0.75,0.0003861,0.25,0.000159,0.25,0.0006872,0.75,0.003896,0.75,0.005888,0.75,0.000441,0.75,0.00859,0.25,0.000320488,0.75,0.0003861,0.25,0.0001026,0.75,0.0003156,0.75,0.0004136
mmlu-professional-law.val.153,meta/llama-2-70b-chat,0.0,0.0003204,0.0,0.0001068,1.0,0.0002856,0.0,0.002856,0.0,0.002856,0.0,0.000356,1.0,0.0036,0.0,0.000276256,0.0,0.0003204,0.0,7.12e-05,0.0,0.0002136,0.0,0.0002848
mmlu-professional-law.val.1301,Llama-2-70b-chat,0.0,0.0,1.0,0.0001094999999999,1.0,0.0002928,1.0,0.002928,0.0,0.002928,1.0,0.000365,0.0,0.00366,0.0,0.00028324,0.0,0.0003284999999999,0.0,7.3e-05,1.0,0.0002189999999999,0.0,0.0002912
hellaswag.val.4043,C) claude-v2,0.0,0.0,0.0,8.46e-05,1.0,0.0002296,1.0,0.002272,1.0,0.002272,1.0,0.000285,1.0,0.00287,0.0,0.000219608,0.0,0.0002547,0.0,5.660000000000001e-05,0.0,0.0001698,1.0,0.0002256
hellaswag.val.455,meta/llama-2-70b-chat,0.0,8.55e-05,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,0.0,7.520000000000001e-05
hellaswag.val.9766,A) claude-instant-v1,0.0,0.0,1.0,8.64e-05,0.0,0.0002312,1.0,0.002312,0.0,0.002312,0.0,0.00029,1.0,0.00292,1.0,0.0002234879999999,1.0,0.0002592,1.0,5.76e-05,1.0,0.0001728,1.0,0.0002296
hellaswag.val.7561,D),0.0,0.0,1.0,8.879999999999999e-05,0.0,0.0002384,1.0,0.002384,0.0,0.002384,0.0,0.000299,1.0,0.00301,1.0,0.000230472,0.0,0.0002673,1.0,5.94e-05,1.0,0.0001782,1.0,0.0002367999999999
mmlu-professional-law.val.520,meta/llama-2-70b-chat,0.0,8.190000000000001e-05,0.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.46e-05,1.0,7.280000000000001e-05
mmlu-professional-law.val.1520,claude-v2,0.0,0.001776,0.0,6.63e-05,0.0,0.0001776,0.0,0.001776,0.0,0.001776,0.0,0.000221,0.0,0.00222,0.0,0.000171496,0.0,0.0001988999999999,0.0,4.420000000000001e-05,0.0,0.0001326,0.0,0.000176
mmlu-machine-learning.val.41,B,0.0,0.0,0.0,5.43e-05,1.0,0.0001456,1.0,0.0014559999999999,1.0,0.0014559999999999,0.0,0.0001809999999999,0.0,0.00182,0.0,0.0001404559999999,0.0,0.0001628999999999,0.0,3.6200000000000006e-05,1.0,0.0001086,0.0,0.0001448
mmlu-human-aging.val.49,Model B,0.0,0.0,1.0,2.04e-05,1.0,5.52e-05,1.0,0.000552,1.0,0.000552,1.0,7.000000000000001e-05,1.0,0.00069,0.0,5.2768e-05,0.0,6.12e-05,0.0,1.36e-05,1.0,4.08e-05,1.0,5.44e-05
mmlu-high-school-statistics.val.81,Model C,0.0,0.0,0.0,3.9e-05,0.0,0.0001048,0.0,0.0010479999999999,0.0,0.0010479999999999,0.0,0.00013,1.0,0.00134,0.0,0.00010088,0.0,0.000117,0.0,2.6e-05,0.0,7.8e-05,1.0,0.000104
mmlu-professional-law.val.997,meta/llama-2-70b-chat,0.0,0.0003069,0.0,0.0001023,1.0,0.0002736,1.0,0.002736,1.0,0.002736,0.0,0.000341,1.0,0.00342,0.0,0.000264616,0.0,0.0003069,0.0,6.819999999999999e-05,0.0,0.0002046,1.0,0.000272
hellaswag.val.8217,B,0.0,0.0,0.0,7.649999999999999e-05,0.0,0.0002072,1.0,0.002048,1.0,0.002048,1.0,0.000257,1.0,0.00256,0.0,0.00019788,1.0,0.0002286,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-moral-scenarios.val.410,"C) Not wrong, Wrong",0.0,0.0,0.0,3.93e-05,0.0,0.0001056,0.0,0.0010559999999999,0.0,0.0010559999999999,0.0,0.0001309999999999,1.0,0.00132,0.0,0.000101656,0.0,0.0001179,0.0,2.62e-05,1.0,7.86e-05,1.0,0.0001048
hellaswag.val.847,claude-v2,1.0,0.000896,0.0,3.33e-05,1.0,8.960000000000001e-05,1.0,0.000896,1.0,0.000896,1.0,0.000113,1.0,0.00112,0.0,8.6136e-05,1.0,9.9e-05,0.0,2.22e-05,1.0,6.659999999999999e-05,1.0,8.800000000000001e-05
mmlu-moral-scenarios.val.496,"C) Not wrong, Wrong",0.0,0.0,0.0,4.6200000000000005e-05,0.0,0.000124,0.0,0.00124,0.0,0.00124,0.0,0.000154,1.0,0.00158,0.0,0.000119504,0.0,0.0001386,0.0,3.08e-05,0.0,9.24e-05,0.0,0.0001224
mmlu-professional-accounting.val.229,Model C,0.0,0.0,0.0,3.24e-05,0.0,8.720000000000002e-05,0.0,0.000872,1.0,0.000872,0.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,0.0,6.48e-05,0.0,8.560000000000002e-05
mmlu-professional-law.val.169,claude-v2,1.0,0.002144,0.0,8.01e-05,1.0,0.0002144,1.0,0.002144,1.0,0.002144,0.0,0.000267,1.0,0.00268,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
grade-school-math.dev.7170,meta/llama-2-70b-chat,0.25,0.0003519,0.75,0.0001479,0.75,0.0004648,0.75,0.004408,0.75,0.005296,0.5,0.0004349999999999,0.5,0.00638,0.25,0.000284016,0.25,0.0003519,0.25,7.400000000000001e-05,0.75,0.0002177999999999,0.75,0.0003488
grade-school-math.dev.5220,meta/llama-2-70b-chat,0.25,0.000531,0.5,0.0002384999999999,0.5,0.0007784,0.5,0.006968,0.5,0.009128,0.25,0.000684,0.5,0.0123699999999999,0.25,0.00050828,0.25,0.000531,0.25,0.0001082,0.5,0.0003444,0.5,0.0005648
chinese_poem.dev.2,llama-2-70b-chat,0.0,0.0,0.0,4.77e-05,0.0,0.0001296,1.0,0.0012959999999999,0.0,0.00132,0.0,0.0001599999999999,1.0,0.00162,0.0,0.000119504,0.0,0.0001476,0.0,3.1400000000000004e-05,0.0,0.0001038,0.0,0.0001352
hellaswag.val.6761,B) claude-v2,0.0,0.0,0.0,7.62e-05,0.0,0.000204,0.0,0.00204,0.0,0.00204,1.0,0.000256,1.0,0.00255,0.0,0.0001971039999999,0.0,0.0002277,0.0,5.080000000000001e-05,1.0,0.0001524,0.0,0.0002024
mmlu-professional-law.val.468,claude-v2,0.0,0.001944,1.0,7.26e-05,1.0,0.0001944,0.0,0.001944,0.0,0.001944,1.0,0.000242,1.0,0.00243,0.0,0.000187792,0.0,0.0002177999999999,0.0,4.84e-05,0.0,0.0001452,1.0,0.0001928
mmlu-medical-genetics.val.38,Model C - Von Hippel-Lindau syndrome,0.0,0.0,1.0,2.61e-05,0.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,1.0,6.96e-05
grade-school-math.dev.4238,meta/llama-2-70b-chat,0.75,0.0003771,0.25,0.0001671,0.75,0.0006328,0.75,0.005272,0.75,0.0068559999999999,0.75,0.000631,0.75,0.0089,0.25,0.000326696,0.75,0.0003771,0.25,9.78e-05,0.75,0.0002748,0.75,0.0004296
winogrande.dev.1256,Model C - claude-v2,0.0,0.0,1.0,1.65e-05,1.0,4.4800000000000005e-05,1.0,0.000448,0.0,0.000448,1.0,5.5e-05,1.0,0.00059,1.0,4.2680000000000005e-05,0.0,4.95e-05,1.0,1.1e-05,0.0,3.3e-05,1.0,4.3200000000000007e-05
grade-school-math.dev.4633,meta/llama-2-70b-chat,0.25,0.000423,0.25,0.0001775999999999,0.25,0.0008192,0.25,0.005912,0.75,0.00848,0.25,0.00061,0.75,0.01228,0.25,0.000350752,0.25,0.000423,0.25,7.860000000000001e-05,0.25,0.0003246,0.25,0.00044
mmlu-professional-law.val.367,C) meta/llama-2-70b-chat,0.0,0.0,1.0,8.730000000000001e-05,1.0,0.0002336,1.0,0.002336,1.0,0.002336,1.0,0.0002909999999999,1.0,0.00292,0.0,0.000225816,0.0,0.0002619,0.0,5.8200000000000005e-05,1.0,0.0001746,1.0,0.000232
mmlu-professional-medicine.val.222,Llama-2-70b-chat,0.0,0.0,0.0,7.38e-05,1.0,0.0001976,1.0,0.001976,0.0,0.001976,1.0,0.000248,1.0,0.0025,0.0,0.0001908959999999,0.0,0.0002214,0.0,4.920000000000001e-05,1.0,0.0001476,1.0,0.000196
winogrande.dev.54,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 1, cost: 0.6
Correct choice: Model B",0.0,0.0,1.0,1.59e-05,1.0,4.32e-05,1.0,0.000432,1.0,0.000432,1.0,5.5e-05,1.0,0.00057,1.0,4.1128e-05,0.0,4.77e-05,1.0,1.06e-05,1.0,3.18e-05,1.0,4.16e-05
mmlu-high-school-macroeconomics.val.298,claude-v2,1.0,0.000768,0.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,0.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-professional-law.val.658,Llama-2-70b-chat,0.0,0.0,1.0,5.1e-05,0.0,0.0001368,0.0,0.0013679999999999,0.0,0.0013679999999999,0.0,0.0001699999999999,0.0,0.00171,0.0,0.0001319199999999,0.0,0.000153,0.0,3.4000000000000007e-05,1.0,0.000102,0.0,0.000136
mmlu-professional-law.val.645,B,0.0,0.0,1.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.000248,1.0,0.00249,0.0,0.000192448,0.0,0.0002232,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
hellaswag.val.589,Model B,0.0,0.0,0.0,2.43e-05,0.0,6.560000000000001e-05,0.0,0.000656,0.0,0.000656,0.0,8.099999999999999e-05,0.0,0.00082,1.0,6.285600000000001e-05,0.0,7.2e-05,1.0,1.62e-05,1.0,4.86e-05,0.0,6.400000000000001e-05
grade-school-math.dev.4772,meta/llama-2-70b-chat,0.5,0.0003429,0.25,0.0001841999999999,0.5,0.0007023999999999,0.75,0.00496,0.75,0.006208,0.5,0.0004399999999999,0.75,0.0070999999999999,0.25,0.000323592,0.5,0.0003429,0.5,8.840000000000001e-05,0.5,0.0002622,0.5,0.0003352
mmlu-professional-law.val.78,claude-v2,1.0,0.003296,0.0,0.0001233,0.0,0.0003296,0.0,0.003296,1.0,0.003296,0.0,0.000411,0.0,0.0041199999999999,0.0,0.000318936,0.0,0.0003699,0.0,8.22e-05,0.0,0.0002466,0.0,0.000328
mmlu-business-ethics.val.64,D,0.0,0.0,0.0,3.96e-05,1.0,0.0001064,1.0,0.0010639999999999,1.0,0.0010639999999999,1.0,0.0001319999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001188,0.0,2.64e-05,1.0,7.92e-05,1.0,0.0001055999999999
bias_detection.dev.60,"Model C — correctness: 1, cost: 0.3

Reasoning: The prompt requires classification of a sentence based on its content type, which is a task that involves understanding and analyzing the sentence's structure and",0.0,0.0,1.0,9.24e-05,1.0,0.0002776,0.0,0.002584,1.0,0.004168,0.0,0.000286,1.0,0.0085399999999999,0.0,0.000242112,0.0,0.0003149999999999,0.0,5.9e-05,0.0,0.000183,0.0,0.0001872
hellaswag.val.611,Meta/llama-2-70b-chat,0.0,0.0,1.0,4.38e-05,1.0,0.0001176,1.0,0.001176,1.0,0.001176,1.0,0.000148,1.0,0.0015,1.0,0.000113296,0.0,0.0001305,1.0,2.92e-05,1.0,8.759999999999999e-05,1.0,0.000116
hellaswag.val.3378,claude-v2,1.0,0.002296,0.0,8.58e-05,0.0,0.0002296,1.0,0.002296,1.0,0.002296,1.0,0.000288,1.0,0.00287,0.0,0.000221936,0.0,0.0002573999999999,0.0,5.720000000000001e-05,0.0,0.0001709999999999,1.0,0.000228
hellaswag.val.9074,B,0.0,0.0,1.0,7.41e-05,0.0,0.0001984,1.0,0.001984,0.0,0.001984,0.0,0.000249,1.0,0.00251,1.0,0.000191672,0.0,0.0002222999999999,1.0,4.94e-05,1.0,0.0001482,1.0,0.0001967999999999
grade-school-math.dev.3017,meta/llama-2-70b-chat,0.75,0.0003618,0.75,0.0001548,0.75,0.0005576,0.75,0.004328,0.75,0.004784,0.75,0.000381,0.75,0.00556,0.25,0.000297984,0.75,0.0003618,0.75,7.120000000000001e-05,0.75,0.0002304,0.0,0.0003384
mmlu-electrical-engineering.val.113,Model C,0.0,0.0,0.0,2.85e-05,0.0,7.680000000000001e-05,0.0,0.000768,0.0,0.000768,0.0,9.5e-05,1.0,0.00099,0.0,7.372e-05,0.0,8.55e-05,1.0,1.9e-05,1.0,5.7e-05,0.0,7.520000000000001e-05
hellaswag.val.7064,C,0.0,0.0,1.0,8.85e-05,1.0,0.0002392,0.0,0.002368,0.0,0.002368,0.0,0.000297,1.0,0.00299,1.0,0.0002289199999999,0.0,0.0002646,1.0,5.9e-05,1.0,0.000177,1.0,0.0002352
grade-school-math.dev.4972,meta/llama-2-70b-chat,0.75,0.0003942,0.75,0.0001635,0.5,0.000632,0.25,0.005312,0.25,0.006056,0.5,0.000513,0.75,0.00757,0.25,0.000245992,0.75,0.0003942,0.25,8.6e-05,0.25,0.0002646,0.25,0.0002416
grade-school-math.dev.7173,meta/llama-2-70b-chat,0.75,0.0003546,0.5,0.0001578,0.75,0.0005616,0.75,0.004632,0.75,0.005952,0.5,0.000455,0.75,0.00633,0.5,0.0003298,0.75,0.0003546,0.5,8.879999999999999e-05,1.0,0.0002256,0.75,0.0003736
mmlu-professional-law.val.874,meta/llama-2-70b-chat,0.0,0.0002331,0.0,7.769999999999999e-05,0.0,0.000208,1.0,0.00208,1.0,0.00208,0.0,0.000259,1.0,0.0026,0.0,0.000200984,0.0,0.0002331,0.0,5.1800000000000005e-05,0.0,0.0001553999999999,0.0,0.0002072
mmlu-jurisprudence.val.47,C,0.0,0.0,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000109,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
grade-school-math.dev.950,meta/llama-2-70b-chat,0.75,0.0003573,0.75,0.0001818,0.75,0.0005736,0.75,0.0044399999999999,0.75,0.0060719999999999,0.5,0.000585,0.75,0.01134,0.5,0.000339112,0.75,0.0003573,0.75,9.6e-05,0.25,0.000327,0.5,0.0004832
mmlu-professional-law.val.862,Llama-2-70b-chat,0.0,0.0,0.0,8.91e-05,0.0,0.0002384,0.0,0.002384,0.0,0.002384,0.0,0.000297,0.0,0.00298,0.0,0.000230472,0.0,0.0002673,0.0,5.94e-05,0.0,0.0001775999999999,0.0,0.0002367999999999
mmlu-anatomy.val.19,Model C,0.0,0.0,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,1.0,4.44e-05,1.0,5.84e-05
mmlu-professional-law.val.408,B,0.0,0.0,1.0,7.230000000000001e-05,1.0,0.0001936,1.0,0.001936,1.0,0.001936,1.0,0.000241,1.0,0.00242,0.0,0.000187016,0.0,0.0002169,0.0,4.8200000000000006e-05,1.0,0.0001446,1.0,0.000192
hellaswag.val.8078,B,0.0,0.0,0.0,8.249999999999999e-05,0.0,0.0002208,0.0,0.002208,1.0,0.002208,1.0,0.000277,0.0,0.00276,0.0,0.0002134,0.0,0.0002466,0.0,5.5e-05,0.0,0.0001649999999999,1.0,0.0002192
hellaswag.val.5949,C,0.0,0.0,0.0,7.08e-05,0.0,0.000192,0.0,0.001896,0.0,0.001896,0.0,0.000236,1.0,0.00237,0.0,0.000183136,0.0,0.0002115,0.0,4.720000000000001e-05,0.0,0.0001416,1.0,0.000188
grade-school-math.dev.1663,meta/llama-2-70b-chat,0.75,0.0003186,0.75,0.0001203,0.75,0.0004232,0.75,0.0038,0.75,0.00416,0.5,0.000425,0.75,0.00514,0.25,0.000350752,0.75,0.0003186,0.5,6.24e-05,0.5,0.0002045999999999,0.25,0.0002168
hellaswag.val.5149,Model C,0.0,0.0,0.0,7.02e-05,0.0,0.0001904,0.0,0.00188,0.0,0.00188,0.0,0.000236,0.0,0.00238,0.0,0.000181584,0.0,0.0002106,0.0,4.6800000000000006e-05,0.0,0.0001404,0.0,0.0001864
grade-school-math.dev.2818,meta/llama-2-70b-chat,0.25,0.0003528,0.5,0.0001368,0.5,0.0005128,0.75,0.005344,0.5,0.0061119999999999,0.5,0.0005759999999999,0.5,0.00755,0.25,0.000315056,0.25,0.0003528,0.5,7.500000000000001e-05,0.5,0.0002238,0.75,0.0003552
winogrande.dev.370,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 1, cost: 0.6
Correct choice: Model B",0.0,0.0,0.0,1.62e-05,1.0,4.4e-05,1.0,0.0004399999999999,1.0,0.0004399999999999,1.0,5.6e-05,1.0,0.00058,1.0,4.1904e-05,0.0,4.860000000000001e-05,0.0,1.08e-05,1.0,3.24e-05,0.0,4.32e-05
chinese_zodiac.dev.157,meta/llama-2-70b-chat,0.0,0.0001251,0.0,3.33e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.000106312,0.0,0.0001251,1.0,2.22e-05,0.0,6.659999999999999e-05,0.0,8.960000000000001e-05
hellaswag.val.8207,Model C,0.0,0.0,1.0,8.01e-05,1.0,0.0002168,0.0,0.002144,0.0,0.002144,1.0,0.000269,1.0,0.00268,1.0,0.000207192,0.0,0.0002394,1.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
grade-school-math.dev.3870,meta/llama-2-70b-chat,0.25,0.0003987,0.75,0.0001619999999999,0.75,0.0005495999999999,0.75,0.005208,0.5,0.00564,0.5,0.000753,0.75,0.00858,0.25,0.00029488,0.25,0.0003987,0.75,8.38e-05,1.0,0.0001877999999999,0.75,0.0003272
hellaswag.val.7324,C) claude-v2,0.0,0.0,0.0,7.29e-05,1.0,0.0001976,1.0,0.001952,1.0,0.001952,1.0,0.000245,1.0,0.00247,0.0,0.000188568,1.0,0.0002178,0.0,4.860000000000001e-05,0.0,0.0001458,1.0,0.0001936
hellaswag.val.1213,claude-v2,1.0,0.000816,0.0,3e-05,1.0,8.16e-05,0.0,0.000816,1.0,0.000816,1.0,0.000103,0.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,0.0,6.06e-05,0.0,8e-05
mmlu-elementary-mathematics.val.298,Model C (13),0.0,0.0,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.5e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,0.0,7.439999999999999e-05
hellaswag.val.9644,Model C,0.0,0.0,1.0,8.189999999999998e-05,1.0,0.0002224,0.0,0.0022,1.0,0.0022,0.0,0.000276,1.0,0.00278,1.0,0.000212624,1.0,0.0002466,1.0,5.480000000000001e-05,1.0,0.0001643999999999,1.0,0.0002184
arc-challenge.test.175,Model C,0.0,0.0,1.0,2.49e-05,1.0,6.720000000000001e-05,1.0,0.000672,1.0,0.000672,1.0,8.5e-05,1.0,0.0008399999999999,0.0,6.4408e-05,1.0,7.470000000000001e-05,1.0,1.66e-05,1.0,4.98e-05,1.0,6.560000000000001e-05
hellaswag.val.8283,Model C,0.0,0.0,1.0,8.34e-05,0.0,0.000224,1.0,0.00224,1.0,0.00224,0.0,0.000281,1.0,0.0028,1.0,0.0002165039999999,1.0,0.0002502,1.0,5.580000000000001e-05,1.0,0.0001674,1.0,0.0002224
hellaswag.val.6174,Meta/llama-2-70b-chat,0.0,0.0,0.0,7.14e-05,0.0,0.0001912,0.0,0.0019119999999999,0.0,0.0019119999999999,0.0,0.0002399999999999,0.0,0.00242,0.0,0.000184688,0.0,0.0002142,0.0,4.7600000000000005e-05,0.0,0.0001428,0.0,0.0001896
mmlu-professional-law.val.348,claude-v2,1.0,0.003184,1.0,0.0001191,0.0,0.0003184,0.0,0.003184,1.0,0.003184,0.0,0.000397,1.0,0.00398,0.0,0.0003080719999999,0.0,0.0003573,0.0,7.939999999999999e-05,0.0,0.0002375999999999,0.0,0.0003168
hellaswag.val.5708,Model C,0.0,0.0,0.0,8.999999999999999e-05,1.0,0.0002432,0.0,0.002408,1.0,0.002408,1.0,0.0003019999999999,1.0,0.00301,0.0,0.0002328,0.0,0.0002691,0.0,6e-05,0.0,0.0001799999999999,1.0,0.0002392
hellaswag.val.4647,Model D,0.0,0.0,1.0,5.97e-05,1.0,0.0001624,1.0,0.0015999999999999,1.0,0.0015999999999999,0.0,0.0002009999999999,1.0,0.00203,1.0,0.000154424,1.0,0.0001791,1.0,3.980000000000001e-05,1.0,0.0001193999999999,1.0,0.0001584
hellaswag.val.2424,Model C,0.0,0.0,0.0,3.5999999999999994e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,1.0,0.00122,0.0,9.3896e-05,1.0,0.000108,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
grade-school-math.dev.5226,meta/llama-2-70b-chat,0.5,0.0003888,0.75,0.0001704,0.75,0.0005888,0.75,0.004616,0.75,0.0061759999999999,0.75,0.000527,0.75,0.007,0.25,0.00033368,0.5,0.0003888,0.25,9.44e-05,0.5,0.0002646,0.75,0.0004384
mmlu-elementary-mathematics.val.329,Model B,0.0,0.0,0.0,2.73e-05,0.0,7.360000000000001e-05,0.0,0.000736,0.0,0.000736,0.0,9.1e-05,0.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.4e-05,0.0,7.280000000000001e-05
grade-school-math.dev.3234,meta/llama-2-70b-chat,0.75,0.0003725999999999,0.5,0.0001512,0.75,0.0005224,0.75,0.004672,0.75,0.005392,0.75,0.0005,0.5,0.00662,0.5,0.0003104,0.75,0.0003725999999999,0.75,8.14e-05,0.5,0.0002766,0.75,0.0004144
mmlu-professional-law.val.1426,claude-v2,0.0,0.002864,0.0,0.0001071,1.0,0.0002864,0.0,0.002864,0.0,0.002864,1.0,0.000357,0.0,0.00358,0.0,0.000277032,0.0,0.0003213,0.0,7.14e-05,0.0,0.0002142,1.0,0.0002848
grade-school-math.dev.4315,meta/llama-2-70b-chat,0.5,0.0003681,0.5,0.000144,0.5,0.000524,0.75,0.003008,0.5,0.005336,0.75,0.000438,0.5,0.0069099999999999,0.5,0.000303416,0.5,0.0003681,0.5,8.58e-05,0.5,0.0002412,0.25,0.0003648
arc-challenge.test.729,meta/llama-2-70b-chat,1.0,0.0001215,1.0,4.05e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,1.0,0.000137,1.0,0.00136,1.0,0.0001047599999999,1.0,0.0001215,1.0,2.6800000000000004e-05,1.0,8.1e-05,1.0,0.0001072
mmlu-moral-scenarios.val.857,"D) Wrong, Not wrong",0.0,0.0,1.0,4.17e-05,0.0,0.000112,0.0,0.00112,0.0,0.00112,1.0,0.000139,0.0,0.0014299999999999,0.0,0.000107864,0.0,0.0001250999999999,0.0,2.78e-05,0.0,8.340000000000001e-05,0.0,0.0001112
mmlu-high-school-mathematics.val.22,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.37e-05,0.0,6.400000000000001e-05,0.0,0.00064,0.0,0.00064,0.0,7.9e-05,0.0,0.0007999999999999,0.0,6.1304e-05,0.0,7.11e-05,1.0,1.58e-05,0.0,4.68e-05,0.0,6.32e-05
mmlu-conceptual-physics.val.170,Model C,0.0,0.0,0.0,2.3400000000000003e-05,0.0,6.32e-05,1.0,0.000632,0.0,0.000632,0.0,7.8e-05,1.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.240000000000001e-05
arc-challenge.test.772,Meta/llama-2-70b-chat,0.0,0.0,1.0,2.37e-05,1.0,6.400000000000001e-05,1.0,0.00064,1.0,0.00064,1.0,8.1e-05,1.0,0.00083,1.0,6.1304e-05,1.0,7.11e-05,1.0,1.58e-05,1.0,4.74e-05,1.0,6.240000000000001e-05
mmlu-high-school-macroeconomics.val.135,Model B,0.0,0.0,1.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,8e-05
mmlu-high-school-world-history.val.151,meta/llama-2-70b-chat,0.0,0.0003267,0.0,0.0001089,1.0,0.0002912,1.0,0.002912,1.0,0.002912,1.0,0.000363,1.0,0.00364,0.0,0.000281688,0.0,0.0003267,0.0,7.26e-05,1.0,0.0002178,1.0,0.0002896
grade-school-math.dev.3831,meta/llama-2-70b-chat,0.5,0.0003771,0.5,0.000174,0.75,0.000552,0.75,0.00456,0.75,0.005496,0.5,0.000549,0.5,0.00681,0.25,0.0003561839999999,0.5,0.0003771,0.75,8.300000000000001e-05,0.5,0.0002934,0.75,0.0003944
mmlu-marketing.val.105,B) Advertising,0.0,0.0,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00101,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.76e-05
grade-school-math.dev.3401,meta/llama-2-70b-chat,0.75,0.0003942,0.75,0.0001331999999999,0.75,0.0004584,0.75,0.0041519999999999,0.75,0.004848,0.75,0.0003799999999999,0.75,0.00714,0.5,0.00028324,0.75,0.0003942,0.5,8.16e-05,0.75,0.0002345999999999,0.75,0.0003664
mmlu-professional-law.val.740,claude-v2,0.0,0.001992,0.0,7.439999999999999e-05,0.0,0.0001992,0.0,0.001992,0.0,0.001992,0.0,0.000248,0.0,0.00249,0.0,0.000192448,0.0,0.0002232,0.0,4.9600000000000006e-05,0.0,0.0001487999999999,0.0,0.0001976
mmlu-miscellaneous.val.776,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.05
Model",0.0,0.0,1.0,2.49e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,1.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.720000000000001e-05
mmlu-philosophy.val.154,meta/llama-2-70b-chat,0.0,7.740000000000001e-05,0.0,2.58e-05,0.0,6.960000000000001e-05,0.0,0.000696,0.0,0.000696,0.0,8.599999999999999e-05,1.0,0.0009,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,0.0,5.16e-05,0.0,6.88e-05
mmlu-professional-law.val.695,D) claude-v2,0.0,0.0,1.0,0.0001023,0.0,0.0002736,0.0,0.002736,0.0,0.002736,0.0,0.000341,0.0,0.00342,0.0,0.000264616,0.0,0.0003069,0.0,6.819999999999999e-05,0.0,0.0002046,1.0,0.000272
mmlu-moral-disputes.val.233,claude-v2,1.0,0.00108,0.0,4.02e-05,1.0,0.000108,1.0,0.00108,1.0,0.00108,1.0,0.000134,1.0,0.00135,0.0,0.000103984,0.0,0.0001206,0.0,2.68e-05,1.0,8.04e-05,1.0,0.0001064
mmlu-professional-law.val.1066,meta/llama-2-70b-chat,0.0,0.0002078999999999,1.0,6.93e-05,0.0,0.0001856,0.0,0.001856,0.0,0.001856,1.0,0.000231,1.0,0.00235,0.0,0.000179256,0.0,0.0002078999999999,0.0,4.6200000000000005e-05,0.0,0.0001386,1.0,0.000184
mmlu-prehistory.val.179,Model C,0.0,0.0,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00086,0.0,6.3632e-05,1.0,7.29e-05,1.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
mmlu-miscellaneous.val.234,D) Portfolio assessment,0.0,0.0,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-high-school-world-history.val.87,claude-v2,1.0,0.00312,1.0,0.0001166999999999,1.0,0.000312,1.0,0.00312,1.0,0.00312,1.0,0.0003889999999999,1.0,0.0039,0.0,0.000301864,0.0,0.0003501,0.0,7.780000000000001e-05,1.0,0.0002333999999999,1.0,0.0003112
grade-school-math.dev.4354,meta/llama-2-70b-chat,0.5,0.0004086,0.75,0.0002078999999999,0.75,0.0005568,0.5,0.005256,0.75,0.005568,0.5,0.000509,0.5,0.00939,0.75,0.00038412,0.5,0.0004086,0.25,7.54e-05,0.75,0.000276,0.25,0.0003688
mmlu-marketing.val.151,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 1, cost: 0.7
Model D",0.0,0.0,1.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,0.0,0.000616,1.0,7.8e-05,1.0,0.0008,0.0,5.8976e-05,0.0,6.840000000000001e-05,1.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
mmlu-professional-law.val.981,B,0.0,0.0,1.0,8.76e-05,1.0,0.0002344,0.0,0.002344,0.0,0.002344,1.0,0.000292,1.0,0.00293,0.0,0.000226592,0.0,0.0002628,0.0,5.84e-05,1.0,0.0001752,0.0,0.0002336
mmlu-marketing.val.134,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.43e-05,0.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
hellaswag.val.4965,C) meta/llama-2-70b-chat,0.0,0.0,0.0,8.04e-05,1.0,0.0002176,1.0,0.002152,1.0,0.002152,1.0,0.00027,1.0,0.00269,0.0,0.0002079679999999,1.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
mmlu-professional-law.val.369,meta/llama-2-70b-chat,0.0,0.0002907,1.0,9.69e-05,1.0,0.0002592,1.0,0.002592,1.0,0.002592,1.0,0.000323,1.0,0.00324,0.0,0.000250648,0.0,0.0002907,0.0,6.46e-05,1.0,0.0001938,1.0,0.0002576
mmlu-prehistory.val.130,meta/llama-2-70b-chat,0.0,9.27e-05,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
hellaswag.val.6671,C) claude-v2,0.0,0.0,0.0,8.52e-05,0.0,0.0002304,0.0,0.00228,1.0,0.00228,1.0,0.000286,1.0,0.00285,0.0,0.0002203839999999,0.0,0.0002556,0.0,5.680000000000001e-05,0.0,0.0001704,0.0,0.0002264
mmlu-moral-disputes.val.122,claude-v2,1.0,0.000584,0.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00076,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
mmlu-nutrition.val.44,B,0.0,0.0,0.0,3.03e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.0001009999999999,0.0,0.00105,0.0,7.8376e-05,0.0,9.09e-05,1.0,2.02e-05,0.0,6.06e-05,0.0,8.08e-05
mmlu-high-school-biology.val.285,Model C - Cactus,0.0,0.0,1.0,2.19e-05,0.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,0.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.84e-05
mmlu-high-school-mathematics.val.25,Model C - meta/llama-2-70b-chat,0.0,0.0,0.0,3.03e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,0.0,6.06e-05,0.0,8.08e-05
hellaswag.val.425,claude-v2,1.0,0.000832,0.0,3.09e-05,1.0,8.320000000000002e-05,0.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,1.0,9.18e-05,0.0,2.0600000000000003e-05,0.0,6.18e-05,1.0,8.160000000000002e-05
grade-school-math.dev.71,meta/llama-2-70b-chat,0.75,0.0003699,0.75,0.0001449,0.75,0.000388,0.75,0.004432,0.75,0.005176,0.25,0.000453,0.5,0.00641,0.75,0.000285568,0.75,0.0003699,0.25,6.220000000000001e-05,0.75,0.0002262,0.25,0.00032
mmlu-professional-law.val.1211,claude-v2,0.0,0.0052,0.0,0.0001947,0.0,0.00052,0.0,0.0052,0.0,0.0052,0.0,0.000649,0.0,0.0065,0.0,0.000503624,0.0,0.0005841,0.0,0.0001298,0.0,0.0003888,0.0,0.0005184
mmlu-professional-law.val.158,claude-v2,0.0,0.002848,0.0,0.0001064999999999,1.0,0.0002848,0.0,0.002848,0.0,0.002848,0.0,0.000355,1.0,0.00356,0.0,0.00027548,0.0,0.0003194999999999,1.0,7.1e-05,0.0,0.0002129999999999,0.0,0.000284
grade-school-math.dev.1942,meta/llama-2-70b-chat,0.75,0.0003672,0.75,0.0001311,0.75,0.0006456,0.75,0.004272,0.75,0.0067919999999999,0.75,0.000464,0.75,0.00642,0.75,0.000331352,0.75,0.0003672,0.75,8.66e-05,0.75,0.0002328,0.75,0.0003848
grade-school-math.dev.5101,meta/llama-2-70b-chat,0.75,0.0003438,0.25,0.0001640999999999,0.75,0.000508,0.75,0.004216,0.75,0.0046,0.75,0.0004349999999999,0.75,0.00707,0.75,0.000293328,0.75,0.0003438,0.25,9.38e-05,0.25,0.0002297999999999,0.75,0.0003472
mmlu-business-ethics.val.38,Model D,0.0,0.0,1.0,4.32e-05,1.0,0.000116,1.0,0.00116,1.0,0.00116,0.0,0.000144,1.0,0.00145,0.0,0.000111744,0.0,0.0001295999999999,1.0,2.88e-05,1.0,8.64e-05,1.0,0.0001152
grade-school-math.dev.7391,meta/llama-2-70b-chat,0.25,0.0003501,0.25,0.0001548,0.75,0.0008,0.75,0.00368,0.5,0.005912,0.75,0.000558,0.75,0.01042,0.25,0.000301864,0.25,0.0003501,0.5,9.320000000000002e-05,0.75,0.0003216,0.5,0.00034
mmlu-professional-law.val.497,D) meta/llama-2-70b-chat,0.0,0.0,1.0,8.249999999999999e-05,0.0,0.0002208,0.0,0.002208,0.0,0.002208,0.0,0.000275,1.0,0.00276,0.0,0.0002134,0.0,0.0002475,0.0,5.5e-05,1.0,0.0001649999999999,1.0,0.0002192
hellaswag.val.2220,mistralai/mixtral-8x7b-chat,1.0,5.88e-05,0.0,2.91e-05,0.0,7.920000000000001e-05,0.0,0.000792,0.0,0.000792,1.0,9.8e-05,0.0,0.00099,1.0,7.604800000000001e-05,0.0,8.73e-05,1.0,1.96e-05,1.0,5.88e-05,0.0,7.760000000000002e-05
mmlu-high-school-physics.val.73,Model C,0.0,0.0,1.0,2.73e-05,0.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,0.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.46e-05,0.0,7.280000000000001e-05
mmlu-high-school-european-history.val.32,meta/llama-2-70b-chat,0.0,0.000396,0.0,0.0001319999999999,0.0,0.0003528,0.0,0.003528,0.0,0.003552,0.0,0.00044,0.0,0.00441,0.0,0.00034144,0.0,0.000396,0.0,8.800000000000001e-05,0.0,0.0002639999999999,0.0,0.000352
hellaswag.val.8100,D) meta/llama-2-70b-chat,0.0,0.0,0.0,6.3e-05,0.0,0.0001688,0.0,0.0016879999999999,1.0,0.0016879999999999,0.0,0.0002119999999999,1.0,0.00211,0.0,0.00016296,1.0,0.0001881,0.0,4.2e-05,0.0,0.000126,0.0,0.0001672
mmlu-high-school-biology.val.244,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 0, cost: 0.1
Model D",0.0,0.0,0.0,2.85e-05,0.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,1.0,1.9e-05,0.0,5.7e-05,0.0,7.6e-05
mmlu-miscellaneous.val.530,Model C,0.0,0.0,1.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,0.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
hellaswag.val.8021,claude-v2,0.0,0.002344,0.0,8.73e-05,0.0,0.0002344,0.0,0.002344,0.0,0.002344,0.0,0.000292,0.0,0.00293,0.0,0.000226592,0.0,0.0002628,0.0,5.84e-05,0.0,0.0001752,0.0,0.0002328
mmlu-professional-law.val.1480,B,0.0,0.0,1.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.000248,1.0,0.00249,0.0,0.000192448,0.0,0.0002232,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
mbpp.dev.226,llama-2-70b-chat,0.0,0.0,0.0,8.850000000000001e-05,0.0,0.0005336,0.0,0.006608,0.0,0.006152,1.0,0.00036,0.0,0.00967,0.0,0.000295656,0.0,0.0004401,0.0,4.46e-05,0.0,0.0002094,0.0,0.0003496
mmlu-professional-law.val.962,Llama-2-70b-chat,0.0,0.0,0.0,7.5e-05,1.0,0.0002008,1.0,0.002008,1.0,0.002008,1.0,0.00025,1.0,0.00251,0.0,0.000194,0.0,0.000225,0.0,5e-05,1.0,0.00015,1.0,0.0001992
hellaswag.val.2947,Model C,0.0,0.0,0.0,3.66e-05,0.0,9.84e-05,0.0,0.000984,0.0,0.001008,1.0,0.000124,0.0,0.00123,0.0,9.4672e-05,0.0,0.0001098,0.0,2.44e-05,0.0,7.32e-05,1.0,9.68e-05
grade-school-math.dev.313,meta/llama-2-70b-chat,0.75,0.0003546,0.5,0.0001389,0.75,0.0005168,0.75,0.004088,0.5,0.004328,0.5,0.00057,0.75,0.00622,0.25,0.000235128,0.75,0.0003546,0.75,6.620000000000001e-05,0.5,0.0001812,0.75,0.000344
hellaswag.val.9827,Meta/llama-2-70b-chat,0.0,0.0,1.0,6.69e-05,0.0,0.0001816,1.0,0.001792,0.0,0.001792,0.0,0.000223,1.0,0.00227,1.0,0.000173048,0.0,0.0002007,1.0,4.460000000000001e-05,1.0,0.0001338,1.0,0.0001776
hellaswag.val.3053,Model D,0.0,0.0,0.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000109,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,0.0,6.42e-05,1.0,8.48e-05
mmlu-professional-law.val.1266,claude-v2,1.0,0.002424,0.0,9.06e-05,1.0,0.0002424,1.0,0.002424,1.0,0.002424,1.0,0.0003019999999999,1.0,0.00306,0.0,0.000234352,0.0,0.0002718,0.0,6.04e-05,0.0,0.0001812,1.0,0.0002408
grade-school-math.dev.2267,meta/llama-2-70b-chat,0.25,0.0006939,0.75,0.0001851,0.75,0.00078,0.5,0.0056159999999999,0.25,0.006912,0.75,0.000652,0.75,0.00984,0.75,0.00039576,0.25,0.0006939,0.25,0.0001222,0.25,0.0004284,0.25,0.0002736
hellaswag.val.9655,D,0.0,0.0,0.0,7.35e-05,0.0,0.0001992,1.0,0.001968,1.0,0.001968,1.0,0.000245,0.0,0.00249,0.0,0.00019012,1.0,0.0002196,0.0,4.9000000000000005e-05,1.0,0.000147,0.0,0.0001952
hellaswag.val.4583,Model C,0.0,0.0,0.0,8.94e-05,1.0,0.0002416,1.0,0.002392,1.0,0.002392,1.0,0.0003,1.0,0.00302,0.0,0.0002312479999999,1.0,0.0002673,0.0,5.9600000000000005e-05,1.0,0.0001788,1.0,0.0002376
abstract2title.test.100,meta/llama-2-70b-chat,0.0,0.0004914,1.0,7.38e-05,1.0,0.0002568,1.0,0.00276,1.0,0.002832,1.0,0.000282,1.0,0.00306,1.0,0.00018236,0.0,0.0004914,1.0,5e-05,1.0,0.0001494,1.0,0.000196
hellaswag.val.2262,claude-v2,0.0,0.000816,1.0,3.03e-05,1.0,8.16e-05,0.0,0.000816,0.0,0.000816,1.0,0.000103,1.0,0.00102,0.0,7.8376e-05,1.0,9e-05,0.0,2.02e-05,1.0,6.06e-05,0.0,8e-05
winogrande.dev.926,Model B,0.0,0.0,1.0,1.53e-05,0.0,4.16e-05,1.0,0.000416,1.0,0.000416,0.0,5.1e-05,0.0,0.00055,0.0,3.9576e-05,1.0,4.59e-05,0.0,1.02e-05,0.0,3.06e-05,1.0,4.08e-05
arc-challenge.test.319,Model B,0.0,0.0,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,8.2e-05,1.0,0.00081,0.0,6.208e-05,1.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
arc-challenge.test.103,Model D,0.0,0.0,0.0,2.3400000000000003e-05,1.0,6.32e-05,1.0,0.000632,1.0,0.000632,1.0,8e-05,1.0,0.00082,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,1.0,6.16e-05
hellaswag.val.8819,claude-v2,0.0,0.002328,0.0,8.699999999999999e-05,0.0,0.0002328,0.0,0.002328,0.0,0.002328,0.0,0.00029,0.0,0.00294,0.0,0.00022504,0.0,0.0002601,0.0,5.800000000000001e-05,0.0,0.0001739999999999,1.0,0.0002312
mmlu-anatomy.val.96,Model C,0.0,0.0,1.0,2.55e-05,0.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00089,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,0.0,5.1e-05,0.0,6.8e-05
hellaswag.val.196,C) claude-v2,0.0,0.0,0.0,3.3600000000000004e-05,1.0,9.04e-05,1.0,0.000904,1.0,0.000904,1.0,0.000112,1.0,0.00113,0.0,8.6912e-05,1.0,9.99e-05,0.0,2.24e-05,0.0,6.720000000000001e-05,1.0,8.96e-05
grade-school-math.dev.3386,meta/llama-2-70b-chat,0.25,0.000414,0.25,0.0001521,0.75,0.0006048,0.25,0.00396,0.25,0.0044639999999999,0.75,0.000553,0.75,0.0066599999999999,0.25,0.000342216,0.25,0.000414,0.25,0.0001022,0.25,0.0003474,0.25,0.0003944
hellaswag.val.8201,Model C,0.0,0.0,0.0,8.13e-05,0.0,0.0002176,0.0,0.002176,0.0,0.002176,0.0,0.0002729999999999,0.0,0.00275,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-moral-disputes.val.140,Model C,0.0,0.0,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,1.0,7.520000000000001e-05
arc-challenge.test.830,Model B,0.0,0.0,1.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,1.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
grade-school-math.dev.1145,meta/llama-2-70b-chat,0.5,0.0003599999999999,0.5,0.0001418999999999,0.25,0.0002704,0.75,0.004528,0.5,0.00436,0.5,0.000406,0.75,0.00713,0.75,0.000295656,0.5,0.0003599999999999,0.25,6.0200000000000006e-05,0.25,0.0002556,0.75,0.0003816
mmlu-philosophy.val.213,Model C,0.0,0.0,0.0,2.3400000000000003e-05,0.0,6.32e-05,1.0,0.000632,1.0,0.000632,0.0,7.8e-05,1.0,0.00082,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,1.0,6.16e-05
hellaswag.val.7120,C) claude-v2,0.0,0.0,0.0,6.48e-05,0.0,0.0001736,0.0,0.001736,1.0,0.001736,0.0,0.000216,1.0,0.0022,0.0,0.000167616,1.0,0.0001943999999999,0.0,4.3200000000000007e-05,0.0,0.0001296,1.0,0.000172
hellaswag.val.3441,B,0.0,0.0,0.0,6.69e-05,1.0,0.0001792,1.0,0.001792,0.0,0.001816,0.0,0.000223,1.0,0.00224,0.0,0.000173048,0.0,0.0001998,0.0,4.460000000000001e-05,0.0,0.0001338,1.0,0.0001776
mmlu-world-religions.val.77,D) Sri (Lakshmi),0.0,0.0,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,1.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
arc-challenge.test.764,B-model,0.0,0.0,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,0.000101,1.0,0.001,0.0,7.682400000000001e-05,1.0,8.91e-05,0.0,1.98e-05,1.0,5.94e-05,1.0,7.840000000000001e-05
mmlu-professional-law.val.1325,claude-v2,1.0,0.002568,0.0,9.6e-05,1.0,0.0002568,1.0,0.002568,1.0,0.002568,0.0,0.0003199999999999,1.0,0.00321,0.0,0.00024832,0.0,0.0002879999999999,0.0,6.4e-05,0.0,0.0001919999999999,1.0,0.0002552
hellaswag.val.3146,claude-v2,0.0,0.000816,0.0,3.03e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.000103,1.0,0.00102,1.0,7.8376e-05,0.0,9.09e-05,1.0,2.02e-05,0.0,6.06e-05,1.0,8e-05
grade-school-math.dev.1692,meta/llama-2-70b-chat,0.25,0.0004464,0.25,0.000168,0.25,0.000832,0.25,0.005776,0.25,0.0076,0.0,0.000877,0.5,0.01115,0.25,0.000502072,0.25,0.0004464,0.25,0.000111,0.25,0.0003432,0.25,0.0005328
hellaswag.val.6066,A) or D),0.0,0.0,0.0,7.26e-05,1.0,0.0001944,1.0,0.001944,0.0,0.001968,1.0,0.000242,1.0,0.00243,0.0,0.000187792,0.0,0.0002177999999999,0.0,4.84e-05,0.0,0.0001452,0.0,0.0001928
grade-school-math.dev.3130,meta/llama-2-70b-chat,0.5,0.0003807,0.5,0.0001449,0.75,0.000456,0.25,0.004944,0.5,0.006096,0.5,0.000354,0.5,0.0069,0.75,0.000294104,0.5,0.0003807,0.5,0.0001006,0.25,0.0002177999999999,0.75,0.0003296
grade-school-math.dev.5970,meta/llama-2-70b-chat,0.75,0.0003744,0.75,0.0001539,0.25,0.0005928,0.75,0.0045839999999999,0.0,0.006456,0.75,0.000503,0.75,0.0078,0.5,0.000308072,0.75,0.0003744,0.75,8.280000000000001e-05,0.5,0.0002478,0.25,0.000228
hellaswag.val.6884,B,0.0,0.0,0.0,7.86e-05,1.0,0.0002104,1.0,0.002104,1.0,0.0021279999999999,1.0,0.0002639999999999,1.0,0.00263,0.0,0.000203312,0.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
hellaswag.val.6534,Model B,0.0,0.0,0.0,7.74e-05,1.0,0.0002096,0.0,0.002072,1.0,0.002072,1.0,0.00026,0.0,0.00262,0.0,0.000200208,1.0,0.0002322,0.0,5.160000000000001e-05,0.0,0.0001548,1.0,0.0002056
mmlu-professional-law.val.892,claude-v2,1.0,0.002208,0.0,8.249999999999999e-05,1.0,0.0002208,1.0,0.002208,1.0,0.002208,0.0,0.000275,1.0,0.00276,0.0,0.0002134,0.0,0.0002475,0.0,5.5e-05,0.0,0.0001649999999999,1.0,0.0002192
hellaswag.val.8173,AI-Claude-v2,0.0,0.0,1.0,7.74e-05,1.0,0.0002072,1.0,0.002072,1.0,0.002072,0.0,0.000258,1.0,0.00262,1.0,0.000200208,1.0,0.0002313,1.0,5.160000000000001e-05,1.0,0.0001548,1.0,0.0002056
chinese_zodiac.dev.244,meta/llama-2-70b-chat,0.0,0.0001152,0.0,3.3600000000000004e-05,0.0,9.28e-05,1.0,0.000928,1.0,0.000928,0.0,0.000114,1.0,0.00116,0.0,0.000114848,0.0,0.0001152,0.0,2.24e-05,1.0,6.720000000000001e-05,0.0,9.36e-05
mmlu-high-school-computer-science.val.98,Model C,0.0,0.0,1.0,4.86e-05,1.0,0.0001304,1.0,0.001304,1.0,0.001304,1.0,0.000162,1.0,0.00163,0.0,0.000125712,0.0,0.0001458,0.0,3.24e-05,1.0,9.72e-05,1.0,0.0001288
grade-school-math.dev.5265,meta/llama-2-70b-chat,0.25,0.0003753,0.5,0.0001487999999999,0.25,0.000576,0.75,0.00432,0.5,0.00528,0.75,0.000534,0.75,0.00825,0.25,0.000351528,0.25,0.0003753,0.25,8.14e-05,0.25,0.0002604,0.25,0.0003544
mmlu-elementary-mathematics.val.140,Model C,0.0,0.0,0.0,3.27e-05,0.0,8.800000000000001e-05,0.0,0.00088,0.0,0.00088,0.0,0.0001089999999999,0.0,0.0011,0.0,8.4584e-05,0.0,9.81e-05,0.0,2.18e-05,0.0,6.54e-05,0.0,8.72e-05
grade-school-math.dev.2580,meta/llama-2-70b-chat,0.25,0.0003447,0.75,0.0001254,0.25,0.0002216,0.5,0.0043999999999999,0.75,0.0042559999999999,0.5,0.0004069999999999,0.75,0.00796,0.25,0.0002925519999999,0.25,0.0003447,0.25,6.8e-05,0.25,0.0002394,0.25,0.0002216
hellaswag.val.3947,D) meta/llama-2-70b-chat,0.0,0.0,0.0,7.979999999999999e-05,1.0,0.0002144,0.0,0.002144,0.0,0.002144,1.0,0.000267,1.0,0.00268,0.0,0.000207192,1.0,0.0002394,0.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
hellaswag.val.4504,B,0.0,0.0,1.0,7.29e-05,0.0,0.0001952,1.0,0.001952,1.0,0.001952,0.0,0.000245,1.0,0.00244,1.0,0.000188568,0.0,0.0002178,1.0,4.860000000000001e-05,1.0,0.0001458,1.0,0.0001936
mmlu-professional-law.val.127,claude-v2,0.0,0.0036,1.0,0.0001347,0.0,0.00036,0.0,0.0036,0.0,0.0036,0.0,0.000449,0.0,0.0045,0.0,0.000348424,0.0,0.0004041,1.0,8.98e-05,1.0,0.0002694,0.0,0.0003592
mmlu-security-studies.val.209,A,0.0,0.0,1.0,4.89e-05,1.0,0.0001312,1.0,0.001312,1.0,0.001312,1.0,0.000163,1.0,0.00164,0.0,0.0001264879999999,0.0,0.0001467,1.0,3.2600000000000006e-05,1.0,9.78e-05,1.0,0.0001296
bias_detection.dev.46,"Model C — correctness: 1, cost: 0.3

Reasoning: The prompt requires classification of a sentence from a news article into specific categories such as fact, opinion, claim, data, quote, narrative,",0.0,0.0,0.0,0.0001269,0.0,0.0003456,0.0,0.004608,0.0,0.006336,0.0,0.00039,0.0,0.01005,0.0,0.000261512,0.0,0.0003591,0.0,6.94e-05,0.0,0.000228,0.0,0.0002152
abstract2title.test.180,meta/llama-2-70b-chat,1.0,0.0003266999999999,1.0,7.11e-05,1.0,0.0002464,1.0,0.00244,1.0,0.002464,1.0,0.000246,1.0,0.00284,1.0,0.000168392,1.0,0.0003266999999999,1.0,4.4800000000000005e-05,1.0,0.000135,1.0,0.0001816
abstract2title.test.72,meta/llama-2-70b-chat,1.0,0.0006948,1.0,0.0001307999999999,1.0,0.0004128,1.0,0.004152,1.0,0.004344,1.0,0.00046,1.0,0.0051,1.0,0.000339112,1.0,0.0006948,1.0,8.78e-05,1.0,0.000267,1.0,0.0003544
winogrande.dev.165,Model B,0.0,0.0,0.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,1.0,0.000392,1.0,5e-05,1.0,0.00052,0.0,3.7248e-05,1.0,4.23e-05,0.0,9.6e-06,0.0,2.8799999999999995e-05,0.0,3.7600000000000006e-05
hellaswag.val.8193,C,0.0,0.0,0.0,8.01e-05,1.0,0.0002152,1.0,0.002152,1.0,0.002152,1.0,0.000268,1.0,0.00269,0.0,0.0002079679999999,0.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
hellaswag.val.6258,claude-v2,0.0,0.002064,0.0,7.71e-05,0.0,0.0002064,0.0,0.002064,0.0,0.002064,0.0,0.000259,0.0,0.00258,0.0,0.0001994319999999,0.0,0.0002304,0.0,5.14e-05,0.0,0.0001542,1.0,0.0002048
grade-school-math.dev.2751,meta/llama-2-70b-chat,0.5,0.0003411,0.75,0.0001505999999999,0.75,0.000544,0.75,0.004792,0.75,0.004984,0.75,0.000389,0.5,0.0063199999999999,0.75,0.00028712,0.5,0.0003411,0.75,8.180000000000001e-05,0.75,0.0002508,0.75,0.000344
hellaswag.val.5432,claude-v2,0.0,0.001896,1.0,7.08e-05,0.0,0.0001896,0.0,0.001896,0.0,0.001896,0.0,0.000236,1.0,0.0024,1.0,0.000183136,0.0,0.0002115,1.0,4.720000000000001e-05,1.0,0.0001416,1.0,0.000188
mbpp.dev.305,meta/llama-2-70b-chat,0.0,0.0003672,1.0,5.88e-05,1.0,0.0005104,1.0,0.002344,1.0,0.006688,1.0,0.000442,1.0,0.00941,1.0,0.000101656,0.0,0.0003672,1.0,5.86e-05,1.0,0.0001074,1.0,0.0002184
mmlu-abstract-algebra.val.6,D) meta/llama-2-70b-chat,0.0,0.0,1.0,3.4200000000000005e-05,0.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.0001139999999999,0.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001026,0.0,2.28e-05,0.0,6.840000000000001e-05,1.0,9.040000000000002e-05
winogrande.dev.26,Model B,0.0,0.0,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,1.0,0.000392,0.0,5e-05,1.0,0.00052,0.0,3.7248e-05,1.0,4.32e-05,0.0,9.6e-06,0.0,2.8799999999999995e-05,1.0,3.8400000000000005e-05
hellaswag.val.8083,Model C,0.0,0.0,1.0,7.049999999999999e-05,1.0,0.000192,0.0,0.001896,0.0,0.001896,0.0,0.000236,1.0,0.00237,1.0,0.000183136,1.0,0.0002115,1.0,4.720000000000001e-05,1.0,0.0001416,1.0,0.000188
chinese_famous_novel.dev.17,meta/llama-2-70b-chat,0.0,5.0400000000000005e-05,0.0,1.2899999999999998e-05,1.0,3.76e-05,1.0,0.000376,1.0,0.000376,0.0,4.1e-05,1.0,0.00047,0.0,2.8712e-05,0.0,5.0400000000000005e-05,0.0,8.400000000000001e-06,0.0,3.42e-05,0.0,4.56e-05
mmlu-professional-law.val.723,claude-v2,1.0,0.002136,0.0,7.98e-05,0.0,0.0002136,0.0,0.002136,1.0,0.002136,0.0,0.000266,0.0,0.00267,0.0,0.0002064159999999,0.0,0.0002394,0.0,5.3200000000000006e-05,0.0,0.0001596,1.0,0.000212
mmlu-electrical-engineering.val.96,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1

Correct choice: Model A",0.0,0.0,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,0.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,1.0,1.8e-05,1.0,5.4e-05,1.0,7.2e-05
hellaswag.val.2837,claude-v2,0.0,0.000904,0.0,3.3600000000000004e-05,1.0,9.04e-05,0.0,0.000904,0.0,0.000904,0.0,0.000112,1.0,0.00113,0.0,8.6912e-05,1.0,9.99e-05,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,8.88e-05
grade-school-math.dev.2124,meta/llama-2-70b-chat,0.25,0.0003618,0.5,0.0002034,0.5,0.000572,0.75,0.004856,0.75,0.005264,0.75,0.000518,0.5,0.00658,0.5,0.000281688,0.25,0.0003618,0.25,7.42e-05,0.5,0.0002964,0.75,0.0003408
winogrande.dev.1010,Model B,0.0,0.0,1.0,1.5e-05,1.0,4.08e-05,1.0,0.000408,0.0,0.000408,1.0,5.2e-05,1.0,0.00051,0.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,1.0,3e-05,1.0,3.92e-05
grade-school-math.dev.3728,meta/llama-2-70b-chat,0.25,0.0004293,0.5,0.0002049,0.75,0.0006648,0.75,0.007176,0.75,0.00756,0.5,0.000535,0.75,0.00942,0.25,0.000472584,0.25,0.0004293,0.25,9.82e-05,0.25,0.000288,0.75,0.0004344
grade-school-math.dev.6052,meta/llama-2-70b-chat,0.5,0.0003186,0.5,0.0001347,0.5,0.0005264,0.5,0.005,0.5,0.005672,0.75,0.000405,0.75,0.00652,0.5,0.000355408,0.5,0.0003186,0.25,9.100000000000002e-05,0.75,0.0002562,0.75,0.0004056
mmlu-security-studies.val.173,claude-v2,1.0,0.002112,0.0,7.89e-05,1.0,0.0002112,1.0,0.002112,1.0,0.002112,0.0,0.000263,0.0,0.00264,0.0,0.000204088,0.0,0.0002367,0.0,5.260000000000001e-05,1.0,0.0001578,1.0,0.0002096
mmlu-professional-law.val.1294,claude-v2,0.0,0.00332,1.0,0.0001241999999999,0.0,0.000332,0.0,0.00332,0.0,0.00332,0.0,0.000414,1.0,0.00415,0.0,0.000321264,0.0,0.0003726,0.0,8.280000000000001e-05,0.0,0.0002483999999999,0.0,0.0003304
hellaswag.val.586,B) claude-v2,0.0,0.0,0.0,3e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.000103,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,0.0,6.06e-05,0.0,8e-05
hellaswag.val.6613,claude-v2,0.0,0.00236,0.0,8.819999999999999e-05,0.0,0.000236,0.0,0.00236,0.0,0.00236,0.0,0.000294,1.0,0.00295,0.0,0.000228144,1.0,0.0002637,0.0,5.8800000000000006e-05,0.0,0.0001757999999999,1.0,0.0002344
Chinese_character_riddles.dev.62,"Model C — correctness: 1, cost: 0.3

Here's the reasoning: The prompt requires understanding Chinese characters, their formation, pronunciation, meaning, and how they can be combined. Model C, being",0.0,0.0,0.0,6.93e-05,0.0,8.400000000000001e-05,0.0,0.002256,0.0,0.00372,0.0,0.000198,0.0,0.01011,0.0,8.846400000000001e-05,0.0,0.0002727,0.0,4.7600000000000005e-05,0.0,0.0001158,0.0,0.000216
hellaswag.val.3339,Model C,0.0,0.0,0.0,7.17e-05,0.0,0.0001944,0.0,0.0019199999999999,0.0,0.0019199999999999,0.0,0.0002409999999999,1.0,0.00243,0.0,0.0001854639999999,0.0,0.0002151,0.0,4.780000000000001e-05,0.0,0.0001434,0.0,0.0001904
hellaswag.val.446,B,0.0,0.0,1.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000856,1.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
mbpp.dev.339,meta/llama-2-70b-chat,0.0,0.0002826,0.0,9.39e-05,0.0,0.0004487999999999,0.0,0.002784,0.0,0.005208,0.0,0.000347,0.0,0.00573,0.0,9.6224e-05,0.0,0.0002826,0.0,5.480000000000001e-05,0.0,0.0001793999999999,0.0,0.000144
mmlu-professional-law.val.935,claude-v2,1.0,0.002416,1.0,9.03e-05,1.0,0.0002416,1.0,0.002416,1.0,0.002416,0.0,0.000301,1.0,0.00302,0.0,0.000233576,0.0,0.0002709,0.0,6.0200000000000006e-05,1.0,0.0001806,1.0,0.00024
consensus_summary.dev.110,Meta/llama-2-70b-chat,0.0,0.0,0.75,0.0001184999999999,1.0,0.0003008,0.25,0.001952,0.75,0.00356,0.75,0.000358,0.75,0.00688,0.75,0.000350752,0.75,0.0003321,0.75,7.24e-05,0.75,0.0002873999999999,0.75,0.0002104
grade-school-math.dev.5675,meta/llama-2-70b-chat,0.5,0.0002952,0.5,0.0001341,0.75,0.0004304,0.5,0.003632,0.75,0.005552,0.5,0.0003369999999999,0.5,0.00568,0.5,0.00024832,0.5,0.0002952,0.75,7.02e-05,1.0,0.0001728,0.75,0.0003032
hellaswag.val.8543,A,0.0,0.0,1.0,8.34e-05,1.0,0.0002232,1.0,0.002232,1.0,0.002232,0.0,0.00028,1.0,0.00279,1.0,0.000215728,1.0,0.0002493,1.0,5.56e-05,0.0,0.0001668,1.0,0.0002216
mmlu-electrical-engineering.val.48,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 1, cost: 0.7
Model D",0.0,0.0,0.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,1.0,7.5e-05,0.0,0.00079,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,0.0,6e-05
hellaswag.val.7667,C,0.0,0.0,1.0,7.649999999999999e-05,1.0,0.0002072,1.0,0.002048,0.0,0.002048,0.0,0.000255,1.0,0.00259,1.0,0.00019788,0.0,0.0002295,1.0,5.1000000000000006e-05,1.0,0.0001529999999999,1.0,0.0002032
hellaswag.val.3854,C,0.0,0.0,1.0,8.79e-05,0.0,0.0002352,0.0,0.002352,0.0,0.002352,0.0,0.000295,1.0,0.00297,1.0,0.0002273679999999,1.0,0.0002637,1.0,5.860000000000001e-05,1.0,0.0001758,1.0,0.0002336
grade-school-math.dev.4618,meta/llama-2-70b-chat,0.5,0.0004356,0.5,0.0001752,0.75,0.0006416,0.75,0.004736,0.75,0.005816,0.75,0.0004529999999999,0.5,0.00994,0.5,0.000405072,0.5,0.0004356,0.75,8.060000000000001e-05,0.25,0.0002556,0.75,0.0003968
chinese_shi_jing.test.5,meta/llama-2-70b-chat,0.0,0.0001134,0.0,3e-05,0.0,8.960000000000001e-05,0.0,0.000944,0.0,0.000896,0.0,0.000104,0.0,0.00115,0.0,7.915199999999999e-05,0.0,0.0001134,0.0,1.9200000000000003e-05,0.0,5.879999999999999e-05,0.0,0.0001624
mmlu-sociology.val.180,Model B - claude-v2,0.0,0.0,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.400000000000001e-05
arc-challenge.val.224,Model C,0.0,0.0,0.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00073,1.0,5.5872e-05,0.0,6.48e-05,1.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
grade-school-math.dev.6784,meta/llama-2-70b-chat,0.75,0.000306,0.75,0.0001305,0.75,0.00052,0.75,0.004144,0.75,0.004912,0.75,0.000354,0.75,0.0076399999999999,0.75,0.000282464,0.75,0.000306,0.25,9.86e-05,0.5,0.0002063999999999,0.5,0.0003472
mmlu-sociology.val.104,Model C,0.0,0.0,1.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,0.0,0.000128,1.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,0.0,2.56e-05,1.0,7.68e-05,1.0,0.0001016
mmlu-professional-psychology.val.489,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 0, cost: 0.3
Model D",0.0,0.0,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,1.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
mmlu-professional-law.val.568,claude-v2,0.0,0.001968,0.0,7.35e-05,0.0,0.0001968,0.0,0.001968,0.0,0.001968,0.0,0.000245,1.0,0.00246,0.0,0.00019012,0.0,0.0002205,1.0,4.9000000000000005e-05,0.0,0.000147,0.0,0.0001952
mmlu-clinical-knowledge.val.138,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00098,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.76e-05
mmlu-jurisprudence.val.82,Model D,0.0,0.0,0.0,2.61e-05,1.0,7.04e-05,0.0,0.000704,0.0,0.000704,0.0,8.7e-05,0.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,0.0,5.22e-05,0.0,6.96e-05
hellaswag.val.3285,claude-v2,1.0,0.001968,0.0,7.35e-05,1.0,0.0001968,0.0,0.001968,1.0,0.001968,1.0,0.000247,1.0,0.00249,0.0,0.00019012,0.0,0.0002205,0.0,4.9000000000000005e-05,0.0,0.000147,1.0,0.0001952
hellaswag.val.8901,Model B,0.0,0.0,0.0,8.34e-05,0.0,0.0002256,1.0,0.002232,1.0,0.002232,0.0,0.00028,1.0,0.00282,0.0,0.000215728,1.0,0.0002502,0.0,5.56e-05,0.0,0.0001668,0.0,0.0002216
mmlu-professional-psychology.val.234,Model B,0.0,0.0,1.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
hellaswag.val.2962,Model D,0.0,0.0,1.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.000112,1.0,0.00111,0.0,8.536000000000001e-05,1.0,9.81e-05,0.0,2.2e-05,1.0,6.6e-05,1.0,8.8e-05
hellaswag.val.7011,Model C,0.0,0.0,1.0,8.669999999999998e-05,0.0,0.0002352,1.0,0.002328,0.0,0.002328,0.0,0.000292,1.0,0.00294,1.0,0.00022504,1.0,0.0002601,1.0,5.800000000000001e-05,1.0,0.0001739999999999,0.0,0.0002312
grade-school-math.dev.7380,meta/llama-2-70b-chat,0.75,0.0003177,0.75,0.0001404,0.75,0.0004408,0.75,0.003784,0.75,0.006232,0.75,0.00041,0.75,0.00701,0.75,0.000318936,0.75,0.0003177,0.5,8.620000000000001e-05,1.0,0.0001872,0.75,0.0003504
grade-school-math.dev.2491,meta/llama-2-70b-chat,0.25,0.000459,0.75,0.0002063999999999,0.25,0.000512,0.5,0.0049999999999999,0.5,0.007904,0.25,0.0006249999999999,0.75,0.0124,0.25,0.000349976,0.25,0.000459,0.25,0.0001132,0.75,0.0002622,0.5,0.0004416
mmlu-college-mathematics.val.70,Model C,0.0,0.0,1.0,5.55e-05,0.0,0.0001488,0.0,0.001488,0.0,0.001488,0.0,0.000185,0.0,0.00186,0.0,0.00014356,0.0,0.0001665,0.0,3.7000000000000005e-05,0.0,0.0001104,0.0,0.000148
mmlu-professional-law.val.1220,meta/llama-2-70b-chat,0.0,0.0001943999999999,0.0,6.48e-05,1.0,0.0001736,1.0,0.001736,1.0,0.001736,1.0,0.0002179999999999,1.0,0.00217,0.0,0.000167616,0.0,0.0001943999999999,0.0,4.3200000000000007e-05,0.0,0.0001296,1.0,0.000172
hellaswag.val.5188,Meta/llama-2-70b-chat,0.0,0.0,0.0,6.66e-05,0.0,0.0001808,0.0,0.001784,0.0,0.001784,1.0,0.000224,1.0,0.00223,0.0,0.0001722719999999,0.0,0.0001988999999999,0.0,4.44e-05,0.0,0.0001332,1.0,0.0001768
mmlu-philosophy.val.91,meta/llama-2-70b-chat,0.0,0.0001718999999999,0.0,5.73e-05,1.0,0.0001536,1.0,0.001536,1.0,0.001536,1.0,0.000191,1.0,0.00192,0.0,0.000148216,0.0,0.0001718999999999,0.0,3.820000000000001e-05,1.0,0.0001146,1.0,0.000152
mmlu-college-medicine.val.65,Model B,0.0,0.0,1.0,2.58e-05,0.0,6.960000000000001e-05,1.0,0.000696,0.0,0.000696,1.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,1.0,5.16e-05,0.0,6.800000000000001e-05
mmlu-moral-scenarios.val.784,"B) Wrong, Wrong",0.0,0.0,0.0,4.08e-05,0.0,0.0001096,0.0,0.001096,0.0,0.001096,0.0,0.000136,1.0,0.0014,0.0,0.000105536,0.0,0.0001224,0.0,2.72e-05,0.0,8.159999999999999e-05,1.0,0.000108
mmlu-prehistory.val.138,Model D,0.0,0.0,1.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
grade-school-math.dev.591,meta/llama-2-70b-chat,0.75,0.0004266,0.25,0.0001574999999999,0.75,0.0005632,0.75,0.005128,0.75,0.006808,0.75,0.000777,0.75,0.00977,0.25,0.000309624,0.75,0.0004266,0.75,9.92e-05,0.75,0.0002934,0.75,0.0004072
hellaswag.val.9941,D,0.0,0.0,0.0,9.06e-05,1.0,0.0002432,0.0,0.002432,1.0,0.002432,1.0,0.000305,1.0,0.00307,0.0,0.0002351279999999,0.0,0.0002727,0.0,6.06e-05,0.0,0.0001818,1.0,0.0002416
hellaswag.val.1988,B) claude-v2,0.0,0.0,0.0,3.48e-05,0.0,9.36e-05,1.0,0.000936,0.0,0.000936,1.0,0.000118,1.0,0.00117,1.0,9.0016e-05,0.0,0.0001035,1.0,2.32e-05,1.0,6.96e-05,1.0,9.2e-05
mmlu-professional-law.val.1341,claude-v2,0.0,0.001776,0.0,6.63e-05,1.0,0.0001776,0.0,0.001776,0.0,0.001776,0.0,0.000221,0.0,0.00222,0.0,0.000171496,0.0,0.0001988999999999,0.0,4.420000000000001e-05,0.0,0.0001326,0.0,0.0001768
grade-school-math.dev.6748,meta/llama-2-70b-chat,0.25,0.0004275,0.25,0.0001955999999999,0.25,0.0005696,0.75,0.005984,0.75,0.006824,0.75,0.000615,0.5,0.00928,0.25,0.000366272,0.25,0.0004275,0.5,0.0001082,0.25,0.000243,0.25,0.0004496
winogrande.dev.284,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Correct choice: Model A",0.0,0.0,1.0,1.6800000000000002e-05,1.0,4.56e-05,1.0,0.0004559999999999,1.0,0.0004559999999999,1.0,5.8e-05,1.0,0.00057,1.0,4.3456000000000005e-05,0.0,4.95e-05,1.0,1.12e-05,1.0,3.3600000000000004e-05,1.0,4.4e-05
grade-school-math.dev.212,meta/llama-2-70b-chat,0.25,0.0005526,0.25,0.0002153999999999,0.25,0.0008376,0.75,0.005472,0.25,0.008112,0.75,0.000742,0.75,0.01128,0.25,0.000450856,0.25,0.0005526,0.25,0.0001309999999999,0.25,0.0003138,0.25,0.0005392
hellaswag.val.9812,C) meta/llama-2-70b-chat,0.0,0.0,0.0,7.26e-05,1.0,0.0001944,0.0,0.001944,1.0,0.001944,0.0,0.000244,1.0,0.00243,0.0,0.000187792,1.0,0.0002177999999999,0.0,4.84e-05,1.0,0.0001452,1.0,0.0001928
mmlu-electrical-engineering.val.72,claude-v2,1.0,0.000608,0.0,2.25e-05,0.0,6.08e-05,1.0,0.000608,1.0,0.000608,0.0,7.5e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,1.0,4.5e-05,0.0,6e-05
mmlu-world-religions.val.27,Model C,0.0,0.0,0.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,0.0,0.000592,1.0,7.3e-05,1.0,0.00077,0.0,5.6648e-05,0.0,6.57e-05,1.0,1.46e-05,1.0,4.38e-05,1.0,5.84e-05
hellaswag.val.7217,Model C,0.0,0.0,1.0,8.07e-05,0.0,0.000216,0.0,0.00216,1.0,0.00216,0.0,0.000269,0.0,0.0027,0.0,0.000208744,0.0,0.0002412,1.0,5.380000000000001e-05,1.0,0.0001614,1.0,0.0002144
grade-school-math.dev.2547,meta/llama-2-70b-chat,0.75,0.0002943,0.75,0.0001307999999999,0.75,0.0005368,0.75,0.0035919999999999,0.75,0.004456,0.75,0.0004089999999999,0.5,0.00515,0.75,0.000225816,0.75,0.0002943,0.75,6.400000000000001e-05,0.75,0.0002177999999999,0.75,0.0003256
mmlu-professional-law.val.471,meta/llama-2-70b-chat,0.0,0.0002151,1.0,7.17e-05,0.0,0.000192,0.0,0.0019199999999999,0.0,0.0019199999999999,0.0,0.0002389999999999,1.0,0.0024,0.0,0.0001854639999999,0.0,0.0002151,0.0,4.780000000000001e-05,0.0,0.0001434,0.0,0.0001904
mmlu-elementary-mathematics.val.216,Model C,0.0,0.0,0.0,3.39e-05,0.0,9.120000000000002e-05,0.0,0.000912,0.0,0.000912,0.0,0.000115,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,0.0,2.2600000000000004e-05,0.0,6.78e-05,1.0,8.960000000000002e-05
grade-school-math.dev.1385,meta/llama-2-70b-chat,0.25,0.0004877999999999,0.25,0.0002007,0.25,0.0008504,0.25,0.006368,0.25,0.009176,0.75,0.0007869999999999,0.75,0.0125499999999999,0.25,0.000433784,0.25,0.0004877999999999,0.25,0.0001064,0.75,0.0002729999999999,0.25,0.0006192
hellaswag.val.4329,A) WizardLM/WizardLM-13B-V1.2,0.0,0.0,1.0,6.93e-05,1.0,0.000188,1.0,0.001856,1.0,0.001856,1.0,0.000233,1.0,0.00235,1.0,0.000179256,1.0,0.000207,1.0,4.6200000000000005e-05,1.0,0.0001386,1.0,0.000184
mmlu-miscellaneous.val.208,Model C,0.0,0.0,0.0,2.82e-05,1.0,7.600000000000002e-05,1.0,0.00076,0.0,0.00076,1.0,9.4e-05,0.0,0.00095,0.0,7.2944e-05,0.0,8.46e-05,0.0,1.8800000000000003e-05,1.0,5.64e-05,0.0,7.520000000000001e-05
hellaswag.val.1772,Model C,0.0,0.0,1.0,4.32e-05,1.0,0.000116,1.0,0.00116,1.0,0.00116,1.0,0.000146,1.0,0.00145,0.0,0.000111744,0.0,0.0001295999999999,1.0,2.88e-05,1.0,8.64e-05,0.0,0.0001144
winogrande.dev.1140,meta/llama-2-70b-chat,1.0,4.410000000000001e-05,1.0,1.4699999999999998e-05,1.0,4e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,1.0,5.1e-05,0.0,0.0005,0.0,3.8024e-05,1.0,4.410000000000001e-05,0.0,9.8e-06,1.0,2.94e-05,0.0,3.84e-05
winogrande.dev.1180,meta/llama-2-70b-chat,1.0,4.2300000000000005e-05,1.0,1.41e-05,1.0,3.84e-05,1.0,0.000384,0.0,0.000384,0.0,4.9000000000000005e-05,0.0,0.00048,0.0,3.6472000000000006e-05,1.0,4.2300000000000005e-05,0.0,9.4e-06,0.0,2.82e-05,1.0,3.76e-05
mmlu-philosophy.val.239,meta/llama-2-70b-chat,0.0,9.09e-05,0.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,1.0,6.06e-05,0.0,8e-05
mmlu-professional-law.val.636,D,0.0,0.0,1.0,9.93e-05,0.0,0.0002656,0.0,0.002656,0.0,0.002656,0.0,0.0003309999999999,0.0,0.00332,0.0,0.0002568559999999,0.0,0.0002979,0.0,6.62e-05,0.0,0.0001986,1.0,0.0002648
hellaswag.val.1292,Model D,0.0,0.0,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,9e-05,1.0,0.00092,0.0,6.828800000000001e-05,1.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,7.039999999999999e-05
mmlu-clinical-knowledge.val.137,Model C,0.0,0.0,1.0,2.85e-05,0.0,7.680000000000001e-05,0.0,0.000768,0.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,1.0,1.9e-05,1.0,5.7e-05,1.0,7.6e-05
grade-school-math.dev.1224,meta/llama-2-70b-chat,0.75,0.0004005,0.25,0.0001676999999999,0.5,0.00054,0.75,0.0054,0.5,0.005904,0.5,0.000497,0.5,0.00987,0.25,0.000312728,0.75,0.0004005,0.25,8.6e-05,0.5,0.0002423999999999,0.25,0.000468
mmlu-marketing.val.75,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,1.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00081,0.0,5.9752000000000007e-05,0.0,6.93e-05,1.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
mmlu-elementary-mathematics.val.60,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,1.0,2.52e-05,1.0,6.800000000000001e-05,0.0,0.00068,0.0,0.00068,1.0,8.4e-05,1.0,0.00088,0.0,6.5184e-05,0.0,7.56e-05,1.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.720000000000001e-05
grade-school-math.dev.7261,meta/llama-2-70b-chat,0.25,0.0004644,0.25,0.000219,0.75,0.0007584,0.5,0.006648,0.5,0.009432,0.25,0.000707,0.5,0.01287,0.5,0.00048112,0.25,0.0004644,0.25,0.0001028,0.25,0.0003984,0.75,0.0005064
mmlu-miscellaneous.val.492,Model D,0.0,0.0,1.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,0.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
mmlu-moral-scenarios.val.7,"C) Not wrong, Wrong",0.0,0.0,0.0,4.47e-05,0.0,0.00012,0.0,0.0012,0.0,0.0012,0.0,0.000149,1.0,0.00153,0.0,0.000115624,0.0,0.0001341,0.0,2.9800000000000003e-05,0.0,8.94e-05,0.0,0.0001192
mmlu-professional-law.val.261,claude-v2,1.0,0.001392,0.0,5.19e-05,0.0,0.0001392,0.0,0.001392,1.0,0.001392,0.0,0.000173,1.0,0.00174,0.0,0.0001342479999999,0.0,0.0001557,0.0,3.460000000000001e-05,0.0,0.0001038,0.0,0.0001376
mmlu-high-school-government-and-politics.val.45,Meta/llama-2-70b-chat,0.0,0.0,1.0,3.75e-05,1.0,0.0001008,1.0,0.001008,1.0,0.001008,1.0,0.000125,1.0,0.00126,0.0,9.7e-05,0.0,0.0001125,1.0,2.5e-05,1.0,7.5e-05,1.0,9.92e-05
hellaswag.val.7332,B,0.0,0.0,0.0,8.01e-05,1.0,0.0002152,0.0,0.002152,0.0,0.002152,0.0,0.00027,1.0,0.00269,0.0,0.0002079679999999,0.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
mmlu-college-computer-science.val.52,meta/llama-2-70b-chat,0.0,0.0001602,1.0,5.34e-05,0.0,0.0001432,0.0,0.001432,0.0,0.001432,0.0,0.000178,1.0,0.00179,0.0,0.0001381279999999,0.0,0.0001602,0.0,3.5600000000000005e-05,1.0,0.0001068,0.0,0.0001424
hellaswag.val.3361,B,0.0,0.0,0.0,7.230000000000001e-05,1.0,0.0001936,1.0,0.001936,0.0,0.001936,0.0,0.000241,1.0,0.00245,0.0,0.000187016,0.0,0.000216,0.0,4.8200000000000006e-05,0.0,0.0001446,1.0,0.000192
grade-school-math.dev.6803,meta/llama-2-70b-chat,0.75,0.0004464,0.75,0.000141,0.75,0.0005191999999999,0.75,0.004424,0.75,0.005648,0.75,0.000494,0.75,0.0075999999999999,0.75,0.000349976,0.75,0.0004464,0.75,8.779999999999999e-05,0.75,0.0002724,0.75,0.00038
mmlu-miscellaneous.val.339,Model C,0.0,0.0,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,1.0,1.48e-05,1.0,4.44e-05,1.0,5.84e-05
winogrande.dev.190,Model B,0.0,0.0,1.0,1.65e-05,1.0,4.720000000000001e-05,1.0,0.000448,1.0,0.000448,1.0,5.7e-05,1.0,0.00059,1.0,4.2680000000000005e-05,0.0,4.95e-05,1.0,1.1e-05,1.0,3.3e-05,0.0,4.4000000000000006e-05
winogrande.dev.89,meta/llama-2-70b-chat,0.0,4.5e-05,0.0,1.47e-05,0.0,4.08e-05,1.0,0.000408,0.0,0.000408,0.0,5.2e-05,0.0,0.00054,1.0,3.880000000000001e-05,0.0,4.5e-05,1.0,9.8e-06,1.0,3e-05,1.0,3.92e-05
hellaswag.val.3936,Model D,0.0,0.0,0.0,6.48e-05,0.0,0.0001736,0.0,0.001736,1.0,0.001736,1.0,0.0002179999999999,1.0,0.00217,0.0,0.000167616,0.0,0.0001935,0.0,4.3200000000000007e-05,0.0,0.0001296,0.0,0.000172
hellaswag.val.1510,B,0.0,0.0,0.0,3.63e-05,0.0,9.76e-05,1.0,0.000976,0.0,0.000976,1.0,0.000123,0.0,0.00122,0.0,9.3896e-05,0.0,0.000108,0.0,2.42e-05,0.0,7.259999999999999e-05,1.0,9.6e-05
abstract2title.test.170,meta/llama-2-70b-chat,1.0,0.0004005,1.0,8.850000000000001e-05,1.0,0.0003104,1.0,0.002936,1.0,0.003128,1.0,0.000315,1.0,0.00364,1.0,0.00021728,1.0,0.0004005,1.0,5.7400000000000006e-05,1.0,0.0001806,1.0,0.0002376
mmlu-high-school-us-history.val.173,llama-2-70b-chat,0.0,0.0,1.0,0.0001008,1.0,0.0002696,1.0,0.002696,1.0,0.002696,1.0,0.000336,1.0,0.00337,0.0,0.000260736,0.0,0.0003024,1.0,6.72e-05,1.0,0.0002016,1.0,0.000268
arc-challenge.test.635,Model C,0.0,0.0,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,9.2e-05,1.0,0.00094,0.0,6.984e-05,1.0,8.1e-05,0.0,1.8e-05,0.0,5.4e-05,1.0,7.120000000000001e-05
hellaswag.val.6456,Model C,0.0,0.0,0.0,7.8e-05,1.0,0.0002088,0.0,0.002088,1.0,0.002088,0.0,0.0002619999999999,1.0,0.00264,0.0,0.00020176,0.0,0.0002331,0.0,5.2e-05,0.0,0.000156,0.0,0.0002072
grade-school-math.dev.432,meta/llama-2-70b-chat,0.75,0.0003996,0.25,0.0001385999999999,0.25,0.0006464,0.25,0.0043999999999999,0.75,0.008648,0.75,0.000601,0.75,0.00823,0.75,0.000392656,0.75,0.0003996,0.25,0.0001448,0.75,0.0002477999999999,0.5,0.000416
hellaswag.val.1496,B) claude-v2,0.0,0.0,1.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,1.0,8.72e-05
Chinese_character_riddles.dev.38,"Model C — correctness: 1, cost: 0.3

While the specific models are not detailed in terms of their capabilities, based on the given correctness and cost, Model C is the optimal choice. It has a",0.0,0.0,0.0,0.0001227,0.0,0.0001184,0.0,0.002288,0.0,0.006104,0.0,0.000207,0.0,0.01279,0.0,9.7776e-05,0.0,0.0002358,0.0,5.58e-05,0.0,0.0001764,0.0,0.0004328
grade-school-math.dev.66,meta/llama-2-70b-chat,0.75,0.0004509,0.25,0.000192,0.75,0.0005992,0.75,0.0047919999999999,1.0,0.00628,0.75,0.000575,0.5,0.00923,0.25,0.00033368,0.75,0.0004509,0.25,9.94e-05,0.25,0.0002111999999999,0.25,0.000588
mmlu-professional-law.val.581,meta/llama-2-70b-chat,0.0,0.0002898,0.0,9.66e-05,0.0,0.0002584,0.0,0.002584,1.0,0.002584,0.0,0.000324,0.0,0.00323,0.0,0.000249872,0.0,0.0002898,0.0,6.44e-05,0.0,0.0001932,1.0,0.0002576
mmlu-college-biology.val.103,Model C,0.0,0.0,1.0,2.94e-05,0.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,0.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,0.0,8.82e-05,1.0,1.96e-05,0.0,5.88e-05,1.0,7.840000000000001e-05
grade-school-math.dev.6473,meta/llama-2-70b-chat,0.25,0.0004122,0.75,0.0001545,0.75,0.0007592,0.75,0.005528,0.5,0.006128,0.75,0.000546,0.75,0.00937,0.25,0.000346872,0.25,0.0004122,0.25,8.16e-05,0.75,0.0003323999999999,0.25,0.000236
mmlu-high-school-physics.val.85,Model B,0.0,0.0,1.0,3.09e-05,0.0,8.320000000000002e-05,0.0,0.000832,0.0,0.000832,0.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,0.0,8.240000000000001e-05
hellaswag.val.4335,C),0.0,0.0,0.0,8.219999999999999e-05,0.0,0.0002224,1.0,0.0022,1.0,0.0022,0.0,0.000276,1.0,0.00278,0.0,0.000212624,0.0,0.0002466,0.0,5.480000000000001e-05,0.0,0.0001643999999999,1.0,0.0002184
mmlu-international-law.val.87,claude-v2,1.0,0.000864,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,1.0,2.14e-05,1.0,6.42e-05,1.0,8.56e-05
mmlu-international-law.val.97,Model C,0.0,0.0,1.0,3.6e-05,1.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.0001199999999999,1.0,0.00121,0.0,9.312e-05,0.0,0.000108,0.0,2.4e-05,1.0,7.2e-05,1.0,9.52e-05
grade-school-math.dev.1638,meta/llama-2-70b-chat,0.5,0.0003699,0.75,0.0001392,0.75,0.0006296,0.75,0.005048,0.75,0.0057199999999999,0.25,0.000577,0.75,0.01069,0.25,0.000320488,0.5,0.0003699,0.25,0.0001372,0.75,0.0002808,0.75,0.0004232
grade-school-math.dev.6457,meta/llama-2-70b-chat,0.25,0.0004464,0.25,0.0002055,0.25,0.0008447999999999,0.25,0.004992,0.75,0.008928,0.25,0.0007589999999999,0.75,0.00993,0.25,0.000330576,0.25,0.0004464,0.25,0.0001,0.25,0.0003461999999999,0.25,0.0005512
mmlu-miscellaneous.val.252,Model D,0.0,0.0,1.0,4.59e-05,1.0,0.0001232,1.0,0.001232,1.0,0.001232,1.0,0.000153,1.0,0.00154,0.0,0.000118728,0.0,0.0001376999999999,1.0,3.0600000000000005e-05,1.0,9.18e-05,1.0,0.0001216
arc-challenge.test.531,C) Heredity of Earlobe Types,0.0,0.0,0.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.8e-05,1.0,0.001,1.0,7.4496e-05,1.0,8.55e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
hellaswag.val.7504,B,0.0,0.0,0.0,8.549999999999999e-05,1.0,0.0002296,1.0,0.002296,1.0,0.002296,1.0,0.000288,1.0,0.0029,0.0,0.000221936,1.0,0.0002565,0.0,5.7e-05,0.0,0.0001716,1.0,0.000228
hellaswag.val.8000,B) claude-v2,0.0,0.0,0.0,8.04e-05,1.0,0.0002152,0.0,0.002152,1.0,0.002152,0.0,0.000268,1.0,0.00269,0.0,0.0002079679999999,1.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
winogrande.dev.164,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2

Correct choice: Model A",0.0,0.0,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,0.0,0.000392,0.0,5e-05,0.0,0.00052,1.0,3.7248e-05,0.0,4.32e-05,1.0,9.6e-06,1.0,2.8799999999999995e-05,1.0,3.7600000000000006e-05
grade-school-math.dev.5900,meta/llama-2-70b-chat,0.75,0.0003294,0.75,0.0001409999999999,0.25,0.0004824,0.75,0.004104,0.75,0.005088,0.75,0.000477,0.75,0.00681,0.75,0.000300312,0.75,0.0003294,0.75,9.48e-05,0.75,0.0002297999999999,0.75,0.0003608
hellaswag.val.5495,B,0.0,0.0,0.0,7.35e-05,1.0,0.0001968,1.0,0.001968,1.0,0.001968,1.0,0.000247,1.0,0.00246,0.0,0.00019012,0.0,0.0002196,0.0,4.9000000000000005e-05,1.0,0.000147,1.0,0.0001952
hellaswag.val.6782,D),0.0,0.0,0.0,8.280000000000001e-05,1.0,0.0002216,0.0,0.002216,0.0,0.002216,1.0,0.000278,0.0,0.0028,0.0,0.0002141759999999,1.0,0.0002483999999999,0.0,5.520000000000001e-05,0.0,0.0001656,0.0,0.00022
mmlu-professional-law.val.31,B,0.0,0.0,1.0,8.58e-05,1.0,0.0002296,1.0,0.002296,0.0,0.002296,1.0,0.000286,1.0,0.00287,0.0,0.000221936,0.0,0.0002573999999999,0.0,5.720000000000001e-05,1.0,0.0001716,1.0,0.000228
mmlu-college-medicine.val.100,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.1
Model C — correctness: 0, cost: 0.2
Model D",0.0,0.0,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,1.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
hellaswag.val.7404,D,0.0,0.0,0.0,8.730000000000001e-05,1.0,0.0002336,1.0,0.002336,1.0,0.002336,1.0,0.0002929999999999,1.0,0.00292,0.0,0.000225816,1.0,0.000261,0.0,5.8200000000000005e-05,1.0,0.0001746,1.0,0.000232
mmlu-logical-fallacies.val.59,Model D,0.0,0.0,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-professional-law.val.926,meta/llama-2-70b-chat,0.0,0.0002628,1.0,8.76e-05,1.0,0.0002344,1.0,0.002344,1.0,0.002344,1.0,0.000294,1.0,0.00296,0.0,0.000226592,0.0,0.0002628,1.0,5.84e-05,1.0,0.0001752,1.0,0.0002328
mmlu-professional-psychology.val.30,Meta/llama-2-70b-chat,0.0,0.0,1.0,3.18e-05,1.0,8.560000000000001e-05,1.0,0.000856,0.0,0.000856,1.0,0.0001059999999999,1.0,0.0010999999999999,0.0,8.2256e-05,0.0,9.54e-05,0.0,2.12e-05,1.0,6.36e-05,1.0,8.48e-05
hellaswag.val.9840,B,0.0,0.0,0.0,9.18e-05,0.0,0.0002463999999999,0.0,0.002464,0.0,0.002464,1.0,0.000309,1.0,0.00311,0.0,0.000238232,0.0,0.0002763,0.0,6.14e-05,1.0,0.0001842,1.0,0.0002448
mmlu-professional-law.val.364,claude-v2,0.0,0.002744,1.0,0.0001026,1.0,0.0002744,1.0,0.002744,0.0,0.002744,1.0,0.000342,1.0,0.00343,0.0,0.000265392,0.0,0.0003078,1.0,6.84e-05,1.0,0.0002045999999999,1.0,0.0002728
grade-school-math.dev.7103,meta/llama-2-70b-chat,0.25,0.0004473,0.25,0.0001532999999999,0.25,0.0004936,0.75,0.005632,0.75,0.006016,0.25,0.000491,0.75,0.00659,0.25,0.000342216,0.25,0.0004473,0.25,9.32e-05,0.75,0.0002663999999999,0.25,0.0004496
mmlu-high-school-macroeconomics.val.39,Model C - gpt-4-1106-preview,0.0,0.0,0.0,3.12e-05,1.0,8.400000000000001e-05,0.0,0.00084,1.0,0.00084,1.0,0.000106,0.0,0.00108,0.0,8.0704e-05,0.0,9.36e-05,0.0,2.08e-05,1.0,6.24e-05,0.0,8.240000000000001e-05
mmlu-logical-fallacies.val.27,Model C,0.0,0.0,1.0,2.97e-05,0.0,8e-05,0.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,0.0,1.98e-05,1.0,5.94e-05,1.0,7.92e-05
grade-school-math.dev.2814,meta/llama-2-70b-chat,0.75,0.0003897,0.5,0.0001737,0.75,0.000632,0.75,0.005288,0.75,0.005888,0.75,0.000551,0.5,0.0094,0.25,0.000389552,0.75,0.0003897,0.75,8.22e-05,0.25,0.0003792,0.25,0.00026
hellaswag.val.6587,D,0.0,0.0,0.0,8.01e-05,1.0,0.0002144,0.0,0.002144,1.0,0.002144,0.0,0.000267,1.0,0.00268,0.0,0.000207192,0.0,0.0002394,0.0,5.34e-05,0.0,0.0001602,0.0,0.0002128
grade-school-math.dev.6135,meta/llama-2-70b-chat,0.25,0.0003645,0.25,0.0001275,0.75,0.0005344,0.75,0.005248,0.75,0.005008,0.75,0.000468,0.75,0.00767,0.25,0.000343768,0.25,0.0003645,0.25,7.780000000000001e-05,0.25,0.0002231999999999,0.5,0.0003696
hellaswag.val.1588,claude-v2,1.0,0.0013759999999999,1.0,5.1e-05,0.0,0.0001376,1.0,0.0013759999999999,1.0,0.0013759999999999,1.0,0.0001709999999999,1.0,0.00172,1.0,0.000132696,0.0,0.0001529999999999,1.0,3.4200000000000005e-05,1.0,0.0001026,0.0,0.000136
grade-school-math.dev.7238,meta/llama-2-70b-chat,0.25,0.0005859,0.25,0.0002021999999999,0.25,0.0008032,0.25,0.006424,0.25,0.0068559999999999,0.5,0.000743,0.25,0.0113,0.25,0.0003918799999999,0.25,0.0005859,0.25,0.0001256,0.25,0.0003101999999999,0.25,0.0005944
hellaswag.val.9242,Model C,0.0,0.0,1.0,7.199999999999999e-05,0.0,0.0001928,0.0,0.001928,0.0,0.001928,0.0,0.000242,1.0,0.00244,1.0,0.00018624,0.0,0.0002151,1.0,4.8e-05,0.0,0.0001439999999999,1.0,0.0001911999999999
hellaswag.val.6985,A),0.0,0.0,1.0,9.3e-05,1.0,0.0002488,1.0,0.002488,1.0,0.002488,1.0,0.000312,1.0,0.00314,0.0,0.00024056,0.0,0.0002781,1.0,6.2e-05,1.0,0.000186,1.0,0.0002472
hellaswag.val.4142,C) claude-v2,0.0,0.0,0.0,7.230000000000001e-05,0.0,0.0001936,0.0,0.001936,1.0,0.001936,1.0,0.000243,1.0,0.00245,0.0,0.000187016,0.0,0.000216,0.0,4.8200000000000006e-05,0.0,0.0001446,1.0,0.000192
mmlu-professional-accounting.val.39,Model C,0.0,0.0,0.0,4.29e-05,0.0,0.0001152,0.0,0.001152,0.0,0.001152,0.0,0.000145,1.0,0.00144,0.0,0.000110968,0.0,0.0001286999999999,0.0,2.8600000000000004e-05,0.0,8.58e-05,0.0,0.0001136
grade-school-math.dev.5158,meta/llama-2-70b-chat,0.25,0.0003681,0.25,0.0001656,0.25,0.0006632,0.25,0.004808,0.25,0.005336,0.25,0.000483,0.75,0.0088899999999999,0.25,0.000326696,0.25,0.0003681,0.25,9.04e-05,0.25,0.0002351999999999,0.25,0.000344
hellaswag.val.12,mistralai/mixtral-8x7b-chat,1.0,7.02e-05,1.0,3.51e-05,1.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000119,1.0,0.00118,0.0,9.0792e-05,0.0,0.0001043999999999,0.0,2.34e-05,1.0,7.02e-05,0.0,9.28e-05
chinese_ancient_masterpieces_dynasty.dev.12,meta/llama-2-70b-chat,0.0,8.37e-05,0.0,1.44e-05,0.0,0.000132,0.0,0.000408,0.0,0.001344,0.0,5.1000000000000006e-05,0.0,0.00057,0.0,4.656e-05,0.0,8.37e-05,0.0,1.02e-05,0.0,6.78e-05,0.0,0.0001656
hellaswag.val.7529,C,0.0,0.0,1.0,6.689999999999999e-05,0.0,0.00018,1.0,0.0018,0.0,0.0018,0.0,0.000224,1.0,0.00228,1.0,0.0001738239999999,1.0,0.0002016,1.0,4.480000000000001e-05,1.0,0.0001344,1.0,0.0001784
mmlu-global-facts.val.99,Model C,0.0,0.0,0.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,0.0,7.199999999999999e-05,1.0,0.00076,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,0.0,4.32e-05,1.0,5.68e-05
mmlu-professional-accounting.val.22,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 0, cost: 0.1
Model D",0.0,0.0,0.0,3.57e-05,1.0,9.600000000000002e-05,1.0,0.00096,1.0,0.00096,1.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001071,1.0,2.3800000000000003e-05,1.0,7.14e-05,1.0,9.440000000000002e-05
mmlu-high-school-geography.val.39,Model C,0.0,0.0,0.0,2.22e-05,0.0,6e-05,1.0,0.0006,0.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,0.0,4.44e-05,1.0,5.92e-05
mmlu-professional-law.val.683,meta/llama-2-70b-chat,0.0,0.0002682,1.0,8.94e-05,1.0,0.0002392,1.0,0.002392,1.0,0.002392,0.0,0.000298,1.0,0.00299,0.0,0.0002312479999999,0.0,0.0002682,0.0,5.9600000000000005e-05,1.0,0.0001788,1.0,0.0002376
hellaswag.val.6464,Meta/llama-2-70b-chat,0.0,0.0,1.0,7.26e-05,1.0,0.0001944,1.0,0.001944,1.0,0.001944,0.0,0.000242,1.0,0.00243,1.0,0.000187792,1.0,0.0002169,1.0,4.84e-05,0.0,0.0001452,1.0,0.0001928
winogrande.dev.341,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 1, cost: 0.6
Correct choice: Model B",0.0,0.0,1.0,1.53e-05,1.0,4.16e-05,1.0,0.000416,1.0,0.000416,1.0,5.3e-05,1.0,0.00055,1.0,3.9576e-05,0.0,4.59e-05,1.0,1.02e-05,1.0,3.06e-05,1.0,4.08e-05
hellaswag.val.8764,D) claude-v2,0.0,0.0,0.0,8.13e-05,1.0,0.0002176,0.0,0.002176,0.0,0.002176,0.0,0.000271,1.0,0.00272,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-professional-law.val.17,claude-v2,0.0,0.002608,1.0,9.75e-05,0.0,0.0002608,0.0,0.002608,0.0,0.002608,0.0,0.000325,0.0,0.00329,0.0,0.0002522,0.0,0.0002924999999999,0.0,6.500000000000001e-05,0.0,0.000195,0.0,0.00026
winogrande.dev.828,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 1, cost: 0.6
Model C — correctness: 0, cost: 0.1
Model D",0.0,0.0,1.0,1.5e-05,1.0,4.08e-05,1.0,0.000408,1.0,0.000408,1.0,5.2e-05,1.0,0.00054,0.0,3.880000000000001e-05,1.0,4.41e-05,0.0,1e-05,0.0,3e-05,1.0,4e-05
grade-school-math.dev.2150,meta/llama-2-70b-chat,0.25,0.0004293,0.75,0.0001659,0.75,0.0006464,0.25,0.006656,0.25,0.006608,0.25,0.000597,0.25,0.01159,0.75,0.000381016,0.25,0.0004293,0.25,9.78e-05,0.75,0.0002808,0.25,0.0002416
grade-school-math.dev.4830,meta/llama-2-70b-chat,0.75,0.0003932999999999,0.5,0.0001704,0.75,0.00052,0.5,0.005224,0.25,0.006712,0.75,0.000432,0.75,0.0070399999999999,0.5,0.000287896,0.75,0.0003932999999999,0.75,9.06e-05,0.75,0.0002544,0.75,0.000332
grade-school-math.dev.4922,meta/llama-2-70b-chat,0.25,0.0004914,0.5,0.0001785,0.25,0.0006328,0.75,0.005752,0.75,0.006712,0.75,0.000603,0.75,0.00869,0.25,0.00045008,0.25,0.0004914,0.75,6.74e-05,0.25,0.000243,0.0,0.0006008
hellaswag.val.8784,B,0.0,0.0,0.0,6.93e-05,1.0,0.000188,0.0,0.001856,0.0,0.001856,1.0,0.000233,0.0,0.00235,0.0,0.000179256,1.0,0.000207,0.0,4.6200000000000005e-05,0.0,0.0001386,1.0,0.000184
mmlu-high-school-psychology.val.122,Model A,0.0,0.0,1.0,4.29e-05,1.0,0.0001152,1.0,0.001152,1.0,0.001152,1.0,0.000143,1.0,0.00147,0.0,0.000110968,0.0,0.0001286999999999,1.0,2.8600000000000004e-05,1.0,8.58e-05,1.0,0.0001136
chinese_zodiac.dev.391,meta/llama-2-70b-chat,0.0,0.0001341,0.0,3.3600000000000004e-05,0.0,9.28e-05,0.0,0.000928,1.0,0.000928,1.0,0.000114,1.0,0.00116,0.0,0.000114072,0.0,0.0001341,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,9.6e-05
hellaswag.val.6989,claude-v2,1.0,0.001808,0.0,6.749999999999999e-05,1.0,0.0001808,1.0,0.001808,1.0,0.001808,0.0,0.000225,1.0,0.00229,0.0,0.0001746,1.0,0.0002016,0.0,4.5e-05,1.0,0.0001349999999999,1.0,0.0001792
arc-challenge.test.725,Model C,0.0,0.0,1.0,1.83e-05,1.0,4.96e-05,0.0,0.000496,0.0,0.000496,0.0,6.1e-05,1.0,0.00065,1.0,4.7336e-05,0.0,5.4900000000000006e-05,1.0,1.22e-05,1.0,3.66e-05,1.0,4.8e-05
hellaswag.val.9709,D) meta/llama-2-70b-chat,0.0,0.0,0.0,8.159999999999999e-05,0.0,0.0002216,0.0,0.002192,0.0,0.002192,0.0,0.0002749999999999,1.0,0.00274,0.0,0.0002118479999999,0.0,0.0002457,0.0,5.4600000000000006e-05,0.0,0.0001638,0.0,0.0002176
grade-school-math.dev.4573,meta/llama-2-70b-chat,0.25,0.0004095,0.25,0.0002322,0.25,0.0008032,0.25,0.005488,0.75,0.008368,0.5,0.000566,0.75,0.0108499999999999,0.25,0.000357736,0.25,0.0004095,0.75,9.8e-05,0.25,0.0002441999999999,0.25,0.0004992
grade-school-math.dev.6941,meta/llama-2-70b-chat,0.5,0.000342,0.75,0.0001227,0.75,0.0004904,0.75,0.004472,0.5,0.004784,0.75,0.0005239999999999,0.75,0.00646,0.75,0.000336784,0.5,0.000342,0.25,9.02e-05,0.5,0.0002111999999999,0.5,0.0003352
mmlu-professional-law.val.1076,B,0.0,0.0,1.0,8.669999999999999e-05,1.0,0.000232,1.0,0.00232,1.0,0.00232,1.0,0.000289,1.0,0.0029,0.0,0.000224264,0.0,0.0002601,0.0,5.780000000000001e-05,1.0,0.0001733999999999,1.0,0.0002304
mbpp.dev.259,Code-llama-instruct-34b-chat,0.0,0.0,0.0,7.05e-05,0.0,0.0004807999999999,0.0,0.003008,0.0,0.005624,0.0,0.000188,0.0,0.00988,0.0,0.000156752,0.0,0.0003176999999999,0.0,4.4e-05,0.0,0.0002142,0.0,0.0002872
winogrande.dev.318,B,0.0,0.0,1.0,1.7100000000000002e-05,1.0,4.72e-05,1.0,0.000472,0.0,0.000472,1.0,6e-05,1.0,0.0005899999999999,1.0,4.500800000000001e-05,0.0,5.22e-05,1.0,1.16e-05,1.0,3.4800000000000006e-05,1.0,4.64e-05
hellaswag.val.5124,Model B,0.0,0.0,0.0,7.47e-05,1.0,0.0002,0.0,0.002,1.0,0.002,1.0,0.000249,1.0,0.00253,0.0,0.0001932239999999,1.0,0.0002232,0.0,4.980000000000001e-05,0.0,0.0001494,1.0,0.0001984
grade-school-math.dev.7054,meta/llama-2-70b-chat,0.25,0.0004518,0.75,0.0001716,0.75,0.0007888,0.75,0.004648,0.75,0.00592,0.75,0.0005549999999999,0.75,0.0089,0.25,0.000301864,0.25,0.0004518,0.25,9.3e-05,0.25,0.0002412,0.75,0.0004048
grade-school-math.dev.3446,meta/llama-2-70b-chat,0.75,0.0003987,0.5,0.0001989,0.75,0.0006232,0.75,0.005776,0.5,0.005872,0.5,0.000565,0.5,0.00941,0.25,0.000284016,0.75,0.0003987,0.5,0.000103,0.5,0.0003185999999999,0.25,0.0005264
mmlu-high-school-macroeconomics.val.46,Model C,0.0,0.0,0.0,2.46e-05,0.0,6.64e-05,1.0,0.000664,1.0,0.000664,0.0,8.2e-05,1.0,0.00083,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,0.0,4.92e-05,1.0,6.48e-05
mmlu-high-school-government-and-politics.val.82,Model B,0.0,0.0,1.0,4.02e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,1.0,0.000135,1.0,0.00136,0.0,0.0001047599999999,0.0,0.0001215,1.0,2.7e-05,1.0,8.1e-05,1.0,0.000108
grade-school-math.dev.5590,meta/llama-2-70b-chat,0.5,0.0003591,0.25,0.0001341,0.75,0.0005456,0.75,0.005072,0.75,0.005888,0.5,0.0004389999999999,0.75,0.00853,0.25,0.000390328,0.5,0.0003591,0.25,8.78e-05,0.75,0.0002676,0.5,0.000412
mmlu-professional-law.val.202,claude-v2,1.0,0.003504,0.0,0.0001311,0.0,0.0003504,0.0,0.003504,1.0,0.003504,0.0,0.000439,0.0,0.00441,0.0,0.000339112,0.0,0.0003932999999999,0.0,8.74e-05,0.0,0.0002622,0.0,0.0003496
mmlu-professional-law.val.361,claude-v2,0.0,0.002288,0.0,8.549999999999999e-05,0.0,0.0002288,0.0,0.002288,0.0,0.002288,0.0,0.000285,0.0,0.00289,0.0,0.00022116,0.0,0.0002565,1.0,5.7e-05,0.0,0.0001709999999999,0.0,0.0002272
grade-school-math.dev.2482,meta/llama-2-70b-chat,0.75,0.0003987,0.75,0.0001674,0.75,0.0005688,0.75,0.0050399999999999,0.75,0.006936,0.75,0.000567,0.75,0.009,0.75,0.000308072,0.75,0.0003987,0.75,8.76e-05,0.75,0.0002802,0.75,0.0003744
arc-challenge.test.837,Model C,0.0,0.0,0.0,3.4200000000000005e-05,1.0,9.2e-05,1.0,0.00092,0.0,0.00092,1.0,0.0001139999999999,1.0,0.00115,1.0,8.846400000000001e-05,0.0,0.0001026,1.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
mmlu-professional-psychology.val.70,Model C,0.0,0.0,1.0,3.15e-05,0.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,0.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,0.0,2.1e-05,0.0,6.3e-05,0.0,8.400000000000001e-05
winogrande.dev.643,claude-v2,1.0,0.000424,1.0,1.56e-05,1.0,4.24e-05,1.0,0.000424,1.0,0.000424,0.0,5.4000000000000005e-05,0.0,0.00056,0.0,4.0352e-05,1.0,4.6800000000000006e-05,0.0,1.04e-05,1.0,3.12e-05,1.0,4.16e-05
abstract2title.test.142,meta/llama-2-70b-chat,1.0,0.0003627,1.0,8.34e-05,1.0,0.0003024,1.0,0.002856,1.0,0.003048,1.0,0.000289,1.0,0.0033,1.0,0.000210296,1.0,0.0003627,1.0,5.28e-05,1.0,0.0001662,1.0,0.0002176
grade-school-math.dev.5772,meta/llama-2-70b-chat,0.25,0.0003168,0.25,0.0001430999999999,0.25,0.0005112,0.75,0.0057599999999999,0.75,0.006576,0.75,0.000536,0.75,0.00852,0.25,0.00029876,0.25,0.0003168,0.25,8.840000000000001e-05,0.25,0.0002483999999999,0.25,0.0003544
mmlu-professional-law.val.550,Llama-2-70b-chat,0.0,0.0,1.0,9.15e-05,1.0,0.0002448,1.0,0.002448,1.0,0.002448,1.0,0.000305,1.0,0.00306,0.0,0.00023668,0.0,0.0002745,1.0,6.1000000000000005e-05,1.0,0.0001829999999999,1.0,0.000244
arc-challenge.val.76,Model C,0.0,0.0,0.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.000107,1.0,0.00106,0.0,8.148e-05,1.0,9.45e-05,0.0,2.1e-05,1.0,6.3e-05,1.0,8.320000000000002e-05
mmlu-high-school-geography.val.137,Model C,0.0,0.0,0.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,1.0,5.46e-05,1.0,7.200000000000002e-05
mbpp.dev.134,meta/llama-2-70b-chat,0.0,0.0005067,0.0,0.0001619999999999,0.0,0.0007248,0.0,0.008784,0.0,0.0078719999999999,0.0,0.000564,0.0,0.0135,0.0,0.000288672,0.0,0.0005067,0.0,0.0003504,0.0,0.0002538,0.0,0.0004504
hellaswag.val.4253,B,0.0,0.0,1.0,7.47e-05,1.0,0.0002,1.0,0.002,1.0,0.002,1.0,0.000251,1.0,0.0025,0.0,0.0001932239999999,1.0,0.0002232,0.0,4.980000000000001e-05,0.0,0.0001494,1.0,0.0001984
winogrande.dev.96,Model B,0.0,0.0,0.0,1.5e-05,1.0,4.08e-05,1.0,0.000408,1.0,0.000408,0.0,5.2e-05,1.0,0.00051,0.0,3.880000000000001e-05,1.0,4.5e-05,0.0,1e-05,0.0,3e-05,1.0,3.92e-05
winogrande.dev.734,Model B,0.0,0.0,0.0,1.41e-05,1.0,3.84e-05,0.0,0.000384,0.0,0.000384,0.0,4.7e-05,1.0,0.00051,1.0,3.6472000000000006e-05,0.0,4.2300000000000005e-05,1.0,9.4e-06,1.0,2.82e-05,1.0,3.68e-05
mmlu-jurisprudence.val.102,Model C - claude-v2,0.0,0.0,1.0,3.93e-05,1.0,0.0001056,1.0,0.0010559999999999,1.0,0.0010559999999999,1.0,0.0001309999999999,1.0,0.00132,0.0,0.000101656,0.0,0.0001179,0.0,2.62e-05,1.0,7.86e-05,1.0,0.0001048
mmlu-high-school-macroeconomics.val.290,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.61e-05,0.0,7.04e-05,0.0,0.000704,0.0,0.000704,0.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,0.0,5.22e-05,0.0,6.88e-05
hellaswag.val.3309,D) claude-v2,0.0,0.0,0.0,8.549999999999999e-05,0.0,0.000232,0.0,0.002296,1.0,0.002296,1.0,0.000288,1.0,0.0029,0.0,0.000221936,1.0,0.0002565,0.0,5.720000000000001e-05,1.0,0.0001716,1.0,0.000228
arc-challenge.test.393,Model C - claude-v2,0.0,0.0,1.0,3.18e-05,1.0,8.560000000000001e-05,1.0,0.000856,1.0,0.000856,1.0,0.000108,1.0,0.00107,0.0,8.2256e-05,1.0,9.54e-05,1.0,2.12e-05,1.0,6.36e-05,1.0,8.400000000000001e-05
hellaswag.val.3355,Model C,0.0,0.0,0.0,7.859999999999999e-05,1.0,0.0002136,1.0,0.002112,1.0,0.002112,1.0,0.000263,1.0,0.00264,0.0,0.000204088,1.0,0.0002358,0.0,5.260000000000001e-05,1.0,0.0001578,1.0,0.0002096
hellaswag.val.1509,D) gpt-4-1106-preview,0.0,0.0,0.0,4.05e-05,0.0,0.0001088,0.0,0.001088,0.0,0.001088,0.0,0.000137,0.0,0.00136,0.0,0.0001047599999999,0.0,0.0001206,1.0,2.7e-05,0.0,8.1e-05,0.0,0.0001072
grade-school-math.dev.7208,meta/llama-2-70b-chat,0.5,0.0003699,0.5,0.0001521,0.75,0.0005352,0.75,0.004992,0.75,0.006192,0.75,0.000455,0.75,0.0071699999999999,0.5,0.0003026399999999,0.5,0.0003699,0.5,7.6e-05,0.75,0.000249,0.75,0.000424
mbpp.dev.43,Code-llama-instruct-34b-chat,0.0,0.0,0.0,6.269999999999999e-05,1.0,0.000504,0.0,0.0067919999999999,1.0,0.005016,1.0,0.0001619999999999,1.0,0.00894,1.0,0.00010864,0.0,0.0003681,1.0,3.5e-05,1.0,0.0002459999999999,0.0,0.0002512
grade-school-math.dev.4543,meta/llama-2-70b-chat,0.75,0.0004023,0.75,0.0001512,0.5,0.000668,0.75,0.004352,0.75,0.0054319999999999,0.5,0.000536,0.5,0.00799,0.75,0.000312728,0.75,0.0004023,0.5,9.84e-05,0.5,0.0002729999999999,0.5,0.0003456
grade-school-math.dev.1847,meta/llama-2-70b-chat,0.75,0.0003627,0.5,0.0001365,0.75,0.0004952,0.75,0.00404,0.75,0.005024,0.5,0.000461,0.5,0.00667,0.75,0.00028712,0.75,0.0003627,0.75,7.02e-05,0.75,0.0002112,0.75,0.00024
mmlu-jurisprudence.val.43,Model D,0.0,0.0,1.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,1.0,2.56e-05,1.0,7.68e-05,1.0,0.0001016
mmlu-high-school-psychology.val.292,Model D,0.0,0.0,0.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
mmlu-nutrition.val.286,Meta/llama-2-70b-chat,0.0,0.0,1.0,4.41e-05,1.0,0.0001184,1.0,0.001184,1.0,0.001184,1.0,0.000147,1.0,0.00148,0.0,0.000114072,0.0,0.0001323,0.0,2.94e-05,1.0,8.82e-05,1.0,0.0001176
hellaswag.val.8769,Model C,0.0,0.0,0.0,7.589999999999999e-05,1.0,0.000204,1.0,0.00204,0.0,0.00204,1.0,0.000256,1.0,0.00258,0.0,0.0001971039999999,0.0,0.0002277,0.0,5.080000000000001e-05,0.0,0.0001524,0.0,0.0002024
hellaswag.val.3473,A) WizardLM/WizardLM-13B-V1.2,0.0,0.0,1.0,6.569999999999998e-05,1.0,0.0001768,0.0,0.001768,1.0,0.001768,0.0,0.00022,1.0,0.00221,0.0,0.00017072,1.0,0.0001971,1.0,4.4000000000000006e-05,1.0,0.0001319999999999,1.0,0.0001752
hellaswag.val.2991,claude-v2,0.0,0.000688,1.0,2.52e-05,0.0,6.88e-05,0.0,0.000688,0.0,0.000688,0.0,8.499999999999999e-05,0.0,0.00086,0.0,6.596e-05,0.0,7.56e-05,1.0,1.7e-05,0.0,5.1e-05,0.0,6.720000000000001e-05
arc-challenge.test.115,Model C,0.0,0.0,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00078,0.0,5.7424e-05,1.0,6.66e-05,0.0,1.48e-05,1.0,4.44e-05,1.0,5.84e-05
grade-school-math.dev.4643,meta/llama-2-70b-chat,0.75,0.0003114,0.75,9.99e-05,0.75,0.0003935999999999,0.5,0.002232,0.75,0.005016,0.5,0.0003199999999999,0.75,0.00549,0.75,0.000258408,0.75,0.0003114,0.75,6.08e-05,0.75,0.0002316,0.75,0.000276
hellaswag.val.8081,A) WizardLM/WizardLM-13B-V1.2,0.0,0.0,1.0,7.29e-05,0.0,0.0001952,1.0,0.001952,1.0,0.001952,1.0,0.000245,1.0,0.00247,1.0,0.000188568,0.0,0.0002187,1.0,4.860000000000001e-05,1.0,0.0001458,1.0,0.0001936
mmlu-medical-genetics.val.99,Model C - Thanatophoric dysplasia,0.0,0.0,0.0,2.61e-05,0.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.96e-05
mmlu-us-foreign-policy.val.6,meta/llama-2-70b-chat,0.0,0.0001062,0.0,3.54e-05,1.0,9.52e-05,1.0,0.000952,1.0,0.000952,1.0,0.000118,1.0,0.00119,0.0,9.1568e-05,0.0,0.0001062,0.0,2.36e-05,1.0,7.08e-05,1.0,9.36e-05
arc-challenge.test.1034,claude-v2,1.0,0.000712,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,9e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,1.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
hellaswag.val.5771,B,0.0,0.0,0.0,7.769999999999999e-05,1.0,0.0002104,1.0,0.00208,1.0,0.00208,1.0,0.000261,1.0,0.00263,0.0,0.000200984,1.0,0.0002331,0.0,5.1800000000000005e-05,0.0,0.0001553999999999,1.0,0.0002064
grade-school-math.dev.2567,meta/llama-2-70b-chat,0.5,0.0003815999999999,0.5,0.0001671,0.75,0.0004768,0.75,0.00496,0.75,0.006304,0.5,0.000475,0.75,0.00998,0.75,0.0003686,0.5,0.0003815999999999,0.25,7.000000000000001e-05,0.25,0.000276,0.75,0.0004
mmlu-clinical-knowledge.val.243,claude-v2,0.0,0.00096,0.0,3.57e-05,1.0,9.600000000000002e-05,0.0,0.00096,0.0,0.00096,0.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001071,0.0,2.3800000000000003e-05,0.0,7.14e-05,1.0,9.52e-05
mmlu-moral-scenarios.val.113,"C) Not wrong, Wrong",0.0,0.0,0.0,4.44e-05,0.0,0.0001192,0.0,0.001192,0.0,0.001192,1.0,0.000148,0.0,0.0015199999999999,0.0,0.000114848,0.0,0.0001331999999999,0.0,2.96e-05,0.0,8.879999999999999e-05,0.0,0.0001176
mmlu-sociology.val.50,claude-v2,1.0,0.000816,1.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,1.0,2.02e-05,1.0,6.06e-05,1.0,8.08e-05
mmlu-medical-genetics.val.77,B) claude-v2,0.0,0.0,0.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.96e-05
mmlu-professional-law.val.907,claude-v2,0.0,0.00276,1.0,0.0001032,0.0,0.000276,0.0,0.00276,0.0,0.00276,0.0,0.000344,1.0,0.00345,0.0,0.0002669439999999,0.0,0.0003096,0.0,6.88e-05,0.0,0.0002064,0.0,0.0002744
winogrande.dev.984,B) claude-v2,0.0,0.0,1.0,1.6800000000000002e-05,1.0,4.56e-05,1.0,0.0004559999999999,1.0,0.0004559999999999,1.0,5.8e-05,1.0,0.0006,0.0,4.3456000000000005e-05,1.0,5.0400000000000005e-05,0.0,1.12e-05,0.0,3.3600000000000004e-05,0.0,4.4e-05
grade-school-math.dev.4814,meta/llama-2-70b-chat,0.25,0.0003672,0.25,0.0001827,0.75,0.0006776,0.75,0.007232,0.75,0.007088,0.75,0.000626,0.75,0.00907,0.25,0.000418264,0.25,0.0003672,0.25,8.760000000000002e-05,0.25,0.0002232,0.75,0.0004464
mmlu-philosophy.val.127,Model C,0.0,0.0,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00098,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.680000000000001e-05
mmlu-high-school-macroeconomics.val.327,Model C,0.0,0.0,1.0,4.17e-05,0.0,0.000112,0.0,0.00112,0.0,0.00112,0.0,0.000139,1.0,0.0014,0.0,0.000107864,0.0,0.0001250999999999,0.0,2.78e-05,1.0,8.340000000000001e-05,0.0,0.0001112
hellaswag.val.8403,Model C,0.0,0.0,1.0,9.12e-05,0.0,0.0002464,0.0,0.00244,1.0,0.00244,0.0,0.000306,1.0,0.00308,1.0,0.000235904,1.0,0.0002736,1.0,6.080000000000001e-05,1.0,0.0001817999999999,1.0,0.0002424
mmlu-high-school-microeconomics.val.105,Model B,0.0,0.0,1.0,2.52e-05,0.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.720000000000001e-05
hellaswag.val.1181,claude-v2,0.0,0.000888,0.0,3.3e-05,0.0,8.88e-05,0.0,0.000888,0.0,0.000888,0.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,0.0,8.72e-05
mmlu-high-school-macroeconomics.val.316,B,0.0,0.0,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.240000000000001e-05
grade-school-math.dev.4373,meta/llama-2-70b-chat,0.25,0.0004122,0.0,0.0001737,0.5,0.0006904,0.75,0.007048,0.5,0.00712,0.75,0.000686,0.5,0.00926,0.25,0.000338336,0.25,0.0004122,0.25,9.48e-05,0.75,0.0002916,0.75,0.000408
grade-school-math.dev.2836,meta/llama-2-70b-chat,0.75,0.0003249,0.75,0.0001059,0.75,0.0004376,0.75,0.0037279999999999,0.75,0.00428,0.75,0.0004129999999999,0.5,0.00562,0.25,0.0002421119999999,0.75,0.0003249,0.75,6.08e-05,0.75,0.0002052,0.5,0.0003352
hellaswag.val.5542,Model C,0.0,0.0,0.0,7.35e-05,0.0,0.0001968,0.0,0.001968,1.0,0.001968,0.0,0.000247,1.0,0.00246,0.0,0.00019012,0.0,0.0002196,0.0,4.9000000000000005e-05,0.0,0.000147,1.0,0.0001952
winogrande.dev.176,Model B,0.0,0.0,1.0,1.41e-05,0.0,4.08e-05,0.0,0.000384,0.0,0.000384,0.0,4.7e-05,1.0,0.00048,1.0,3.6472000000000006e-05,0.0,4.2300000000000005e-05,1.0,9.4e-06,1.0,2.82e-05,1.0,3.68e-05
arc-challenge.val.20,Model C,0.0,0.0,1.0,1.7100000000000002e-05,1.0,4.64e-05,1.0,0.000464,1.0,0.000464,1.0,5.7e-05,1.0,0.00061,0.0,4.4232e-05,0.0,5.13e-05,1.0,1.14e-05,1.0,3.4200000000000005e-05,0.0,4.48e-05
arc-challenge.test.693,Model D,0.0,0.0,1.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,1.0,0.0001035,1.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
arc-challenge.test.1139,A) meta/llama-2-70b-chat,0.0,0.0,1.0,3.39e-05,1.0,9.120000000000002e-05,1.0,0.000912,1.0,0.000912,1.0,0.000115,1.0,0.00117,1.0,8.768799999999999e-05,1.0,0.0001017,1.0,2.2600000000000004e-05,1.0,6.78e-05,1.0,8.960000000000002e-05
mmlu-professional-psychology.val.607,Model C,0.0,0.0,0.0,2.55e-05,0.0,6.88e-05,0.0,0.000688,0.0,0.000688,0.0,8.499999999999999e-05,1.0,0.00089,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,0.0,6.720000000000001e-05
hellaswag.val.1134,claude-v2,1.0,0.000752,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.5e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.280000000000001e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.36e-05
hellaswag.val.3510,B,0.0,0.0,0.0,9.03e-05,1.0,0.0002424,1.0,0.002424,1.0,0.002424,1.0,0.0003039999999999,1.0,0.00306,0.0,0.000234352,1.0,0.0002718,0.0,6.04e-05,1.0,0.0001812,1.0,0.0002408
hellaswag.val.3289,B) claude-v2,0.0,0.0,0.0,6.69e-05,1.0,0.0001792,1.0,0.001792,1.0,0.001792,1.0,0.000225,1.0,0.00227,0.0,0.000173048,1.0,0.0001998,0.0,4.460000000000001e-05,0.0,0.0001338,0.0,0.0001776
mmlu-high-school-us-history.val.180,meta/llama-2-70b-chat,0.0,0.0001872,1.0,6.24e-05,0.0,0.0001672,0.0,0.0016719999999999,1.0,0.0016719999999999,1.0,0.000208,1.0,0.00209,0.0,0.0001614079999999,0.0,0.0001872,0.0,4.160000000000001e-05,1.0,0.0001248,1.0,0.0001664
hellaswag.val.2612,Model C,0.0,0.0,0.0,3.57e-05,1.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.000122,1.0,0.00124,0.0,9.312e-05,1.0,0.0001071,0.0,2.4e-05,1.0,7.2e-05,1.0,9.52e-05
grade-school-math.dev.7045,meta/llama-2-70b-chat,0.5,0.0004347,0.75,0.0001695,0.75,0.000736,0.75,0.005848,0.5,0.00796,0.5,0.00062,0.75,0.00917,0.5,0.000370928,0.5,0.0004347,0.25,9.38e-05,0.75,0.0003054,0.25,0.0004072
grade-school-math.dev.6840,meta/llama-2-70b-chat,0.25,0.0003779999999999,0.25,0.0001314,0.5,0.0006224,0.75,0.0046879999999999,0.5,0.006128,0.75,0.00049,0.75,0.00775,0.25,0.000304968,0.25,0.0003779999999999,0.25,8.48e-05,0.75,0.0002634,0.75,0.0004032
mmlu-computer-security.val.46,D) backdoor,0.0,0.0,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
mbpp.dev.35,meta/llama-2-70b-chat,0.0,0.0001746,0.0,0.0001821,1.0,0.000324,1.0,0.00324,1.0,0.006096,1.0,0.000316,1.0,0.0066,1.0,0.000110968,0.0,0.0001746,0.0,4.96e-05,0.0,0.0002352,1.0,0.000176
hellaswag.val.4847,Model C,0.0,0.0,0.0,6.09e-05,1.0,0.0001656,1.0,0.001632,0.0,0.001632,0.0,0.000203,1.0,0.00204,0.0,0.000157528,0.0,0.0001827,0.0,4.06e-05,0.0,0.0001217999999999,0.0,0.0001616
mmlu-professional-law.val.900,D) meta/llama-2-70b-chat,0.0,0.0,0.0,0.0001133999999999,0.0,0.0003032,0.0,0.003032,1.0,0.003032,1.0,0.0003779999999999,0.0,0.00379,0.0,0.000293328,0.0,0.0003402,0.0,7.56e-05,0.0,0.0002267999999999,1.0,0.0003016
mmlu-international-law.val.114,claude-v2,0.0,0.0010639999999999,1.0,3.96e-05,0.0,0.0001064,1.0,0.0010639999999999,0.0,0.0010639999999999,1.0,0.0001319999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001188,1.0,2.64e-05,1.0,7.92e-05,1.0,0.0001055999999999
hellaswag.val.5148,D) meta/llama-2-70b-chat,0.0,0.0,0.0,8.249999999999999e-05,0.0,0.0002208,0.0,0.002208,0.0,0.002208,0.0,0.000277,1.0,0.00276,0.0,0.0002134,1.0,0.0002466,0.0,5.5e-05,0.0,0.0001649999999999,0.0,0.0002192
grade-school-math.dev.6255,meta/llama-2-70b-chat,0.5,0.0003465,0.75,0.0001278,0.5,0.000448,0.5,0.002632,0.75,0.0045039999999999,0.5,0.000328,0.5,0.00662,0.25,0.000297984,0.5,0.0003465,0.75,7.66e-05,0.75,0.0002214,0.25,0.000356
mmlu-sociology.val.120,Model C,0.0,0.0,1.0,2.01e-05,1.0,5.44e-05,1.0,0.000544,1.0,0.000544,1.0,6.699999999999999e-05,1.0,0.0006799999999999,0.0,5.1992000000000006e-05,0.0,6.03e-05,1.0,1.34e-05,1.0,4.02e-05,1.0,5.36e-05
mmlu-conceptual-physics.val.137,Model C,0.0,0.0,0.0,2.49e-05,0.0,6.720000000000001e-05,0.0,0.000672,0.0,0.000672,1.0,8.5e-05,0.0,0.0008399999999999,0.0,6.4408e-05,0.0,7.470000000000001e-05,0.0,1.66e-05,0.0,4.98e-05,0.0,6.64e-05
hellaswag.val.1742,claude-v2,0.0,0.001088,0.0,4.05e-05,0.0,0.0001088,0.0,0.001088,0.0,0.001088,1.0,0.000137,1.0,0.00136,0.0,0.0001047599999999,0.0,0.0001206,0.0,2.7e-05,0.0,8.1e-05,0.0,0.0001072
mmlu-professional-law.val.591,meta/llama-2-70b-chat,0.0,0.0002115,0.0,7.049999999999999e-05,0.0,0.0001888,0.0,0.001888,0.0,0.001888,0.0,0.000235,0.0,0.00236,0.0,0.0001823599999999,0.0,0.0002115,1.0,4.7e-05,1.0,0.0001409999999999,0.0,0.0001872
mmlu-marketing.val.8,Model C) meta/llama-2-70b-chat,0.0,0.0,0.0,2.07e-05,0.0,5.6e-05,1.0,0.00056,1.0,0.00056,1.0,6.9e-05,1.0,0.0007,0.0,5.354400000000001e-05,0.0,6.21e-05,0.0,1.38e-05,1.0,4.14e-05,1.0,5.44e-05
mmlu-professional-law.val.1422,C) claude-v2,0.0,0.0,1.0,8.46e-05,1.0,0.0002264,1.0,0.002264,1.0,0.002264,1.0,0.0002819999999999,1.0,0.00283,0.0,0.000218832,0.0,0.0002538,0.0,5.64e-05,1.0,0.0001692,1.0,0.0002248
grade-school-math.dev.4568,meta/llama-2-70b-chat,0.75,0.0003222,0.75,0.0001376999999999,0.5,0.0002208,0.75,0.00312,0.75,0.005112,0.75,0.00041,0.75,0.00789,0.25,0.000260736,0.75,0.0003222,0.75,8.14e-05,0.25,0.000222,0.25,0.0004424
mmlu-college-computer-science.val.80,D) Model D (D),0.0,0.0,0.0,3.72e-05,0.0,0.0001,0.0,0.001,0.0,0.001,1.0,0.000124,0.0,0.00125,0.0,9.6224e-05,0.0,0.0001116,1.0,2.4800000000000003e-05,0.0,7.44e-05,0.0,9.92e-05
mmlu-international-law.val.111,claude-v2,1.0,0.00084,0.0,3.12e-05,0.0,8.400000000000001e-05,0.0,0.00084,1.0,0.00084,1.0,0.000104,1.0,0.00105,0.0,8.0704e-05,0.0,9.36e-05,0.0,2.08e-05,0.0,6.24e-05,1.0,8.32e-05
mmlu-high-school-macroeconomics.val.285,B-model,0.0,0.0,1.0,2.28e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,1.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
grade-school-math.dev.7195,meta/llama-2-70b-chat,0.75,0.0003815999999999,0.75,0.0001296,0.75,0.0006168,0.5,0.006624,0.5,0.006336,0.75,0.000563,0.75,0.00618,0.75,0.000336784,0.75,0.0003815999999999,0.25,7.32e-05,0.25,0.0002658,0.75,0.000392
chinese_zodiac.dev.184,meta/llama-2-70b-chat,0.0,0.0001269,0.0,3.33e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,0.0,0.000113,1.0,0.00115,0.0,0.00011252,0.0,0.0001269,0.0,2.22e-05,0.0,6.659999999999999e-05,0.0,8.960000000000001e-05
grade-school-math.dev.3674,meta/llama-2-70b-chat,0.75,0.0003492,0.5,0.0001422,0.75,0.0005976,0.5,0.006384,0.75,0.005424,0.75,0.000646,0.75,0.00849,0.25,0.0002770319999999,0.75,0.0003492,0.75,8.840000000000001e-05,0.75,0.000264,0.75,0.0004528
mmlu-high-school-world-history.val.138,claude-v2,1.0,0.002712,1.0,0.0001014,1.0,0.0002712,1.0,0.002712,1.0,0.002712,1.0,0.000338,1.0,0.00339,0.0,0.0002622879999999,0.0,0.0003042,0.0,6.76e-05,1.0,0.0002028,1.0,0.0002696
mmlu-high-school-mathematics.val.198,Meta/llama-2-70b-chat,0.0,0.0,1.0,3.27e-05,0.0,8.800000000000001e-05,0.0,0.00088,0.0,0.00088,0.0,0.0001089999999999,0.0,0.0011,0.0,8.4584e-05,0.0,9.81e-05,0.0,2.18e-05,1.0,6.48e-05,0.0,8.72e-05
grade-school-math.dev.5134,meta/llama-2-70b-chat,0.5,0.000423,0.25,0.0001512,0.5,0.0005719999999999,0.5,0.005888,0.25,0.007568,0.75,0.000647,0.25,0.0121299999999999,0.25,0.000420592,0.5,0.000423,0.25,0.0001006,0.25,0.0003083999999999,0.75,0.0004512
mmlu-high-school-psychology.val.444,Model C) frequency,0.0,0.0,0.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,0.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,1.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
grade-school-math.dev.3740,meta/llama-2-70b-chat,0.25,0.0003212999999999,0.75,0.0001395,0.75,0.000608,0.75,0.004448,0.75,0.00476,0.75,0.00041,0.75,0.00775,0.25,0.000346096,0.25,0.0003212999999999,0.25,8.5e-05,0.25,0.0002202,0.75,0.0003352
hellaswag.val.9601,B,0.0,0.0,0.0,8.219999999999998e-05,1.0,0.0002208,1.0,0.002208,1.0,0.002208,1.0,0.000277,1.0,0.00276,0.0,0.0002134,1.0,0.0002475,0.0,5.5e-05,0.0,0.0001649999999999,1.0,0.0002192
hellaswag.val.9041,Model C,0.0,0.0,0.0,8.01e-05,0.0,0.0002144,0.0,0.002144,0.0,0.002144,1.0,0.000269,0.0,0.00268,0.0,0.000207192,1.0,0.0002394,0.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
mmlu-professional-law.val.12,claude-v2,1.0,0.002848,1.0,0.0001064999999999,1.0,0.0002848,0.0,0.002848,1.0,0.002848,0.0,0.000355,1.0,0.00356,0.0,0.00027548,0.0,0.0003194999999999,0.0,7.1e-05,0.0,0.0002129999999999,0.0,0.0002832
hellaswag.val.2905,B,0.0,0.0,0.0,3e-05,0.0,8.080000000000001e-05,0.0,0.000808,0.0,0.000808,0.0,0.000102,1.0,0.00104,1.0,7.76e-05,0.0,8.91e-05,1.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
mmlu-high-school-world-history.val.100,meta/llama-2-70b-chat,0.0,0.0002385,1.0,7.95e-05,1.0,0.0002128,1.0,0.002128,1.0,0.002128,1.0,0.000265,1.0,0.00266,0.0,0.00020564,0.0,0.0002385,0.0,5.300000000000001e-05,1.0,0.000159,1.0,0.0002112
hellaswag.val.6108,D) gpt-4-1106-preview,0.0,0.0,0.0,8.31e-05,0.0,0.0002248,0.0,0.002224,0.0,0.002224,1.0,0.000279,0.0,0.00278,0.0,0.000214952,1.0,0.0002493,0.0,5.5400000000000005e-05,0.0,0.0001662,1.0,0.0002208
hellaswag.val.516,Model C,0.0,0.0,0.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,1.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
grade-school-math.dev.3587,meta/llama-2-70b-chat,0.75,0.000324,0.75,0.0001293,0.75,0.000484,0.75,0.003784,0.5,0.004336,0.75,0.000399,0.75,0.0086,0.25,0.00031428,0.75,0.000324,0.5,7.04e-05,0.75,0.0002394,0.25,0.0003112
mmlu-professional-law.val.401,meta/llama-2-70b-chat,0.0,0.0003024,1.0,0.0001008,1.0,0.0002696,1.0,0.002696,1.0,0.002696,1.0,0.000336,1.0,0.00337,0.0,0.000260736,0.0,0.0003024,0.0,6.72e-05,1.0,0.0002016,1.0,0.000268
grade-school-math.dev.3689,meta/llama-2-70b-chat,0.25,0.0004131,0.25,0.0001653,0.75,0.000688,0.75,0.004768,0.75,0.0056799999999999,0.25,0.00106,0.75,0.0086299999999999,0.25,0.000398088,0.25,0.0004131,0.25,9.3e-05,0.75,0.000318,0.25,0.000536
hellaswag.val.3051,Model C,0.0,0.0,0.0,2.94e-05,0.0,7.920000000000001e-05,0.0,0.000792,0.0,0.000792,1.0,9.8e-05,0.0,0.00099,0.0,7.604800000000001e-05,1.0,8.73e-05,0.0,1.96e-05,0.0,5.88e-05,0.0,7.760000000000002e-05
mmlu-jurisprudence.val.4,claude-v2,0.0,0.000872,0.0,3.24e-05,0.0,8.720000000000002e-05,0.0,0.000872,0.0,0.000872,0.0,0.000108,0.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,0.0,6.48e-05,0.0,8.560000000000002e-05
hellaswag.val.5010,B,0.0,0.0,0.0,7.95e-05,1.0,0.0002128,1.0,0.002128,1.0,0.002128,0.0,0.000267,1.0,0.00269,0.0,0.00020564,1.0,0.0002385,0.0,5.300000000000001e-05,0.0,0.000159,1.0,0.0002112
mmlu-professional-law.val.1060,claude-v2,0.0,0.003104,0.0,0.0001161,0.0,0.0003104,0.0,0.003104,0.0,0.003104,1.0,0.000387,1.0,0.00388,0.0,0.0003003119999999,0.0,0.0003483,1.0,7.74e-05,1.0,0.0002322,1.0,0.0003088
hellaswag.val.7957,claude-v2,1.0,0.002184,1.0,8.13e-05,1.0,0.0002184,0.0,0.002184,1.0,0.002184,0.0,0.000274,1.0,0.00273,0.0,0.000211072,0.0,0.0002448,0.0,5.44e-05,0.0,0.0001632,1.0,0.0002167999999999
hellaswag.val.4835,Model C,0.0,0.0,0.0,8.52e-05,0.0,0.0002304,0.0,0.00228,0.0,0.00228,1.0,0.000286,1.0,0.00288,0.0,0.0002203839999999,0.0,0.0002556,0.0,5.680000000000001e-05,0.0,0.0001704,1.0,0.0002264
mmlu-miscellaneous.val.131,B) 50,0.0,0.0,0.0,2.04e-05,0.0,5.52e-05,1.0,0.000552,0.0,0.000552,1.0,6.8e-05,1.0,0.00069,0.0,5.2768e-05,0.0,6.12e-05,0.0,1.36e-05,1.0,4.08e-05,1.0,5.36e-05
mmlu-public-relations.val.45,Model C,0.0,0.0,0.0,3.3600000000000004e-05,1.0,9.04e-05,1.0,0.000904,1.0,0.000904,1.0,0.000112,1.0,0.00116,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,1.0,8.88e-05
winogrande.dev.1159,Model A (Logan),0.0,0.0,1.0,1.4699999999999998e-05,1.0,4e-05,0.0,0.0003999999999999,0.0,0.0003999999999999,1.0,4.9e-05,0.0,0.00053,1.0,3.8024e-05,0.0,4.3200000000000007e-05,1.0,9.8e-06,1.0,2.94e-05,1.0,3.84e-05
mmlu-miscellaneous.val.304,Model B,0.0,0.0,0.0,2.01e-05,0.0,5.44e-05,0.0,0.000544,1.0,0.000544,0.0,6.9e-05,1.0,0.0006799999999999,0.0,5.1992000000000006e-05,0.0,6.03e-05,0.0,1.34e-05,0.0,4.02e-05,1.0,5.36e-05
hellaswag.val.3092,Model C,0.0,0.0,1.0,3.78e-05,0.0,0.0001016,0.0,0.001016,1.0,0.001016,0.0,0.000126,0.0,0.00127,0.0,9.7776e-05,0.0,0.0001125,1.0,2.52e-05,1.0,7.56e-05,0.0,0.0001
hellaswag.val.6586,B,0.0,0.0,0.0,8.369999999999999e-05,1.0,0.0002272,1.0,0.002248,1.0,0.002248,1.0,0.0002819999999999,1.0,0.00284,0.0,0.00021728,1.0,0.0002511,0.0,5.6000000000000006e-05,0.0,0.000168,1.0,0.0002232
mmlu-high-school-computer-science.val.48,Model C,0.0,0.0,0.0,4.98e-05,1.0,0.0001336,1.0,0.001336,1.0,0.001336,1.0,0.000166,1.0,0.00167,0.0,0.000128816,0.0,0.0001493999999999,0.0,3.320000000000001e-05,1.0,9.96e-05,1.0,0.000132
grade-school-math.dev.2624,meta/llama-2-70b-chat,0.25,0.0003987,0.25,0.0001373999999999,0.75,0.0006536,0.75,0.004376,0.75,0.0052879999999999,0.75,0.0005809999999999,0.5,0.0076,0.25,0.000317384,0.25,0.0003987,0.25,8.280000000000001e-05,0.75,0.0002441999999999,0.75,0.0004056
accounting_audit.dev.6,Meta/llama-2-70b-chat,0.0,0.0,0.0,4.41e-05,1.0,0.0001136,1.0,0.0011359999999999,0.0,0.0011359999999999,1.0,0.0001409999999999,1.0,0.00142,0.0,0.000114072,0.0,0.0001323,0.0,2.9400000000000003e-05,0.0,8.819999999999999e-05,0.0,0.0001128
mmlu-sociology.val.129,Model C,0.0,0.0,1.0,3.06e-05,1.0,8.240000000000001e-05,1.0,0.000824,1.0,0.000824,1.0,0.000102,1.0,0.00103,0.0,7.9152e-05,0.0,9.18e-05,0.0,2.04e-05,1.0,6.12e-05,1.0,8.080000000000001e-05
mmlu-professional-accounting.val.191,Model C,0.0,0.0,1.0,3.4200000000000005e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,0.0,0.0001139999999999,1.0,0.0011799999999999,0.0,8.846400000000001e-05,0.0,0.0001026,0.0,2.28e-05,0.0,6.840000000000001e-05,0.0,9.12e-05
mmlu-miscellaneous.val.335,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2

Correct choice: Model A",0.0,0.0,1.0,1.98e-05,1.0,5.44e-05,1.0,0.000544,1.0,0.000544,1.0,6.699999999999999e-05,1.0,0.0006799999999999,0.0,5.1992000000000006e-05,1.0,6.03e-05,1.0,1.34e-05,1.0,4.02e-05,1.0,5.36e-05
mmlu-professional-psychology.val.521,Model C,0.0,0.0,0.0,4.14e-05,1.0,0.0001112,1.0,0.001112,1.0,0.001112,1.0,0.000138,1.0,0.00139,0.0,0.000107088,0.0,0.0001241999999999,0.0,2.7600000000000003e-05,1.0,8.28e-05,1.0,0.0001104
mmlu-college-biology.val.14,claude-v2,1.0,0.0016799999999999,0.0,6.269999999999999e-05,1.0,0.000168,1.0,0.0016799999999999,1.0,0.0016799999999999,0.0,0.0002089999999999,1.0,0.0021,0.0,0.000162184,0.0,0.0001881,0.0,4.1800000000000006e-05,0.0,0.0001253999999999,1.0,0.0001664
mmlu-miscellaneous.val.435,Model C,0.0,0.0,1.0,2.07e-05,1.0,5.6e-05,1.0,0.00056,1.0,0.00056,1.0,6.9e-05,1.0,0.0007,0.0,5.354400000000001e-05,1.0,6.12e-05,0.0,1.38e-05,1.0,4.14e-05,1.0,5.44e-05
mmlu-professional-law.val.691,claude-v2,0.0,0.002832,1.0,0.0001059,0.0,0.0002832,0.0,0.002832,0.0,0.002832,0.0,0.000353,1.0,0.00354,0.0,0.000273928,0.0,0.0003177,0.0,7.06e-05,1.0,0.0002118,0.0,0.0002824
mmlu-high-school-european-history.val.63,llama-2-70b-chat,0.0,0.0,0.0,0.0001049999999999,1.0,0.0002808,0.0,0.002808,0.0,0.002808,1.0,0.00035,1.0,0.00351,0.0,0.0002716,0.0,0.0003149999999999,0.0,7.000000000000001e-05,0.0,0.0002099999999999,1.0,0.0002792
hellaswag.val.8470,B,0.0,0.0,0.0,7.89e-05,1.0,0.0002136,1.0,0.002112,1.0,0.002112,0.0,0.000265,1.0,0.00267,0.0,0.000204088,0.0,0.0002367,0.0,5.260000000000001e-05,0.0,0.0001578,0.0,0.0002096
mmlu-professional-law.val.675,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00089,0.0,6.596e-05,0.0,7.65e-05,1.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
grade-school-math.dev.655,meta/llama-2-70b-chat,0.75,0.0003492,0.75,0.0001386,0.75,0.0003968,0.75,0.0038959999999999,0.75,0.0049999999999999,0.75,0.000425,0.75,0.00658,0.75,0.000294104,0.75,0.0003492,0.75,7.58e-05,0.75,0.0002586,0.75,0.0002904
mmlu-high-school-world-history.val.105,claude-v2,1.0,0.003448,1.0,0.000129,0.0,0.0003448,0.0,0.003448,1.0,0.003448,0.0,0.00043,1.0,0.00431,0.0,0.00033368,0.0,0.000387,1.0,8.6e-05,1.0,0.000258,1.0,0.0003432
arc-challenge.val.285,Model C) claude-v2,0.0,0.0,1.0,1.8e-05,1.0,4.88e-05,1.0,0.000488,1.0,0.000488,1.0,6.2e-05,1.0,0.00061,0.0,4.656e-05,0.0,5.4000000000000005e-05,1.0,1.2e-05,1.0,3.6e-05,1.0,4.72e-05
mmlu-elementary-mathematics.val.328,Model C,0.0,0.0,0.0,4.47e-05,0.0,0.00012,0.0,0.0012,0.0,0.0012,0.0,0.000149,1.0,0.0015,0.0,0.000115624,0.0,0.0001341,0.0,2.9800000000000003e-05,0.0,8.94e-05,0.0,0.0001192
winogrande.dev.82,meta/llama-2-70b-chat,1.0,4.59e-05,1.0,1.53e-05,0.0,4.16e-05,1.0,0.000416,1.0,0.000416,0.0,5.3e-05,0.0,0.00055,0.0,3.9576e-05,1.0,4.59e-05,0.0,1.02e-05,0.0,3.06e-05,0.0,4.08e-05
hellaswag.val.8367,C) claude-v2,0.0,0.0,0.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00252,0.0,0.000192448,0.0,0.0002223,0.0,4.9600000000000006e-05,0.0,0.0001487999999999,0.0,0.0001976
mmlu-miscellaneous.val.102,Model C - Copper(II) sulfate hydrate,0.0,0.0,0.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,7.039999999999999e-05
hellaswag.val.3759,D,0.0,0.0,0.0,8.07e-05,1.0,0.0002184,1.0,0.00216,0.0,0.00216,0.0,0.000269,0.0,0.0027,0.0,0.000208744,0.0,0.0002412,0.0,5.380000000000001e-05,0.0,0.0001614,1.0,0.0002144
mmlu-professional-law.val.112,meta/llama-2-70b-chat,0.0,0.0003051,1.0,0.0001016999999999,0.0,0.000272,0.0,0.00272,1.0,0.00272,1.0,0.000339,1.0,0.0034,0.0,0.000263064,0.0,0.0003051,0.0,6.780000000000001e-05,1.0,0.0002033999999999,1.0,0.0002704
hellaswag.val.7917,Model D,0.0,0.0,1.0,7.56e-05,0.0,0.0002048,1.0,0.002024,0.0,0.002024,0.0,0.000252,1.0,0.00256,1.0,0.000195552,0.0,0.0002259,1.0,5.0400000000000005e-05,1.0,0.0001512,1.0,0.0002008
abstract2title.test.171,meta/llama-2-70b-chat,1.0,0.0003672,1.0,9.51e-05,1.0,0.000304,1.0,0.002968,1.0,0.003088,1.0,0.000323,1.0,0.00353,1.0,0.000235904,1.0,0.0003672,1.0,6.0200000000000006e-05,1.0,0.0001962,1.0,0.0002504
grade-school-math.dev.947,meta/llama-2-70b-chat,0.25,0.0006642,0.25,0.0001632,0.75,0.0007304,0.25,0.006536,0.25,0.006536,0.25,0.000745,0.75,0.01228,0.25,0.000287896,0.25,0.0006642,0.25,0.0001212,0.25,0.000285,0.25,0.000236
hellaswag.val.4317,Model A,0.0,0.0,1.0,7.83e-05,0.0,0.000212,1.0,0.002096,0.0,0.00212,0.0,0.000263,1.0,0.00265,1.0,0.000202536,1.0,0.0002349,1.0,5.220000000000001e-05,1.0,0.0001566,1.0,0.000208
mmlu-professional-psychology.val.226,C) claude-v2,0.0,0.0,1.0,4.32e-05,1.0,0.000116,1.0,0.00116,1.0,0.00116,1.0,0.000144,1.0,0.00145,0.0,0.000111744,0.0,0.0001295999999999,0.0,2.88e-05,1.0,8.64e-05,1.0,0.0001144
mmlu-professional-law.val.412,meta/llama-2-70b-chat,0.0,0.0002547,0.0,8.49e-05,0.0,0.0002272,0.0,0.002272,0.0,0.002272,0.0,0.000283,1.0,0.00284,0.0,0.000219608,0.0,0.0002547,1.0,5.660000000000001e-05,1.0,0.0001698,0.0,0.0002256
hellaswag.val.7144,Model C,0.0,0.0,0.0,8.31e-05,0.0,0.0002248,0.0,0.002224,1.0,0.002224,1.0,0.000279,1.0,0.00278,0.0,0.000214952,1.0,0.0002493,0.0,5.5400000000000005e-05,0.0,0.0001662,1.0,0.0002208
arc-challenge.test.876,A,0.0,0.0,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.9e-05,1.0,0.00101,1.0,7.5272e-05,1.0,8.730000000000001e-05,1.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.680000000000001e-05
mmlu-high-school-statistics.val.151,claude-v2,1.0,0.000944,1.0,3.51e-05,0.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000117,1.0,0.00121,0.0,9.0792e-05,0.0,0.0001053,0.0,2.34e-05,1.0,7.02e-05,0.0,9.28e-05
hellaswag.val.9294,D,0.0,0.0,1.0,8.4e-05,0.0,0.000228,0.0,0.002256,1.0,0.002256,0.0,0.000281,1.0,0.00285,1.0,0.000218056,0.0,0.000252,1.0,5.62e-05,1.0,0.0001686,1.0,0.000224
mmlu-clinical-knowledge.val.46,Model B,0.0,0.0,0.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
mmlu-security-studies.val.43,D,0.0,0.0,0.0,8.01e-05,0.0,0.0002144,1.0,0.002144,0.0,0.002144,0.0,0.000267,0.0,0.00268,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,0.0,0.0002136
mmlu-college-physics.val.44,Model C - meta/llama-2-70b-chat,0.0,0.0,0.0,3.96e-05,0.0,0.0001064,0.0,0.0010639999999999,0.0,0.0010639999999999,0.0,0.0001319999999999,0.0,0.00133,0.0,0.000102432,0.0,0.0001188,1.0,2.64e-05,1.0,7.92e-05,0.0,0.0001055999999999
mmlu-high-school-mathematics.val.154,Model A - WizardLM/WizardLM-13B-V1.2,0.0,0.0,0.0,2.73e-05,1.0,7.360000000000001e-05,0.0,0.000736,0.0,0.000736,0.0,9.3e-05,0.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.46e-05,0.0,7.280000000000001e-05
mmlu-high-school-chemistry.val.83,Model C,0.0,0.0,0.0,2.1e-05,1.0,5.6800000000000005e-05,1.0,0.000568,1.0,0.000568,1.0,7e-05,1.0,0.00071,0.0,5.432e-05,0.0,6.3e-05,0.0,1.4e-05,1.0,4.2e-05,1.0,5.520000000000001e-05
mmlu-professional-law.val.1285,claude-v2,1.0,0.002376,0.0,8.88e-05,0.0,0.0002376,0.0,0.002376,1.0,0.002376,0.0,0.000296,0.0,0.00297,0.0,0.000229696,0.0,0.0002664,0.0,5.920000000000001e-05,0.0,0.000177,0.0,0.000236
mmlu-sociology.val.73,Model C,0.0,0.0,1.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,1.0,5.46e-05,1.0,7.200000000000002e-05
mmlu-high-school-government-and-politics.val.98,Model D,0.0,0.0,0.0,3.69e-05,1.0,9.92e-05,1.0,0.000992,1.0,0.000992,1.0,0.000123,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001107,0.0,2.46e-05,1.0,7.379999999999999e-05,1.0,9.76e-05
hellaswag.val.1801,Model C,0.0,0.0,1.0,2.52e-05,0.0,6.800000000000001e-05,1.0,0.00068,0.0,0.00068,0.0,8.6e-05,1.0,0.00088,1.0,6.5184e-05,0.0,7.469999999999999e-05,1.0,1.6800000000000002e-05,1.0,5.04e-05,0.0,6.640000000000001e-05
hellaswag.val.2830,claude-v2,0.0,0.001024,0.0,3.78e-05,0.0,0.0001024,0.0,0.001024,0.0,0.001024,0.0,0.000127,1.0,0.00128,1.0,9.8552e-05,0.0,0.0001133999999999,1.0,2.54e-05,1.0,7.62e-05,0.0,0.0001008
mmlu-miscellaneous.val.183,Meta/llama-2-70b-chat,0.0,0.0,0.0,3.33e-05,1.0,8.960000000000001e-05,1.0,0.000896,1.0,0.000896,1.0,0.000111,1.0,0.00112,0.0,8.6136e-05,0.0,9.990000000000002e-05,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,8.88e-05
mmlu-college-mathematics.val.58,Meta/llama-2-70b-chat,0.0,0.0,0.0,3.69e-05,0.0,9.92e-05,0.0,0.000992,0.0,0.000992,0.0,0.000125,0.0,0.00124,0.0,9.5448e-05,0.0,0.0001107,0.0,2.46e-05,0.0,7.379999999999999e-05,0.0,9.84e-05
mmlu-high-school-microeconomics.val.98,Model D,0.0,0.0,1.0,3.03e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,1.0,2.02e-05,0.0,6.06e-05,1.0,8.08e-05
mmlu-professional-law.val.382,claude-v2,0.0,0.002784,0.0,0.0001041,0.0,0.0002784,0.0,0.002784,0.0,0.002784,0.0,0.000347,0.0,0.00351,0.0,0.000269272,0.0,0.0003123,0.0,6.939999999999999e-05,0.0,0.0002082,0.0,0.0002768
mmlu-high-school-statistics.val.53,B) gpt-4-1106-preview,0.0,0.0,1.0,5.25e-05,1.0,0.0001408,1.0,0.001408,1.0,0.001408,0.0,0.000175,1.0,0.00176,0.0,0.0001357999999999,0.0,0.0001575,0.0,3.5000000000000004e-05,1.0,0.000105,1.0,0.00014
hellaswag.val.8229,B,0.0,0.0,0.0,9.119999999999998e-05,1.0,0.0002472,1.0,0.002448,1.0,0.002448,1.0,0.000307,1.0,0.00306,0.0,0.00023668,1.0,0.0002736,0.0,6.1000000000000005e-05,1.0,0.0001829999999999,1.0,0.0002432
grade-school-math.dev.922,meta/llama-2-70b-chat,0.25,0.0004545,0.25,0.0001905,0.75,0.000772,0.5,0.006376,0.25,0.007936,0.5,0.000767,0.75,0.01172,0.75,0.0004074,0.25,0.0004545,0.25,0.0001526,0.5,0.0002742,0.25,0.00056
hellaswag.val.7511,Model C,0.0,0.0,1.0,7.110000000000001e-05,0.0,0.0001928,0.0,0.0019039999999999,1.0,0.0019039999999999,0.0,0.0002389999999999,1.0,0.00238,1.0,0.000183912,1.0,0.0002123999999999,1.0,4.74e-05,1.0,0.0001422,1.0,0.0001887999999999
grade-school-math.dev.6434,meta/llama-2-70b-chat,0.75,0.0003689999999999,0.75,0.0001365,0.75,0.0005688,0.75,0.00456,0.75,0.005736,0.75,0.000472,0.75,0.00756,0.75,0.000336784,0.75,0.0003689999999999,0.25,0.0001028,0.75,0.0002556,0.75,0.000424
hellaswag.val.9818,A) WizardLM/WizardLM-13B-V1.2,0.0,0.0,1.0,5.73e-05,0.0,0.000156,1.0,0.001536,1.0,0.001536,1.0,0.000193,1.0,0.00195,1.0,0.000148216,1.0,0.0001718999999999,1.0,3.820000000000001e-05,1.0,0.0001146,1.0,0.000152
grade-school-math.dev.942,meta/llama-2-70b-chat,0.25,0.0003402,0.25,0.0001323,0.75,0.0004936,0.75,0.004072,0.5,0.0046,0.5,0.0004579999999999,0.75,0.0077,0.25,0.000381792,0.25,0.0003402,0.25,9.52e-05,1.0,0.0002435999999999,0.75,0.0003512
hellaswag.val.316,D,0.0,0.0,0.0,5.94e-05,1.0,0.0001592,0.0,0.0015919999999999,0.0,0.0015919999999999,1.0,0.0001999999999999,1.0,0.00199,0.0,0.000153648,1.0,0.0001773,1.0,3.960000000000001e-05,1.0,0.0001188,1.0,0.0001576
mmlu-professional-law.val.1110,claude-v2,0.0,0.00252,0.0,9.42e-05,0.0,0.000252,0.0,0.00252,0.0,0.00252,0.0,0.000314,0.0,0.00315,0.0,0.000243664,0.0,0.0002826,0.0,6.280000000000001e-05,0.0,0.0001884,0.0,0.0002504
hellaswag.val.1034,B) claude-v2,0.0,0.0,1.0,3.66e-05,1.0,9.84e-05,1.0,0.000984,1.0,0.000984,1.0,0.000124,1.0,0.00123,0.0,9.4672e-05,1.0,0.0001088999999999,0.0,2.44e-05,1.0,7.32e-05,1.0,9.76e-05
mmlu-moral-scenarios.val.712,"B) Not wrong, Wrong",0.0,0.0,0.0,4.05e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,0.0,0.000135,1.0,0.00139,0.0,0.0001047599999999,0.0,0.0001215,0.0,2.7e-05,0.0,8.1e-05,1.0,0.000108
grade-school-math.dev.2904,meta/llama-2-70b-chat,0.25,0.0003744,0.25,0.0001352999999999,0.75,0.000556,0.75,0.004072,0.75,0.006616,0.75,0.00051,0.75,0.00713,0.25,0.000342992,0.25,0.0003744,0.25,8.5e-05,0.75,0.0002873999999999,0.0,0.000496
abstract2title.test.209,meta/llama-2-70b-chat,1.0,0.0005418,1.0,0.0001029,1.0,0.0003128,1.0,0.003128,1.0,0.003392,1.0,0.000363,1.0,0.004,1.0,0.000246768,1.0,0.0005418,1.0,6.54e-05,1.0,0.000234,1.0,0.0002632
hellaswag.val.7557,D) meta/llama-2-70b-chat,0.0,0.0,0.0,8.069999999999998e-05,1.0,0.0002192,0.0,0.002168,1.0,0.002168,1.0,0.000272,1.0,0.00274,0.0,0.00020952,0.0,0.000243,0.0,5.4000000000000005e-05,0.0,0.0001613999999999,1.0,0.0002152
grade-school-math.dev.3294,meta/llama-2-70b-chat,0.25,0.0003573,0.25,0.0001496999999999,0.75,0.0006672,0.75,0.004416,0.5,0.006072,0.5,0.0005189999999999,0.5,0.00963,0.25,0.000262288,0.25,0.0003573,0.25,9.56e-05,0.25,0.0002963999999999,0.25,0.0002472
mmlu-professional-law.val.486,Llama-2-70b-chat,0.0,0.0,0.0,7.199999999999999e-05,1.0,0.0001928,0.0,0.001928,1.0,0.001928,0.0,0.00024,1.0,0.00241,0.0,0.00018624,0.0,0.000216,0.0,4.8e-05,0.0,0.0001439999999999,0.0,0.0001911999999999
hellaswag.val.7004,Model C,0.0,0.0,0.0,7.230000000000001e-05,1.0,0.000196,1.0,0.001936,1.0,0.001936,1.0,0.000243,1.0,0.00242,0.0,0.000187016,1.0,0.000216,0.0,4.8200000000000006e-05,0.0,0.0001446,1.0,0.000192
mmlu-professional-law.val.1108,claude-v2,0.0,0.002208,0.0,8.249999999999999e-05,1.0,0.0002208,1.0,0.002208,0.0,0.002208,1.0,0.000275,1.0,0.00279,0.0,0.0002134,0.0,0.0002475,1.0,5.5e-05,0.0,0.0001649999999999,1.0,0.0002192
mmlu-econometrics.val.25,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Model C — correctness: 0, cost: 0.5
Model D",0.0,0.0,1.0,2.7e-05,0.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
winogrande.dev.803,B) Patricia,0.0,0.0,1.0,1.95e-05,0.0,5.36e-05,1.0,0.000536,0.0,0.000536,0.0,6.8e-05,1.0,0.0007,1.0,5.1216000000000006e-05,0.0,5.940000000000001e-05,1.0,1.32e-05,1.0,3.96e-05,0.0,5.28e-05
grade-school-math.dev.3284,meta/llama-2-70b-chat,0.25,0.0004842,0.25,0.0001886999999999,0.25,0.0008416,0.25,0.0067119999999999,0.75,0.009856,0.75,0.000722,0.75,0.0098,0.75,0.000305744,0.25,0.0004842,0.25,0.0001012,0.25,0.0002862,0.25,0.0007264
grade-school-math.dev.3258,meta/llama-2-70b-chat,0.25,0.0004239,0.75,0.0001989,0.75,0.0006848,0.75,0.006704,0.75,0.006416,0.75,0.000664,0.75,0.01054,0.75,0.000377912,0.25,0.0004239,0.25,9.74e-05,0.5,0.0002681999999999,0.75,0.0004576
grade-school-math.dev.2426,meta/llama-2-70b-chat,0.75,0.0003662999999999,0.75,0.0001623,0.5,0.0005928,0.75,0.004776,0.5,0.0060479999999999,0.5,0.000433,0.5,0.00942,0.5,0.000322816,0.75,0.0003662999999999,0.25,0.0003988,0.25,0.0001962,0.75,0.0004264
grade-school-math.dev.2676,meta/llama-2-70b-chat,0.25,0.0004167,0.75,0.0001299,0.5,0.0005344,0.75,0.004576,0.75,0.006232,0.75,0.000501,0.5,0.00659,0.25,0.000296432,0.25,0.0004167,0.5,7.54e-05,0.75,0.000252,0.75,0.0003376
mbpp.dev.22,Code-llama-instruct-34b-chat,0.0,0.0,1.0,7.199999999999999e-05,1.0,0.0004576,1.0,0.003688,1.0,0.004888,1.0,0.000361,1.0,0.0074,1.0,0.000193224,1.0,0.0001988999999999,1.0,4.54e-05,1.0,0.0001266,1.0,0.0002832
hellaswag.val.3023,Model D,0.0,0.0,1.0,2.64e-05,0.0,7.12e-05,0.0,0.000712,0.0,0.000712,0.0,9e-05,0.0,0.00089,1.0,6.828800000000001e-05,0.0,7.83e-05,1.0,1.7599999999999998e-05,0.0,5.28e-05,0.0,6.96e-05
mmlu-security-studies.val.105,Model C,0.0,0.0,1.0,7.29e-05,1.0,0.0001952,1.0,0.001952,1.0,0.001952,1.0,0.000243,1.0,0.00244,0.0,0.000188568,0.0,0.0002187,1.0,4.860000000000001e-05,1.0,0.0001458,1.0,0.0001936
chinese_zodiac.dev.266,meta/llama-2-70b-chat,0.0,0.0001008,0.0,3.33e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.000122608,0.0,0.0001008,0.0,2.22e-05,1.0,6.659999999999999e-05,1.0,8.88e-05
mmlu-prehistory.val.144,Model C,0.0,0.0,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00102,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.840000000000001e-05
chinese_ancient_poetry.dev.5,meta/llama-2-70b-chat,0.0,0.0001269,0.0,3.63e-05,0.0,0.0001192,0.0,0.00112,1.0,0.001144,0.0,0.000129,0.0,0.00146,0.0,9.7776e-05,0.0,0.0001269,0.0,2.46e-05,0.0,7.500000000000001e-05,0.0,9.92e-05
hellaswag.val.9722,B) claude-v2,0.0,0.0,1.0,6.93e-05,1.0,0.000188,0.0,0.00188,1.0,0.001856,0.0,0.000233,1.0,0.00235,0.0,0.000179256,1.0,0.000207,0.0,4.6200000000000005e-05,0.0,0.0001386,1.0,0.000184
mmlu-professional-psychology.val.137,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.31e-05,1.0,6.24e-05,0.0,0.000624,0.0,0.000624,0.0,7.699999999999999e-05,0.0,0.00081,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,0.0,4.6200000000000005e-05,0.0,6.16e-05
hellaswag.val.6591,C,0.0,0.0,0.0,9.18e-05,1.0,0.000248,1.0,0.002456,1.0,0.002456,1.0,0.000306,1.0,0.00307,0.0,0.0002374559999999,1.0,0.0002745,0.0,6.120000000000001e-05,0.0,0.0001836,1.0,0.000244
grade-school-math.dev.2728,meta/llama-2-70b-chat,0.25,0.0004113,0.25,0.000204,0.25,0.0007968,0.25,0.0066,0.25,0.0078,0.25,0.000684,0.25,0.0120299999999999,0.25,0.000394984,0.25,0.0004113,0.25,0.0001238,0.25,0.00033,1.0,0.0002984
arc-challenge.test.273,Model B,0.0,0.0,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.7e-05,1.0,0.00086,0.0,6.596e-05,1.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
hellaswag.val.9608,B,0.0,0.0,1.0,8.43e-05,0.0,0.000228,0.0,0.002256,0.0,0.002256,0.0,0.000283,1.0,0.00285,1.0,0.000218056,0.0,0.0002528999999999,1.0,5.62e-05,0.0,0.0001686,1.0,0.000224
grade-school-math.dev.6486,meta/llama-2-70b-chat,0.25,0.0003789,0.25,0.0001314,0.75,0.000652,0.75,0.004576,0.75,0.0074319999999999,0.25,0.000666,0.75,0.0095,0.75,0.00036084,0.25,0.0003789,0.75,8.44e-05,0.75,0.0002304,0.75,0.000396
mmlu-high-school-macroeconomics.val.244,Meta/llama-2-70b-chat,0.0,0.0,0.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,1.0,5.46e-05,1.0,7.200000000000002e-05
arc-challenge.test.651,Model D,0.0,0.0,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00083,0.0,6.3632e-05,1.0,7.38e-05,0.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
hellaswag.val.8183,B,0.0,0.0,0.0,6.749999999999999e-05,1.0,0.0001808,1.0,0.001808,1.0,0.001808,0.0,0.000225,1.0,0.00229,0.0,0.0001746,0.0,0.0002025,0.0,4.5e-05,0.0,0.0001349999999999,0.0,0.0001792
hellaswag.val.4282,Model B,0.0,0.0,0.0,7.529999999999999e-05,1.0,0.0002048,1.0,0.002024,1.0,0.002024,1.0,0.000254,1.0,0.00256,0.0,0.000195552,1.0,0.0002267999999999,0.0,5.0400000000000005e-05,0.0,0.0001512,1.0,0.0002008
hellaswag.val.9162,Model D,0.0,0.0,1.0,6.81e-05,0.0,0.0001824,1.0,0.0018239999999999,0.0,0.0018239999999999,0.0,0.0002289999999999,1.0,0.00231,1.0,0.0001761519999999,0.0,0.0002042999999999,1.0,4.5400000000000006e-05,1.0,0.0001362,1.0,0.0001808
mmlu-moral-scenarios.val.371,"D) Wrong, Not wrong",0.0,0.0,0.0,4.32e-05,0.0,0.000116,0.0,0.00116,0.0,0.00116,0.0,0.000144,1.0,0.00148,0.0,0.000111744,0.0,0.0001295999999999,0.0,2.88e-05,0.0,8.64e-05,0.0,0.0001144
hellaswag.val.1878,Model C,0.0,0.0,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,1.0,0.00122,0.0,9.3896e-05,1.0,0.000108,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
mmlu-security-studies.val.28,claude-v2,1.0,0.00204,1.0,7.62e-05,1.0,0.000204,1.0,0.00204,1.0,0.00204,1.0,0.000254,1.0,0.00255,0.0,0.0001971039999999,0.0,0.0002286,1.0,5.080000000000001e-05,1.0,0.0001524,1.0,0.0002032
grade-school-math.dev.1733,meta/llama-2-70b-chat,0.25,0.0005625,0.5,0.0002321999999999,0.75,0.0007064,0.25,0.006824,0.75,0.008672,0.25,0.000553,0.5,0.01081,0.25,0.00041128,0.25,0.0005625,0.25,0.0001164,0.5,0.0003492,0.25,0.0005528
mmlu-professional-accounting.val.66,meta/llama-2-70b-chat,0.0,0.0001422,0.0,4.74e-05,1.0,0.0001272,1.0,0.001272,1.0,0.001272,1.0,0.0001599999999999,1.0,0.00162,0.0,0.000122608,0.0,0.0001422,0.0,3.160000000000001e-05,1.0,9.48e-05,0.0,0.0001256
mbpp.dev.379,meta/llama-2-70b-chat,0.0,0.0002961,1.0,8.369999999999999e-05,1.0,0.0005455999999999,0.0,0.007568,1.0,0.006152,1.0,0.000231,0.0,0.00997,1.0,0.0001652879999999,0.0,0.0002961,0.0,5.660000000000001e-05,1.0,0.0001824,0.0,0.0001944
grade-school-math.dev.6547,meta/llama-2-70b-chat,0.75,0.0003465,0.75,0.000132,0.75,0.0006031999999999,0.75,0.005528,0.75,0.006344,0.5,0.000413,0.75,0.0060999999999999,0.75,0.000297208,0.75,0.0003465,0.5,8.14e-05,0.5,0.0001962,0.5,0.0003336
mmlu-abstract-algebra.val.52,Model B,0.0,0.0,0.0,2.61e-05,0.0,7.04e-05,0.0,0.000704,0.0,0.000704,0.0,8.7e-05,0.0,0.00091,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,0.0,5.16e-05,0.0,6.96e-05
grade-school-math.dev.3745,meta/llama-2-70b-chat,0.75,0.0003591,0.75,0.0001404,0.75,0.0005448,0.75,0.004248,0.75,0.004704,0.75,0.00049,0.75,0.0078599999999999,0.75,0.00034144,0.75,0.0003591,0.75,8.82e-05,0.75,0.0002556,0.75,0.00038
grade-school-math.dev.1351,meta/llama-2-70b-chat,0.25,0.000423,0.25,0.0001484999999999,0.5,0.0005304,0.75,0.005376,0.75,0.005328,0.5,0.000423,0.75,0.00801,0.75,0.00039188,0.25,0.000423,0.75,8.82e-05,0.75,0.0002165999999999,0.75,0.0003664
mmlu-professional-accounting.val.75,Model B - Variable sampling,0.0,0.0,1.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.0009,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,1.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
grade-school-math.dev.6279,meta/llama-2-70b-chat,0.5,0.0003708,0.75,0.0001758,0.75,0.000584,0.75,0.003992,0.75,0.00692,0.75,0.000553,0.5,0.00712,0.5,0.000321264,0.5,0.0003708,0.75,9.36e-05,0.75,0.0002652,0.75,0.0003712
grade-school-math.dev.2022,meta/llama-2-70b-chat,0.75,0.0003978,0.75,0.000144,0.75,0.000608,0.75,0.004496,0.75,0.005288,0.75,0.000428,0.75,0.00592,0.75,0.000311952,0.75,0.0003978,0.75,9e-05,0.75,0.0002688,0.75,0.000376
hellaswag.val.1423,Model C (claude-v2),0.0,0.0,1.0,2.7e-05,0.0,7.280000000000001e-05,0.0,0.000728,1.0,0.000728,1.0,9.2e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.01e-05,0.0,1.8e-05,0.0,5.4e-05,0.0,7.120000000000001e-05
mmlu-prehistory.val.64,claude-v2,1.0,0.0015199999999999,1.0,5.67e-05,1.0,0.000152,1.0,0.0015199999999999,1.0,0.0015199999999999,1.0,0.0001889999999999,1.0,0.0019,0.0,0.000146664,0.0,0.0001701,1.0,3.78e-05,1.0,0.0001134,1.0,0.0001504
mbpp.dev.68,Code-llama-instruct-34b-chat,0.0,0.0,1.0,4.62e-05,1.0,0.0002496,1.0,0.001416,1.0,0.004344,1.0,0.000184,1.0,0.00513,0.0,9.312e-05,1.0,0.0001997999999999,1.0,4.14e-05,1.0,7.26e-05,1.0,0.0001
chinese_zodiac.dev.127,meta/llama-2-70b-chat,0.0,0.0001197,1.0,3.33e-05,0.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.000113,1.0,0.00115,0.0,0.000114072,0.0,0.0001197,0.0,2.22e-05,1.0,6.659999999999999e-05,0.0,8.960000000000001e-05
grade-school-math.dev.5988,meta/llama-2-70b-chat,0.5,0.0003564,0.25,0.0001721999999999,0.75,0.0005512,0.75,0.003928,0.75,0.005128,0.75,0.000424,0.75,0.00713,0.75,0.00024832,0.5,0.0003564,0.75,7.400000000000001e-05,0.75,0.0002123999999999,0.75,0.000308
grade-school-math.dev.7240,meta/llama-2-70b-chat,0.25,0.0005985,0.25,0.0002156999999999,0.25,0.000784,0.25,0.0055119999999999,0.25,0.006448,0.25,0.001009,0.25,0.01145,0.25,0.000377912,0.25,0.0005985,0.25,0.000112,0.25,0.0003384,0.25,0.0005224
grade-school-math.dev.717,meta/llama-2-70b-chat,0.25,0.0003582,0.25,0.0001539,0.25,0.0004864,0.25,0.004504,0.0,0.002632,0.75,0.000379,0.75,0.00686,0.5,0.000313504,0.25,0.0003582,0.25,0.0001038,0.75,0.0002267999999999,0.25,0.0003176
grade-school-math.dev.4926,meta/llama-2-70b-chat,0.75,0.0003582,0.5,0.0001418999999999,0.75,0.0004992,0.75,0.004128,0.75,0.004632,0.75,0.000456,0.75,0.00717,0.5,0.00030652,0.75,0.0003582,0.75,8.6e-05,0.25,0.0002501999999999,0.25,0.0002384
mmlu-jurisprudence.val.69,claude-v2,0.0,0.000832,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,0.0,0.000832,1.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,1.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.240000000000001e-05
consensus_summary.dev.259,Meta/llama-2-70b-chat,0.0,0.0,0.75,0.0001728,0.5,0.0003336,1.0,0.0027359999999999,0.75,0.0069359999999999,0.75,0.000414,1.0,0.00408,0.75,0.0004275759999999,0.75,0.0004446,0.75,9.3e-05,0.75,0.0002988,0.75,0.0002928
hellaswag.val.6994,C,0.0,0.0,0.0,8.730000000000001e-05,0.0,0.0002336,0.0,0.002336,1.0,0.002336,1.0,0.0002929999999999,1.0,0.00295,0.0,0.000225816,1.0,0.0002619,0.0,5.8200000000000005e-05,0.0,0.0001746,1.0,0.000232
mmlu-high-school-geography.val.105,A) Latin America,0.0,0.0,1.0,2.52e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00089,0.0,6.596e-05,0.0,7.65e-05,1.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
mmlu-professional-law.val.36,meta/llama-2-70b-chat,0.0,0.0003654,0.0,0.0001217999999999,0.0,0.0003256,0.0,0.003256,0.0,0.003256,0.0,0.000408,0.0,0.0041,0.0,0.000315056,0.0,0.0003654,1.0,8.120000000000001e-05,0.0,0.0002435999999999,0.0,0.000324
grade-school-math.dev.1814,meta/llama-2-70b-chat,0.75,0.0003285,0.75,0.000129,0.75,0.000468,0.75,0.003912,0.75,0.0060479999999999,0.75,0.000469,0.5,0.0074699999999999,0.25,0.000251424,0.75,0.0003285,0.5,8.900000000000001e-05,0.75,0.0002351999999999,0.75,0.0003888
hellaswag.val.6557,C) claude-v2,0.0,0.0,0.0,6.36e-05,1.0,0.0001704,1.0,0.001704,1.0,0.001704,1.0,0.000214,1.0,0.00216,0.0,0.0001645119999999,1.0,0.0001908,0.0,4.24e-05,1.0,0.0001272,1.0,0.0001687999999999
mmlu-miscellaneous.val.61,claude-v2,1.0,0.000616,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,0.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
mmlu-high-school-macroeconomics.val.372,Model C,0.0,0.0,0.0,2.73e-05,0.0,7.360000000000001e-05,0.0,0.000736,0.0,0.000736,0.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,1.0,1.82e-05,0.0,5.46e-05,0.0,7.200000000000002e-05
grade-school-math.dev.3286,meta/llama-2-70b-chat,0.5,0.0003852,0.5,0.0001497,0.75,0.0005728,0.5,0.004696,0.75,0.005824,0.5,0.000459,0.5,0.00746,0.25,0.00035696,0.5,0.0003852,0.25,0.0001052,0.5,0.0002622,0.5,0.0004216
chinese_zodiac.dev.318,meta/llama-2-70b-chat,0.0,0.0001269,0.0,3.33e-05,0.0,9.2e-05,1.0,0.00092,0.0,0.00092,0.0,0.000113,0.0,0.00115,0.0,0.000110968,0.0,0.0001269,0.0,2.22e-05,0.0,6.659999999999999e-05,0.0,8.960000000000001e-05
mmlu-prehistory.val.0,meta/llama-2-70b-chat,0.0,7.020000000000001e-05,1.0,2.3400000000000003e-05,0.0,6.32e-05,0.0,0.000632,0.0,0.000632,0.0,8e-05,0.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.16e-05
grade-school-math.dev.4051,meta/llama-2-70b-chat,0.5,0.0003969,0.75,0.0001484999999999,0.75,0.0006616,0.75,0.005008,0.75,0.006664,0.75,0.000543,0.75,0.0095599999999999,0.25,0.000355408,0.5,0.0003969,0.25,8.740000000000001e-05,0.75,0.0002573999999999,0.75,0.0004272
mmlu-professional-law.val.139,claude-v2,0.0,0.003632,1.0,0.0001358999999999,0.0,0.0003632,0.0,0.003632,0.0,0.003632,1.0,0.000453,1.0,0.00454,0.0,0.000351528,0.0,0.0004077,1.0,9.06e-05,1.0,0.0002717999999999,0.0,0.0003623999999999
mmlu-professional-psychology.val.138,Model D) alliance,0.0,0.0,0.0,2.46e-05,1.0,6.64e-05,0.0,0.000664,0.0,0.000664,0.0,8.2e-05,1.0,0.00086,0.0,6.3632e-05,0.0,7.38e-05,1.0,1.64e-05,0.0,4.92e-05,0.0,6.48e-05
hellaswag.val.3173,claude-v2,1.0,0.000976,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000123,1.0,0.00125,1.0,9.3896e-05,0.0,0.000108,1.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
mmlu-moral-disputes.val.213,claude-v2,1.0,0.000976,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,0.0,0.000121,0.0,0.00122,0.0,9.3896e-05,0.0,0.0001089,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
hellaswag.val.3354,Model D,0.0,0.0,0.0,7.89e-05,0.0,0.0002112,0.0,0.002112,0.0,0.002112,0.0,0.000263,0.0,0.00264,0.0,0.000204088,0.0,0.0002358,0.0,5.260000000000001e-05,0.0,0.0001578,1.0,0.0002096
grade-school-math.dev.5493,meta/llama-2-70b-chat,0.75,0.0003906,0.25,0.0001529999999999,0.75,0.0005808,0.25,0.005568,0.75,0.005088,0.25,0.000614,0.75,0.00906,0.25,0.000268496,0.75,0.0003906,0.25,9.46e-05,0.25,0.0002657999999999,0.25,0.000468
grade-school-math.dev.5058,meta/llama-2-70b-chat,0.5,0.0004104,0.5,0.0001610999999999,0.75,0.0006768,0.75,0.00636,0.5,0.00672,0.25,0.000579,0.5,0.00957,0.25,0.000479568,0.5,0.0004104,0.75,9.2e-05,0.25,0.0002309999999999,0.75,0.0004416
chinese_tang_poetries.dev.28,meta/llama-2-70b-chat,0.0,4.6800000000000006e-05,0.0,1.65e-05,0.0,4.4e-05,0.0,0.00044,0.0,0.00044,0.0,5.1e-05,0.0,0.00055,0.0,3.4920000000000004e-05,0.0,4.6800000000000006e-05,0.0,9.199999999999998e-06,0.0,3.06e-05,0.0,4.08e-05
grade-school-math.dev.2867,meta/llama-2-70b-chat,0.75,0.000441,0.5,0.0001782,0.25,0.0006376,0.5,0.00688,0.25,0.007024,0.75,0.000652,0.25,0.01544,0.25,0.000358512,0.75,0.000441,0.25,0.00011,0.5,0.0003252,0.25,0.000508
grade-school-math.dev.18,meta/llama-2-70b-chat,0.25,0.0003519,0.25,0.0001532999999999,0.25,0.0005904,0.25,0.005352,0.75,0.007296,0.25,0.000543,0.5,0.00861,0.25,0.000339112,0.25,0.0003519,0.25,0.0001092,0.25,0.0002298,0.25,0.000244
mmlu-conceptual-physics.val.179,Model C,0.0,0.0,0.0,2.61e-05,0.0,7.04e-05,0.0,0.000704,0.0,0.000704,0.0,8.7e-05,0.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,0.0,6.96e-05
mmlu-moral-scenarios.val.235,"D) Wrong, Not wrong",0.0,0.0,1.0,4.11e-05,0.0,0.0001104,0.0,0.001104,0.0,0.001104,1.0,0.000137,1.0,0.00141,0.0,0.000106312,0.0,0.0001233,0.0,2.74e-05,0.0,8.22e-05,0.0,0.0001096
hellaswag.val.6955,C) claude-v2,0.0,0.0,0.0,8.549999999999999e-05,1.0,0.0002296,1.0,0.002296,1.0,0.002296,1.0,0.000288,1.0,0.0029,0.0,0.000221936,0.0,0.0002565,0.0,5.720000000000001e-05,1.0,0.0001716,1.0,0.000228
hellaswag.val.4326,B,0.0,0.0,0.0,8.249999999999999e-05,0.0,0.0002208,0.0,0.002208,1.0,0.002208,0.0,0.000277,1.0,0.00276,0.0,0.0002134,0.0,0.0002466,0.0,5.5e-05,0.0,0.0001649999999999,0.0,0.0002192
mmlu-public-relations.val.93,claude-v2,0.0,0.000632,1.0,2.3400000000000003e-05,1.0,6.32e-05,1.0,0.000632,0.0,0.000632,1.0,7.8e-05,0.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.16e-05
hellaswag.val.2960,claude-v2,1.0,0.000792,0.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,0.0,0.00099,0.0,7.604800000000001e-05,1.0,8.73e-05,0.0,1.96e-05,1.0,5.88e-05,0.0,7.760000000000002e-05
hellaswag.val.6222,claude-v2,1.0,0.002176,0.0,8.099999999999999e-05,0.0,0.0002176,0.0,0.002176,1.0,0.002176,0.0,0.0002729999999999,0.0,0.00275,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
hellaswag.val.7973,Model C,0.0,0.0,1.0,8.52e-05,1.0,0.0002304,1.0,0.00228,1.0,0.00228,0.0,0.000286,1.0,0.00288,1.0,0.0002203839999999,0.0,0.0002556,1.0,5.680000000000001e-05,1.0,0.0001697999999999,1.0,0.0002264
winogrande.dev.1241,"Model A — correctness: 1, cost: 0.8
Model B — correctness: 0, cost: 0.2
Correct choice: Model A",0.0,0.0,0.0,1.56e-05,1.0,4.24e-05,1.0,0.000424,1.0,0.000424,1.0,5.4000000000000005e-05,1.0,0.00056,1.0,4.0352e-05,0.0,4.6800000000000006e-05,1.0,1.04e-05,1.0,3.12e-05,0.0,4.16e-05
hellaswag.val.3231,Model C,0.0,0.0,0.0,4.11e-05,0.0,0.0001112,0.0,0.001112,0.0,0.001112,0.0,0.000138,1.0,0.00139,0.0,0.000107088,0.0,0.0001233,0.0,2.7600000000000003e-05,0.0,8.28e-05,0.0,0.0001096
mmlu-human-sexuality.val.56,claude-v2,1.0,0.000616,1.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6.08e-05
mmlu-clinical-knowledge.val.83,Model B,0.0,0.0,1.0,2.94e-05,0.0,7.920000000000001e-05,0.0,0.000792,1.0,0.000792,0.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,0.0,5.88e-05,0.0,7.760000000000002e-05
grade-school-math.dev.5082,meta/llama-2-70b-chat,0.5,0.0003393,0.5,0.0001557,0.5,0.0005096,0.75,0.0043999999999999,0.75,0.005312,0.75,0.000547,0.5,0.00655,0.5,0.000344544,0.5,0.0003393,0.75,9.04e-05,0.5,0.000276,0.75,0.0003528
hellaswag.val.5679,A,0.0,0.0,1.0,6.9e-05,1.0,0.0001872,1.0,0.001848,0.0,0.001872,1.0,0.000232,1.0,0.00234,1.0,0.0001784799999999,1.0,0.000207,1.0,4.600000000000001e-05,1.0,0.000138,1.0,0.0001832
hellaswag.val.3105,Model C,0.0,0.0,1.0,3.48e-05,1.0,9.36e-05,1.0,0.000936,1.0,0.000936,1.0,0.000118,1.0,0.0012,0.0,9.0016e-05,0.0,0.0001044,0.0,2.32e-05,0.0,6.96e-05,1.0,9.2e-05
hellaswag.val.9923,B,0.0,0.0,0.0,8.879999999999999e-05,1.0,0.0002384,1.0,0.002384,1.0,0.002384,0.0,0.000299,1.0,0.00301,0.0,0.000230472,0.0,0.0002673,0.0,5.94e-05,0.0,0.0001782,1.0,0.0002367999999999
mmlu-high-school-chemistry.val.142,Model C,0.0,0.0,0.0,4.71e-05,1.0,0.0001264,1.0,0.001264,1.0,0.001264,1.0,0.000157,1.0,0.00158,0.0,0.000121832,0.0,0.0001413,0.0,3.1400000000000004e-05,0.0,9.42e-05,0.0,0.0001256
