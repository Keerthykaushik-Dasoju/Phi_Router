sample_id,phi_prediction,phi_correctness,phi_cost,WizardLM/WizardLM-13B-V1.2_correctness,WizardLM/WizardLM-13B-V1.2_cost,claude-instant-v1_correctness,claude-instant-v1_cost,claude-v1_correctness,claude-v1_cost,claude-v2_correctness,claude-v2_cost,gpt-3.5-turbo-1106_correctness,gpt-3.5-turbo-1106_cost,gpt-4-1106-preview_correctness,gpt-4-1106-preview_cost,meta/code-llama-instruct-34b-chat_correctness,meta/code-llama-instruct-34b-chat_cost,meta/llama-2-70b-chat_correctness,meta/llama-2-70b-chat_cost,mistralai/mistral-7b-chat_correctness,mistralai/mistral-7b-chat_cost,mistralai/mixtral-8x7b-chat_correctness,mistralai/mixtral-8x7b-chat_cost,zero-one-ai/Yi-34B-Chat_correctness,zero-one-ai/Yi-34B-Chat_cost
mmlu-high-school-macroeconomics.val.127,meta/llama-2-70b-chat,0.0,7.56e-05,0.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.720000000000001e-05
mmlu-formal-logic.val.82,meta/code-llama-instruct-34b-chat,0.0,0.00012416,0.0,4.8e-05,1.0,0.0001288,1.0,0.0012879999999999,1.0,0.0012879999999999,0.0,0.0001599999999999,1.0,0.00161,0.0,0.00012416,0.0,0.000144,0.0,3.2000000000000005e-05,0.0,9.6e-05,0.0,0.000128
hellaswag.val.7968,claude-v1,0.0,0.001848,0.0,6.9e-05,0.0,0.0001848,0.0,0.001848,0.0,0.001848,1.0,0.000232,1.0,0.00234,0.0,0.0001784799999999,0.0,0.0002061,0.0,4.600000000000001e-05,0.0,0.000138,1.0,0.0001832
grade-school-math.dev.4037,claude-v1,0.25,0.00428,0.75,0.0001311,0.0,0.0005072,0.25,0.00428,0.25,0.0045439999999999,0.25,0.000429,0.75,0.0053799999999999,0.75,0.000241336,0.75,0.0002808,0.25,6.64e-05,0.75,0.0002219999999999,0.25,0.00022
mmlu-global-facts.val.6,meta/llama-2-70b-chat,0.0,7.290000000000001e-05,1.0,2.43e-05,0.0,6.560000000000001e-05,0.0,0.000656,0.0,0.000656,0.0,8.099999999999999e-05,0.0,0.00085,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,0.0,6.400000000000001e-05
grade-school-math.dev.1409,gpt-4-1106-preview,0.75,0.00679,0.75,0.0001437,0.75,0.0004856,0.75,0.004112,0.75,0.004712,0.75,0.000407,0.75,0.00679,0.75,0.000311176,0.75,0.0003546,0.75,8.68e-05,0.75,0.0002274,0.75,0.0003872
mmlu-high-school-microeconomics.val.133,gpt-4-1106-preview,1.0,0.00077,1.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00077,0.0,5.6648e-05,0.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.84e-05
grade-school-math.dev.3926,claude-v2,0.75,0.00456,0.75,0.0001368,0.75,0.000384,0.5,0.002184,0.75,0.00456,0.75,0.000394,0.5,0.0051,0.75,0.000265392,0.75,0.0003564,0.25,7.28e-05,0.25,0.0002058,0.5,0.0002223999999999
mmlu-moral-scenarios.val.389,claude-v1,1.0,0.0011439999999999,1.0,4.26e-05,0.0,0.0001144,1.0,0.0011439999999999,1.0,0.0011439999999999,0.0,0.0001419999999999,1.0,0.00146,0.0,0.000110192,0.0,0.0001278,0.0,2.84e-05,1.0,8.52e-05,1.0,0.0001128
hellaswag.val.3378,claude-v1,1.0,0.002296,0.0,8.58e-05,0.0,0.0002296,1.0,0.002296,1.0,0.002296,1.0,0.000288,1.0,0.00287,0.0,0.000221936,0.0,0.0002573999999999,0.0,5.720000000000001e-05,0.0,0.0001709999999999,1.0,0.000228
winogrande.dev.647,claude-v1,0.0,0.000384,1.0,1.41e-05,1.0,3.84e-05,0.0,0.000384,1.0,0.000384,0.0,4.9000000000000005e-05,1.0,0.00051,0.0,3.6472000000000006e-05,1.0,4.2300000000000005e-05,0.0,9.4e-06,0.0,2.82e-05,0.0,3.76e-05
mmlu-security-studies.val.180,gpt-4-1106-preview,0.0,0.00307,0.0,9.18e-05,0.0,0.0002456,1.0,0.002456,1.0,0.002456,0.0,0.000306,0.0,0.00307,0.0,0.0002374559999999,0.0,0.0002754,0.0,6.120000000000001e-05,1.0,0.0001836,1.0,0.0002448
arc-challenge.test.959,meta/llama-2-70b-chat,1.0,5.67e-05,1.0,1.89e-05,1.0,5.12e-05,1.0,0.000512,1.0,0.000512,1.0,6.5e-05,1.0,0.00067,0.0,4.8888e-05,1.0,5.67e-05,1.0,1.26e-05,1.0,3.78e-05,1.0,4.9600000000000006e-05
mmlu-professional-law.val.558,gpt-4-1106-preview,1.0,0.00467,0.0,0.0001397999999999,0.0,0.0003736,0.0,0.003736,0.0,0.003736,0.0,0.000466,1.0,0.00467,0.0,0.000361616,0.0,0.0004194,0.0,9.32e-05,0.0,0.0002795999999999,0.0,0.000372
mmlu-high-school-psychology.val.109,gpt-4-1106-preview,1.0,0.00078,0.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
grade-school-math.dev.5826,gpt-4-1106-preview,0.75,0.00784,0.75,0.0001518,0.75,0.0005816,0.75,0.0057199999999999,0.5,0.006416,0.75,0.000512,0.75,0.00784,0.25,0.000255304,0.75,0.000414,0.25,8.420000000000001e-05,0.25,0.0002556,0.5,0.000328
mmlu-high-school-psychology.val.206,gpt-4-1106-preview,1.0,0.00092,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00092,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
mmlu-professional-law.val.763,claude-v1,1.0,0.0017519999999999,1.0,6.54e-05,1.0,0.0001752,1.0,0.0017519999999999,1.0,0.0017519999999999,1.0,0.0002179999999999,1.0,0.00219,0.0,0.000169168,0.0,0.0001962,0.0,4.36e-05,1.0,0.0001308,1.0,0.0001736
hellaswag.val.6880,claude-v1,0.0,0.0022,0.0,8.219999999999999e-05,0.0,0.0002224,0.0,0.0022,1.0,0.0022,0.0,0.000276,1.0,0.00278,0.0,0.000212624,0.0,0.0002466,0.0,5.480000000000001e-05,0.0,0.0001643999999999,1.0,0.0002184
hellaswag.val.6222,claude-v1,0.0,0.002176,0.0,8.099999999999999e-05,0.0,0.0002176,0.0,0.002176,1.0,0.002176,0.0,0.0002729999999999,0.0,0.00275,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-high-school-microeconomics.val.49,WizardLM/WizardLM-13B-V1.2,1.0,3.21e-05,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,1.0,2.14e-05,1.0,6.42e-05,1.0,8.56e-05
arc-challenge.test.59,WizardLM/WizardLM-13B-V1.2,1.0,2.3400000000000003e-05,1.0,2.3400000000000003e-05,1.0,6.32e-05,1.0,0.000632,1.0,0.000632,1.0,8e-05,1.0,0.00082,0.0,6.0528e-05,1.0,7.020000000000001e-05,1.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,1.0,6.16e-05
mmlu-high-school-psychology.val.532,meta/llama-2-70b-chat,0.0,7.65e-05,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,0.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,0.0,5.1e-05,1.0,6.720000000000001e-05
mmlu-electrical-engineering.val.72,gpt-4-1106-preview,1.0,0.0007599999999999,0.0,2.25e-05,0.0,6.08e-05,1.0,0.000608,1.0,0.000608,0.0,7.5e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,1.0,4.5e-05,0.0,6e-05
arc-challenge.val.125,meta/llama-2-70b-chat,1.0,5.8500000000000006e-05,0.0,1.95e-05,1.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.7e-05,1.0,0.00069,0.0,5.044e-05,1.0,5.8500000000000006e-05,0.0,1.3e-05,1.0,3.9e-05,1.0,5.12e-05
mmlu-professional-psychology.val.581,meta/llama-2-70b-chat,0.0,0.0001017,0.0,3.39e-05,1.0,9.120000000000002e-05,1.0,0.000912,1.0,0.000912,1.0,0.000113,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,0.0,2.2600000000000004e-05,0.0,6.78e-05,0.0,8.960000000000002e-05
hellaswag.val.9534,claude-v1,1.0,0.001896,1.0,7.08e-05,1.0,0.0001896,1.0,0.001896,1.0,0.001896,1.0,0.0002379999999999,1.0,0.00237,0.0,0.000183136,1.0,0.0002115,0.0,4.720000000000001e-05,0.0,0.0001416,1.0,0.000188
mmlu-human-aging.val.140,meta/llama-2-70b-chat,0.0,8.01e-05,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
grade-school-math.dev.3972,gpt-4-1106-preview,0.5,0.00593,0.5,0.0001392,0.75,0.0003952,0.75,0.00472,0.5,0.004912,0.5,0.00041,0.5,0.00593,0.25,0.000303416,0.5,0.0003429,0.5,7.840000000000001e-05,0.5,0.0002766,0.75,0.000324
mmlu-professional-law.val.1172,meta/llama-2-70b-chat,0.0,6.840000000000001e-05,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,0.0,4.56e-05,0.0,6e-05
grade-school-math.dev.1844,meta/llama-2-70b-chat,0.25,0.0003429,0.75,0.0001305,0.75,0.000488,0.75,0.0042559999999999,0.75,0.005216,0.75,0.000452,0.75,0.00673,0.25,0.000285568,0.25,0.0003429,0.25,7.56e-05,0.75,0.0002219999999999,0.25,0.0002312
hellaswag.val.2702,claude-v1,1.0,0.000912,0.0,3.39e-05,1.0,9.120000000000002e-05,1.0,0.000912,0.0,0.000912,1.0,0.000115,1.0,0.00114,1.0,8.768799999999999e-05,0.0,0.0001008,1.0,2.2600000000000004e-05,1.0,6.78e-05,0.0,8.960000000000002e-05
grade-school-math.dev.6893,gpt-4-1106-preview,0.75,0.0066599999999999,0.75,0.0001413,0.75,0.0005136,0.75,0.0042,0.75,0.004392,0.75,0.000567,0.75,0.0066599999999999,0.75,0.000334456,0.75,0.0003519,0.25,5.58e-05,0.25,0.000195,0.75,0.0003064
hellaswag.val.2730,meta/llama-2-70b-chat,1.0,0.0001278,1.0,4.29e-05,1.0,0.0001152,0.0,0.001152,1.0,0.001152,1.0,0.000145,1.0,0.00144,0.0,0.000110968,1.0,0.0001278,0.0,2.8600000000000004e-05,0.0,8.58e-05,1.0,0.0001136
grade-school-math.dev.4709,gpt-4-1106-preview,0.5,0.00734,0.75,0.0001305,0.5,0.000532,0.75,0.004264,0.5,0.005416,0.75,0.000461,0.5,0.00734,0.75,0.000291,0.5,0.0003627,0.25,8.280000000000001e-05,0.75,0.0002387999999999,0.5,0.0003512
mmlu-elementary-mathematics.val.360,claude-v2,0.0,0.000736,0.0,2.73e-05,0.0,7.360000000000001e-05,0.0,0.000736,0.0,0.000736,1.0,9.1e-05,0.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,1.0,1.82e-05,0.0,5.46e-05,0.0,7.200000000000002e-05
mmlu-human-sexuality.val.63,gpt-4-1106-preview,1.0,0.00109,1.0,3.24e-05,0.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
mmlu-college-medicine.val.154,meta/llama-2-70b-chat,0.0,7.2e-05,1.0,2.4e-05,0.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,0.0,7.999999999999999e-05,1.0,0.00084,0.0,6.208e-05,0.0,7.2e-05,1.0,1.6000000000000003e-05,0.0,4.8e-05,1.0,6.320000000000002e-05
grade-school-math.dev.6302,meta/llama-2-70b-chat,0.75,0.0003438,0.25,0.0001944,0.75,0.0005552,0.75,0.004064,0.75,0.005672,0.75,0.000569,0.75,0.00748,0.25,0.000354632,0.75,0.0003438,0.25,9.48e-05,0.25,0.0002718,0.75,0.0004024
grade-school-math.dev.2020,gpt-4-1106-preview,0.5,0.00764,0.25,0.0001281,0.75,0.0004623999999999,0.75,0.004552,0.75,0.00532,0.75,0.000488,0.5,0.00764,0.25,0.000222712,0.25,0.0003429,0.75,8.5e-05,0.5,0.0002573999999999,0.75,0.000356
mmlu-high-school-world-history.val.163,gpt-4-1106-preview,1.0,0.00454,1.0,0.0001358999999999,1.0,0.0003632,1.0,0.003632,1.0,0.003632,1.0,0.000453,1.0,0.00454,0.0,0.000351528,0.0,0.0004077,1.0,9.06e-05,1.0,0.0002717999999999,1.0,0.0003616
mmlu-econometrics.val.76,meta/llama-2-70b-chat,0.0,0.0001845,1.0,6.149999999999999e-05,1.0,0.0001648,1.0,0.001648,1.0,0.001648,0.0,0.000205,1.0,0.00206,0.0,0.0001583039999999,0.0,0.0001845,0.0,4.100000000000001e-05,1.0,0.0001229999999999,1.0,0.0001632
consensus_summary.dev.13,gpt-4-1106-preview,0.75,0.00313,0.75,8.549999999999999e-05,0.75,0.0003848,1.0,0.001496,0.75,0.003488,0.75,0.000345,0.75,0.00313,0.75,0.00020564,0.75,0.0003149999999999,0.75,5.98e-05,0.75,0.000168,0.75,0.0001904
hellaswag.val.10012,claude-v1,1.0,0.002344,1.0,8.73e-05,0.0,0.0002368,1.0,0.002344,1.0,0.002344,0.0,0.000294,1.0,0.00296,1.0,0.000226592,1.0,0.0002628,1.0,5.84e-05,1.0,0.0001752,1.0,0.0002328
arc-challenge.val.282,meta/llama-2-70b-chat,1.0,9e-05,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,1.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
mmlu-professional-law.val.93,meta/llama-2-70b-chat,0.0,7.65e-05,0.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
mmlu-world-religions.val.11,meta/llama-2-70b-chat,0.0,6.48e-05,0.0,2.16e-05,0.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
winogrande.dev.742,claude-v1,1.0,0.000392,1.0,1.4399999999999998e-05,1.0,3.92e-05,1.0,0.000392,0.0,0.000392,0.0,5e-05,1.0,0.00052,0.0,3.7248e-05,0.0,4.32e-05,1.0,9.6e-06,1.0,2.8799999999999995e-05,1.0,3.7600000000000006e-05
mmlu-electrical-engineering.val.135,WizardLM/WizardLM-13B-V1.2,0.0,2.46e-05,0.0,2.46e-05,0.0,6.64e-05,0.0,0.000664,0.0,0.000664,0.0,8.2e-05,1.0,0.00083,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,0.0,4.86e-05,1.0,6.56e-05
mmlu-elementary-mathematics.val.233,gpt-4-1106-preview,0.0,0.00108,0.0,3.21e-05,0.0,8.64e-05,0.0,0.000864,0.0,0.000864,0.0,0.000109,0.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,0.0,6.42e-05,0.0,8.56e-05
mmlu-world-religions.val.44,meta/llama-2-70b-chat,0.0,8.280000000000001e-05,0.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,0.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
consensus_summary.dev.292,meta/llama-2-70b-chat,0.75,0.0003807,0.75,0.0001010999999999,1.0,0.0002896,0.25,0.002008,0.75,0.004,0.25,0.000249,0.0,0.00251,0.75,0.000270824,0.75,0.0003807,0.75,7.14e-05,0.75,0.0002166,0.75,0.0002392
mmlu-clinical-knowledge.val.142,meta/llama-2-70b-chat,0.0,8.280000000000001e-05,0.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,0.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
mmlu-high-school-geography.val.73,meta/llama-2-70b-chat,0.0,7.65e-05,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,0.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.8e-05
hellaswag.val.5893,claude-v1,0.0,0.002224,0.0,8.28e-05,0.0,0.0002224,0.0,0.002224,1.0,0.002224,1.0,0.000279,0.0,0.00281,0.0,0.000214952,1.0,0.0002484,0.0,5.5400000000000005e-05,0.0,0.0001662,1.0,0.0002208
mmlu-professional-law.val.1110,WizardLM/WizardLM-13B-V1.2,0.0,9.42e-05,0.0,9.42e-05,0.0,0.000252,0.0,0.00252,0.0,0.00252,0.0,0.000314,0.0,0.00315,0.0,0.000243664,0.0,0.0002826,0.0,6.280000000000001e-05,0.0,0.0001884,0.0,0.0002504
mmlu-business-ethics.val.77,meta/llama-2-70b-chat,0.0,7.11e-05,1.0,2.37e-05,0.0,6.400000000000001e-05,1.0,0.00064,1.0,0.00064,1.0,7.9e-05,1.0,0.0007999999999999,0.0,6.1304e-05,0.0,7.11e-05,0.0,1.58e-05,1.0,4.74e-05,1.0,6.240000000000001e-05
mmlu-professional-accounting.val.138,gpt-4-1106-preview,1.0,0.00108,0.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,0.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
grade-school-math.dev.4374,gpt-4-1106-preview,0.75,0.00732,0.75,0.0001391999999999,0.75,0.0006456,0.75,0.005136,0.75,0.0054719999999999,0.75,0.0004759999999999,0.75,0.00732,0.25,0.000332128,0.75,0.0003726,0.25,0.0005434,0.75,0.0002417999999999,0.75,0.0003664
mmlu-professional-law.val.1169,gpt-4-1106-preview,1.0,0.00327,0.0,9.78e-05,1.0,0.0002616,1.0,0.002616,1.0,0.002616,0.0,0.000326,1.0,0.00327,0.0,0.0002529759999999,0.0,0.0002934,0.0,6.52e-05,1.0,0.0001956,0.0,0.00026
mmlu-professional-law.val.278,gpt-4-1106-preview,0.0,0.00253,0.0,7.56e-05,1.0,0.0002024,1.0,0.002024,0.0,0.002024,0.0,0.000252,0.0,0.00253,0.0,0.000195552,0.0,0.0002267999999999,0.0,5.0400000000000005e-05,0.0,0.0001512,0.0,0.0002016
mmlu-high-school-psychology.val.475,claude-v1,1.0,0.00072,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.00093,0.0,6.9064e-05,0.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
bias_detection.dev.62,meta/llama-2-70b-chat,0.0,0.0003483,0.0,0.0001049999999999,1.0,0.0003775999999999,1.0,0.002768,1.0,0.005336,1.0,0.000318,1.0,0.00631,0.0,0.000259184,0.0,0.0003483,0.0,6.24e-05,1.0,0.0002165999999999,0.0,0.0002176
arc-challenge.test.667,claude-v1,1.0,0.000632,1.0,2.3400000000000003e-05,0.0,6.32e-05,1.0,0.000632,0.0,0.000632,1.0,8e-05,1.0,0.00082,1.0,6.0528e-05,0.0,7.020000000000001e-05,1.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,0.0,6.16e-05
accounting_audit.dev.6,gpt-4-1106-preview,1.0,0.00142,0.0,4.41e-05,1.0,0.0001136,1.0,0.0011359999999999,0.0,0.0011359999999999,1.0,0.0001409999999999,1.0,0.00142,0.0,0.000114072,0.0,0.0001323,0.0,2.9400000000000003e-05,0.0,8.819999999999999e-05,0.0,0.0001128
hellaswag.val.7030,claude-v1,1.0,0.00208,0.0,7.769999999999999e-05,1.0,0.000208,1.0,0.00208,1.0,0.00208,0.0,0.000261,1.0,0.0026,0.0,0.000200984,1.0,0.0002331,0.0,5.1800000000000005e-05,0.0,0.0001553999999999,1.0,0.0002064
hellaswag.val.8370,claude-v1,1.0,0.002272,1.0,8.49e-05,0.0,0.0002272,1.0,0.002272,1.0,0.002272,0.0,0.000283,1.0,0.00284,1.0,0.000219608,1.0,0.0002547,1.0,5.660000000000001e-05,1.0,0.0001698,1.0,0.0002256
mmlu-international-law.val.119,meta/llama-2-70b-chat,0.0,0.0001197,1.0,3.99e-05,1.0,0.0001072,1.0,0.001072,1.0,0.001072,1.0,0.000133,1.0,0.00134,0.0,0.000103208,0.0,0.0001197,0.0,2.6600000000000003e-05,1.0,7.98e-05,1.0,0.0001064
arc-challenge.test.602,gpt-4-1106-preview,1.0,0.00089,0.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,0.0,9e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
mmlu-professional-law.val.498,meta/llama-2-70b-chat,0.0,0.0001853999999999,0.0,6.18e-05,1.0,0.0001656,1.0,0.001656,1.0,0.001656,1.0,0.000206,1.0,0.00207,0.0,0.000159856,0.0,0.0001853999999999,0.0,4.1200000000000005e-05,1.0,0.0001236,0.0,0.000164
mmlu-professional-law.val.1023,claude-v1,0.0,0.003992,0.0,0.0001493999999999,0.0,0.0003992,0.0,0.003992,0.0,0.003992,0.0,0.000498,0.0,0.00499,0.0,0.000386448,0.0,0.0004482,0.0,9.96e-05,0.0,0.0002982,1.0,0.0003984
hellaswag.val.3857,gpt-4-1106-preview,1.0,0.00225,0.0,6.599999999999999e-05,1.0,0.00018,0.0,0.001776,0.0,0.001776,1.0,0.000223,1.0,0.00225,0.0,0.000171496,1.0,0.000198,0.0,4.420000000000001e-05,0.0,0.0001326,0.0,0.000176
hellaswag.val.1881,mistralai/mistral-7b-chat,0.0,3.160000000000001e-05,0.0,4.74e-05,1.0,0.0001272,0.0,0.001272,0.0,0.001296,0.0,0.000158,1.0,0.00159,0.0,0.000122608,0.0,0.0001413,0.0,3.160000000000001e-05,1.0,9.48e-05,1.0,0.0001256
mmlu-college-chemistry.val.80,meta/llama-2-70b-chat,0.0,6.93e-05,0.0,2.31e-05,0.0,6.24e-05,0.0,0.000624,0.0,0.000624,0.0,7.699999999999999e-05,0.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,0.0,4.6200000000000005e-05,0.0,6.16e-05
hellaswag.val.325,meta/llama-2-70b-chat,0.0,0.0001026,0.0,3.45e-05,0.0,9.28e-05,0.0,0.000928,0.0,0.000928,1.0,0.0001149999999999,0.0,0.00116,1.0,8.924e-05,0.0,0.0001026,1.0,2.3e-05,0.0,6.9e-05,0.0,9.12e-05
mmlu-professional-law.val.1113,WizardLM/WizardLM-13B-V1.2,0.0,6.24e-05,0.0,6.24e-05,0.0,0.0001672,0.0,0.0016719999999999,0.0,0.0016719999999999,1.0,0.000208,1.0,0.00209,0.0,0.0001614079999999,0.0,0.0001872,1.0,4.160000000000001e-05,1.0,0.0001248,0.0,0.0001664
mmlu-prehistory.val.36,claude-v1,1.0,0.000784,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00101,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.76e-05
grade-school-math.dev.2065,meta/llama-2-70b-chat,0.25,0.0003663,0.5,0.0001407,0.25,0.0005024,0.25,0.005024,0.75,0.00596,0.75,0.0004849999999999,0.75,0.00739,0.25,0.000317384,0.25,0.0003663,0.5,8.2e-05,0.25,0.0002339999999999,0.75,0.0004016
grade-school-math.dev.7399,meta/llama-2-70b-chat,0.5,0.0003861,0.5,0.0001743,0.5,0.0005808,0.75,0.005832,0.5,0.0060479999999999,0.75,0.000507,0.5,0.00813,0.25,0.000417488,0.5,0.0003861,0.25,8.94e-05,0.25,0.0002676,0.25,0.0004056
mmlu-high-school-us-history.val.104,meta/llama-2-70b-chat,0.0,0.0002241,1.0,7.47e-05,1.0,0.0002,0.0,0.002,0.0,0.002,1.0,0.000249,1.0,0.00253,0.0,0.0001932239999999,0.0,0.0002241,1.0,4.980000000000001e-05,1.0,0.0001494,1.0,0.0001984
mmlu-nutrition.val.45,WizardLM/WizardLM-13B-V1.2,0.0,4.77e-05,0.0,4.77e-05,1.0,0.000128,1.0,0.0012799999999999,1.0,0.0012799999999999,1.0,0.000159,1.0,0.0016,0.0,0.000123384,0.0,0.0001431,0.0,3.180000000000001e-05,1.0,9.54e-05,1.0,0.0001272
arc-challenge.test.986,meta/llama-2-70b-chat,1.0,8.91e-05,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,0.000101,1.0,0.00103,0.0,7.682400000000001e-05,1.0,8.91e-05,0.0,1.98e-05,1.0,5.94e-05,1.0,7.840000000000001e-05
mmlu-professional-law.val.846,gpt-4-1106-preview,0.0,0.00418,0.0,0.0001250999999999,0.0,0.0003344,0.0,0.003344,0.0,0.003344,0.0,0.000417,0.0,0.00418,0.0,0.000323592,0.0,0.0003753,0.0,8.34e-05,0.0,0.0002501999999999,0.0,0.0003328
mmlu-anatomy.val.29,meta/llama-2-70b-chat,0.0,0.0001413,0.0,4.71e-05,0.0,0.0001264,0.0,0.001264,1.0,0.001264,0.0,0.000157,1.0,0.00158,0.0,0.000121832,0.0,0.0001413,0.0,3.1400000000000004e-05,0.0,9.42e-05,0.0,0.0001256
mmlu-professional-law.val.1523,claude-v1,1.0,0.003736,1.0,0.0001397999999999,1.0,0.0003736,1.0,0.003736,0.0,0.003736,1.0,0.000466,0.0,0.00467,0.0,0.000361616,0.0,0.0004194,0.0,9.32e-05,1.0,0.0002795999999999,1.0,0.0003728
mmlu-moral-scenarios.val.785,WizardLM/WizardLM-13B-V1.2,0.0,4.2e-05,0.0,4.2e-05,1.0,0.0001128,0.0,0.0011279999999999,0.0,0.0011279999999999,0.0,0.00014,1.0,0.00141,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,0.0,8.4e-05,1.0,0.0001112
grade-school-math.dev.2940,gpt-4-1106-preview,0.75,0.01266,0.75,0.0002022,0.75,0.0006696,0.25,0.006576,0.75,0.0072239999999999,0.75,0.000817,0.75,0.01266,0.25,0.000404296,0.25,0.0004086,0.75,0.0001096,0.75,0.0002898,0.75,0.0005824
hellaswag.val.3806,claude-v1,0.0,0.002176,0.0,8.13e-05,0.0,0.0002176,0.0,0.002176,0.0,0.002176,1.0,0.0002729999999999,0.0,0.00275,0.0,0.0002102959999999,1.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-professional-law.val.192,claude-v1,1.0,0.002344,0.0,8.76e-05,1.0,0.0002344,1.0,0.002344,0.0,0.002344,1.0,0.000292,1.0,0.00293,0.0,0.000226592,0.0,0.0002628,0.0,5.84e-05,1.0,0.0001752,1.0,0.0002328
mmlu-management.val.57,gpt-4-1106-preview,0.0,0.00102,0.0,2.94e-05,0.0,7.920000000000001e-05,1.0,0.000792,0.0,0.000792,1.0,9.8e-05,0.0,0.00102,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,0.0,5.88e-05,0.0,7.840000000000001e-05
mmlu-human-sexuality.val.62,WizardLM/WizardLM-13B-V1.2,0.0,3e-05,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,0.0,0.0001,0.0,0.00104,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,0.0,6e-05,1.0,8e-05
mmlu-high-school-psychology.val.444,meta/llama-2-70b-chat,1.0,7.2e-05,0.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,0.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,1.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
mmlu-moral-scenarios.val.362,claude-v1,0.0,0.0011279999999999,0.0,4.2e-05,0.0,0.0001128,0.0,0.0011279999999999,0.0,0.0011279999999999,0.0,0.0001419999999999,1.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,0.0,8.4e-05,0.0,0.0001112
mmlu-professional-law.val.997,meta/llama-2-70b-chat,0.0,0.0003069,0.0,0.0001023,1.0,0.0002736,1.0,0.002736,1.0,0.002736,0.0,0.000341,1.0,0.00342,0.0,0.000264616,0.0,0.0003069,0.0,6.819999999999999e-05,0.0,0.0002046,1.0,0.000272
grade-school-math.dev.7413,gpt-4-1106-preview,0.75,0.00844,0.25,0.0001407,0.75,0.0005912,0.75,0.00524,0.75,0.00536,0.75,0.000626,0.75,0.00844,0.25,0.000270824,0.25,0.0004032,0.75,9.7e-05,0.5,0.0002885999999999,0.75,0.0003511999999999
hellaswag.val.7488,claude-v1,1.0,0.002528,0.0,9.42e-05,1.0,0.0002528,1.0,0.002528,1.0,0.002528,0.0,0.000315,1.0,0.00316,0.0,0.00024444,1.0,0.0002834999999999,0.0,6.3e-05,0.0,0.0001889999999999,0.0,0.0002512
grade-school-math.dev.4320,gpt-4-1106-preview,0.5,0.01249,0.25,0.0001512,0.75,0.0006704,0.5,0.005216,0.5,0.008744,0.5,0.0006969999999999,0.5,0.01249,0.75,0.000398864,0.75,0.0003807,0.25,0.0001124,0.5,0.0003149999999999,0.5,0.0004768
mmlu-philosophy.val.96,gpt-4-1106-preview,0.0,0.00067,0.0,1.98e-05,1.0,5.36e-05,0.0,0.000536,1.0,0.000536,0.0,6.599999999999999e-05,0.0,0.00067,0.0,5.1216000000000006e-05,0.0,5.940000000000001e-05,0.0,1.32e-05,0.0,3.96e-05,0.0,5.28e-05
grade-school-math.dev.1249,gpt-4-1106-preview,0.75,0.00909,0.5,0.0002103,0.75,0.0004944,0.25,0.005208,0.75,0.005952,0.25,0.000567,0.75,0.00909,0.25,0.000270048,0.25,0.0004068,0.25,0.000103,0.25,0.0002982,0.5,0.00046
mmlu-miscellaneous.val.515,WizardLM/WizardLM-13B-V1.2,1.0,3.3600000000000004e-05,1.0,3.3600000000000004e-05,1.0,9.04e-05,1.0,0.000904,1.0,0.000904,1.0,0.000112,1.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,1.0,2.24e-05,1.0,6.720000000000001e-05,1.0,8.96e-05
hellaswag.val.3235,meta/llama-2-70b-chat,0.0,9.18e-05,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000105,1.0,0.00107,1.0,7.992800000000001e-05,0.0,9.18e-05,1.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
mmlu-professional-law.val.1344,meta/llama-2-70b-chat,0.0,0.0005238,0.0,0.0001746,0.0,0.0004664,0.0,0.004664,0.0,0.004664,1.0,0.000582,1.0,0.0058299999999999,0.0,0.000451632,0.0,0.0005238,1.0,0.0001164,1.0,0.0003492,0.0,0.0004656
arc-challenge.val.249,claude-v1,1.0,0.001024,1.0,3.81e-05,0.0,0.0001024,1.0,0.001024,1.0,0.001024,1.0,0.000129,1.0,0.00128,0.0,9.8552e-05,1.0,0.0001143,1.0,2.54e-05,1.0,7.62e-05,1.0,0.0001008
grade-school-math.dev.5107,gpt-4-1106-preview,0.75,0.0075,0.25,0.0001683,0.75,0.0005376,0.75,0.004248,0.75,0.005112,0.5,0.000511,0.75,0.0075,0.5,0.000323592,0.25,0.00036,0.75,7.76e-05,0.75,0.0002514,0.75,0.0004088
hellaswag.val.9635,claude-v1,1.0,0.002024,1.0,7.56e-05,0.0,0.0002024,1.0,0.002024,0.0,0.002024,0.0,0.000252,1.0,0.00256,1.0,0.000195552,1.0,0.0002259,1.0,5.0400000000000005e-05,1.0,0.0001512,1.0,0.0002008
mmlu-econometrics.val.62,meta/code-llama-instruct-34b-chat,0.0,0.000140456,0.0,5.46e-05,0.0,0.0001464,1.0,0.001464,1.0,0.001464,1.0,0.000182,1.0,0.00183,0.0,0.000140456,0.0,0.0001638,0.0,3.62e-05,1.0,0.0001092,0.0,0.0001448
mmlu-professional-law.val.618,gpt-4-1106-preview,0.0,0.00358,1.0,0.0001061999999999,0.0,0.000284,0.0,0.00284,0.0,0.00284,0.0,0.000354,0.0,0.00358,0.0,0.0002747039999999,0.0,0.0003186,0.0,7.08e-05,0.0,0.0002123999999999,1.0,0.0002824
mmlu-moral-scenarios.val.674,claude-v1,0.0,0.001088,0.0,4.05e-05,0.0,0.0001088,0.0,0.001088,0.0,0.001088,0.0,0.000135,1.0,0.00136,0.0,0.0001047599999999,0.0,0.0001215,0.0,2.7e-05,0.0,8.1e-05,0.0,0.0001072
mmlu-professional-law.val.708,gpt-4-1106-preview,1.0,0.00261,0.0,7.8e-05,1.0,0.0002088,0.0,0.002088,1.0,0.002088,0.0,0.00026,1.0,0.00261,0.0,0.00020176,0.0,0.000234,0.0,5.2e-05,0.0,0.0001553999999999,0.0,0.000208
hellaswag.val.7853,meta/code-llama-instruct-34b-chat,0.0,0.0002234879999999,0.0,8.609999999999999e-05,1.0,0.0002336,1.0,0.002312,1.0,0.002312,1.0,0.00029,1.0,0.00292,0.0,0.0002234879999999,1.0,0.0002583,0.0,5.76e-05,0.0,0.0001728,1.0,0.0002296
mmlu-security-studies.val.98,gpt-4-1106-preview,1.0,0.00271,0.0,8.099999999999999e-05,1.0,0.0002168,1.0,0.002168,1.0,0.002168,1.0,0.00027,1.0,0.00271,0.0,0.00020952,0.0,0.000243,0.0,5.4000000000000005e-05,1.0,0.0001619999999999,1.0,0.0002152
hellaswag.val.9812,meta/llama-2-70b-chat,1.0,0.0002177999999999,0.0,7.26e-05,1.0,0.0001944,0.0,0.001944,1.0,0.001944,0.0,0.000244,1.0,0.00243,0.0,0.000187792,1.0,0.0002177999999999,0.0,4.84e-05,1.0,0.0001452,1.0,0.0001928
mmlu-clinical-knowledge.val.137,gpt-4-1106-preview,1.0,0.00096,1.0,2.85e-05,0.0,7.680000000000001e-05,0.0,0.000768,0.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,1.0,1.9e-05,1.0,5.7e-05,1.0,7.6e-05
grade-school-math.dev.4419,gpt-4-1106-preview,0.75,0.01116,0.75,0.0001944,0.75,0.0007199999999999,0.25,0.00516,0.75,0.006192,0.75,0.0006169999999999,0.75,0.01116,0.25,0.00038024,0.25,0.0004059,0.25,0.0001076,0.25,0.0002904,0.75,0.0004696
mmlu-high-school-microeconomics.val.171,gpt-4-1106-preview,1.0,0.00108,0.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,0.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
grade-school-math.dev.564,gpt-4-1106-preview,0.75,0.00804,0.25,0.0001329,0.75,0.0005135999999999,0.5,0.003936,0.5,0.004344,0.75,0.0003779999999999,0.75,0.00804,0.75,0.000256856,0.75,0.0003411,0.5,6.94e-05,0.75,0.0002628,0.75,0.0003016
hellaswag.val.7887,claude-v1,0.0,0.002144,0.0,8.01e-05,1.0,0.0002144,0.0,0.002144,1.0,0.002144,0.0,0.000269,1.0,0.00268,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,1.0,0.0002128
mmlu-professional-law.val.1526,gpt-4-1106-preview,0.0,0.00255,0.0,7.62e-05,0.0,0.000204,0.0,0.00204,0.0,0.00204,0.0,0.000254,0.0,0.00255,0.0,0.0001971039999999,0.0,0.0002286,1.0,5.080000000000001e-05,0.0,0.0001524,0.0,0.0002024
hellaswag.val.278,mistralai/mistral-7b-chat,0.0,2.2e-05,1.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,1.0,9.9e-05,0.0,2.2e-05,1.0,6.6e-05,1.0,8.72e-05
grade-school-math.dev.2210,gpt-4-1106-preview,0.75,0.00718,0.75,0.0001329,0.25,0.0005288,0.75,0.004208,0.75,0.0046879999999999,0.25,0.000407,0.75,0.00718,0.75,0.000356184,0.75,0.0004041,0.25,0.0001028,0.25,0.0002334,0.75,0.0003368
mbpp.dev.175,gpt-4-1106-preview,1.0,0.0137899999999999,0.0,0.0001365,1.0,0.0005584,1.0,0.005128,1.0,0.005368,1.0,0.000565,1.0,0.0137899999999999,1.0,0.000241336,0.0,0.0003933,0.0,6.48e-05,0.0,0.0002538,0.0,0.0002968
grade-school-math.dev.3037,gpt-4-1106-preview,0.75,0.005,0.75,0.0001386,0.75,0.0004936,0.75,0.003544,0.75,0.005392,0.75,0.000388,0.75,0.005,0.75,0.000300312,0.75,0.0003303,0.75,7.640000000000001e-05,0.75,0.0002406,0.75,0.0003728
hellaswag.val.4522,claude-v1,0.0,0.0019199999999999,1.0,7.17e-05,1.0,0.000192,0.0,0.0019199999999999,0.0,0.0019199999999999,0.0,0.0002409999999999,1.0,0.00243,1.0,0.0001854639999999,0.0,0.0002142,1.0,4.780000000000001e-05,1.0,0.0001434,1.0,0.0001904
hellaswag.val.7739,meta/llama-2-70b-chat,1.0,0.0002421,0.0,8.099999999999999e-05,1.0,0.0002168,1.0,0.002168,1.0,0.002168,1.0,0.000272,1.0,0.00271,0.0,0.00020952,1.0,0.0002421,0.0,5.4000000000000005e-05,0.0,0.0001619999999999,1.0,0.0002152
mmlu-anatomy.val.26,meta/llama-2-70b-chat,1.0,8.1e-05,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,1.0,8.1e-05,1.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
mmlu-professional-law.val.406,gpt-4-1106-preview,0.0,0.00265,0.0,7.92e-05,0.0,0.000212,0.0,0.00212,0.0,0.00212,0.0,0.000264,0.0,0.00265,0.0,0.000204864,0.0,0.0002376,1.0,5.280000000000001e-05,0.0,0.0001584,0.0,0.0002104
mmlu-professional-accounting.val.45,claude-v1,1.0,0.00084,0.0,3.12e-05,0.0,8.400000000000001e-05,1.0,0.00084,1.0,0.00084,0.0,0.000106,0.0,0.00108,0.0,8.0704e-05,0.0,9.36e-05,0.0,2.08e-05,1.0,6.24e-05,0.0,8.240000000000001e-05
grade-school-math.dev.1740,gpt-4-1106-preview,0.75,0.00638,0.5,0.0001401,0.75,0.0004792,0.75,0.004024,0.75,0.00568,0.5,0.000368,0.75,0.00638,0.75,0.000270048,0.75,0.0003006,0.75,6.38e-05,0.25,0.0002399999999999,0.75,0.0003488
grade-school-math.dev.416,gpt-4-1106-preview,0.75,0.0075699999999999,0.75,0.00015,0.75,0.0005312,0.75,0.004328,0.75,0.005768,0.75,0.000396,0.75,0.0075699999999999,0.75,0.000305744,0.75,0.0003509999999999,0.75,9.48e-05,0.25,0.0002375999999999,0.25,0.0002392
mmlu-abstract-algebra.val.48,gpt-4-1106-preview,1.0,0.00129,0.0,3.75e-05,0.0,0.0001008,1.0,0.001008,0.0,0.001008,0.0,0.000125,1.0,0.00129,0.0,9.7e-05,0.0,0.0001125,0.0,2.5e-05,0.0,7.439999999999999e-05,0.0,0.0001
mmlu-professional-law.val.107,gpt-4-1106-preview,1.0,0.00164,0.0,4.89e-05,1.0,0.0001312,1.0,0.001312,1.0,0.001312,1.0,0.000165,1.0,0.00164,0.0,0.0001264879999999,0.0,0.0001467,0.0,3.2600000000000006e-05,1.0,9.78e-05,1.0,0.0001296
mmlu-high-school-macroeconomics.val.109,gpt-4-1106-preview,1.0,0.0009299999999999,0.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,0.0,8.280000000000001e-05,0.0,1.84e-05,0.0,5.52e-05,1.0,7.36e-05
mmlu-moral-scenarios.val.27,claude-v1,1.0,0.001192,0.0,4.44e-05,1.0,0.0001192,1.0,0.001192,1.0,0.001192,0.0,0.000148,1.0,0.00149,0.0,0.000114848,0.0,0.0001331999999999,0.0,2.96e-05,0.0,8.879999999999999e-05,1.0,0.0001176
mmlu-astronomy.val.17,claude-v1,1.0,0.00112,0.0,4.17e-05,0.0,0.000112,1.0,0.00112,1.0,0.00112,0.0,0.000139,1.0,0.0014,0.0,0.000107864,0.0,0.0001250999999999,0.0,2.78e-05,1.0,8.340000000000001e-05,1.0,0.0001112
grade-school-math.dev.7261,gpt-4-1106-preview,0.5,0.01287,0.25,0.000219,0.75,0.0007584,0.5,0.006648,0.5,0.009432,0.25,0.000707,0.5,0.01287,0.5,0.00048112,0.25,0.0004644,0.25,0.0001028,0.25,0.0003984,0.75,0.0005064
grade-school-math.dev.4133,gpt-4-1106-preview,0.5,0.01062,0.75,0.0001733999999999,0.5,0.0006456,0.75,0.005376,0.5,0.005952,0.5,0.000681,0.5,0.01062,0.25,0.000468704,0.75,0.000414,0.5,9.14e-05,0.5,0.0002783999999999,0.75,0.000428
mmlu-professional-law.val.335,gpt-4-1106-preview,1.0,0.00418,0.0,0.0001250999999999,1.0,0.0003344,0.0,0.003344,1.0,0.003344,0.0,0.000417,1.0,0.00418,0.0,0.000323592,0.0,0.0003753,0.0,8.34e-05,0.0,0.0002501999999999,0.0,0.0003336
grade-school-math.dev.1202,gpt-4-1106-preview,0.75,0.01321,0.25,0.0001994999999999,0.25,0.0007064,0.5,0.007064,0.75,0.009752,0.25,0.000821,0.75,0.01321,0.25,0.0006099359999999,0.25,0.0005607,0.25,0.0001398,0.25,0.0003197999999999,0.25,0.0005224
hellaswag.val.5461,meta/llama-2-70b-chat,0.0,0.0002357999999999,0.0,7.86e-05,1.0,0.0002104,0.0,0.002104,0.0,0.002104,0.0,0.0002619999999999,0.0,0.00266,0.0,0.000203312,0.0,0.0002357999999999,0.0,5.24e-05,0.0,0.0001565999999999,1.0,0.0002087999999999
hellaswag.val.4468,claude-v1,0.0,0.002368,0.0,8.85e-05,0.0,0.0002368,0.0,0.002368,0.0,0.002368,1.0,0.000297,1.0,0.00296,0.0,0.0002289199999999,0.0,0.0002655,0.0,5.9e-05,0.0,0.000177,0.0,0.0002352
consensus_summary.dev.189,meta/llama-2-70b-chat,0.75,0.0002277,1.0,5.58e-05,0.5,0.0001456,0.5,0.001456,0.75,0.002608,0.5,0.0001799999999999,0.5,0.00182,0.75,0.000150544,0.75,0.0002277,0.75,3.88e-05,0.75,0.0001193999999999,0.5,0.0001424
hellaswag.val.883,mistralai/mistral-7b-chat,0.0,2.82e-05,0.0,4.23e-05,1.0,0.0001136,1.0,0.0011359999999999,1.0,0.0011359999999999,1.0,0.0001409999999999,1.0,0.00142,0.0,0.000109416,1.0,0.000126,0.0,2.82e-05,0.0,8.46e-05,1.0,0.000112
mmlu-high-school-chemistry.val.171,gpt-4-1106-preview,0.0,0.00114,0.0,3.39e-05,0.0,9.120000000000002e-05,0.0,0.000912,0.0,0.000912,0.0,0.000113,0.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,0.0,2.2600000000000004e-05,0.0,6.78e-05,0.0,9.040000000000002e-05
arc-challenge.val.72,meta/llama-2-70b-chat,1.0,7.920000000000001e-05,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,1.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
hellaswag.val.3385,claude-v1,0.0,0.00224,0.0,8.34e-05,0.0,0.0002264,0.0,0.00224,0.0,0.00224,0.0,0.000281,0.0,0.00283,0.0,0.0002165039999999,0.0,0.0002511,0.0,5.580000000000001e-05,0.0,0.0001674,1.0,0.0002224
hellaswag.val.293,gpt-4-1106-preview,0.0,0.00122,0.0,3.63e-05,0.0,9.76e-05,0.0,0.000976,0.0,0.000976,0.0,0.000121,0.0,0.00122,1.0,9.3896e-05,0.0,0.000108,1.0,2.42e-05,0.0,7.259999999999999e-05,0.0,9.6e-05
mbpp.dev.110,gpt-4-1106-preview,1.0,0.0054199999999999,1.0,4.3499999999999993e-05,1.0,0.0002319999999999,1.0,0.00184,1.0,0.004504,1.0,0.000196,1.0,0.0054199999999999,1.0,9.7e-05,0.0,0.0002178,1.0,5.14e-05,1.0,0.000129,1.0,0.000124
hellaswag.val.1374,mistralai/mistral-7b-chat,0.0,2.24e-05,1.0,3.3600000000000004e-05,1.0,9.04e-05,1.0,0.000904,1.0,0.000904,1.0,0.000114,1.0,0.00113,0.0,8.6912e-05,1.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,1.0,8.88e-05
mmlu-machine-learning.val.66,gpt-4-1106-preview,1.0,0.00196,0.0,5.76e-05,0.0,0.0001544,1.0,0.001544,1.0,0.001544,1.0,0.000192,1.0,0.00196,0.0,0.0001489919999999,0.0,0.0001728,0.0,3.8400000000000005e-05,1.0,0.0001152,1.0,0.0001528
mmlu-elementary-mathematics.val.292,claude-v1,1.0,0.000832,1.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,0.0,6.18e-05,1.0,8.160000000000002e-05
hellaswag.val.6867,claude-v1,1.0,0.001872,1.0,6.989999999999999e-05,1.0,0.0001872,1.0,0.001872,1.0,0.001872,1.0,0.000235,1.0,0.00234,0.0,0.000180808,1.0,0.0002088,0.0,4.660000000000001e-05,1.0,0.0001397999999999,1.0,0.0001856
mmlu-high-school-european-history.val.63,gpt-4-1106-preview,1.0,0.00351,0.0,0.0001049999999999,1.0,0.0002808,0.0,0.002808,0.0,0.002808,1.0,0.00035,1.0,0.00351,0.0,0.0002716,0.0,0.0003149999999999,0.0,7.000000000000001e-05,0.0,0.0002099999999999,1.0,0.0002792
hellaswag.val.907,meta/llama-2-70b-chat,0.0,0.0001341,1.0,4.47e-05,1.0,0.0001208,1.0,0.0012079999999999,1.0,0.0012079999999999,1.0,0.00015,1.0,0.00151,0.0,0.0001164,0.0,0.0001341,0.0,3e-05,1.0,9e-05,1.0,0.0001192
arc-challenge.test.1064,claude-v1,1.0,0.00088,0.0,3.27e-05,0.0,8.800000000000001e-05,1.0,0.00088,1.0,0.00088,1.0,0.000111,1.0,0.0011,0.0,8.4584e-05,0.0,9.72e-05,0.0,2.18e-05,1.0,6.54e-05,0.0,8.640000000000001e-05
arc-challenge.test.989,meta/llama-2-70b-chat,1.0,7.740000000000001e-05,1.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.8e-05,1.0,0.0009,1.0,6.673599999999999e-05,1.0,7.740000000000001e-05,1.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
hellaswag.val.1289,gpt-4-1106-preview,1.0,0.00099,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,0.0001,1.0,0.00099,0.0,7.604800000000001e-05,1.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
mmlu-miscellaneous.val.139,meta/llama-2-70b-chat,0.0,6.12e-05,0.0,2.04e-05,1.0,5.52e-05,1.0,0.000552,0.0,0.000552,1.0,6.8e-05,0.0,0.00069,0.0,5.2768e-05,0.0,6.12e-05,0.0,1.36e-05,0.0,4.08e-05,0.0,5.44e-05
grade-school-math.dev.5011,gpt-4-1106-preview,0.75,0.0086,0.75,0.0001512,0.75,0.0005968,0.75,0.00484,0.75,0.006712,0.75,0.000559,0.75,0.0086,0.25,0.000355408,0.75,0.0003501,0.75,9.54e-05,0.75,0.0002208,0.75,0.0004064
grade-school-math.dev.71,gpt-4-1106-preview,0.5,0.00641,0.75,0.0001449,0.75,0.000388,0.75,0.004432,0.75,0.005176,0.25,0.000453,0.5,0.00641,0.75,0.000285568,0.75,0.0003699,0.25,6.220000000000001e-05,0.75,0.0002262,0.25,0.00032
mmlu-high-school-chemistry.val.57,gpt-4-1106-preview,1.0,0.00188,1.0,5.61e-05,0.0,0.0001504,0.0,0.001504,1.0,0.001504,0.0,0.000187,1.0,0.00188,0.0,0.000145112,0.0,0.0001683,0.0,3.74e-05,0.0,0.0001122,0.0,0.0001496
hellaswag.val.8945,meta/code-llama-instruct-34b-chat,0.0,0.0002203839999999,0.0,8.52e-05,1.0,0.000228,1.0,0.00228,1.0,0.00228,0.0,0.000286,1.0,0.00288,0.0,0.0002203839999999,1.0,0.0002547,0.0,5.680000000000001e-05,0.0,0.0001704,1.0,0.0002264
arc-challenge.test.1065,meta/llama-2-70b-chat,1.0,8.46e-05,1.0,2.82e-05,1.0,7.600000000000002e-05,1.0,0.00076,1.0,0.00076,1.0,9.4e-05,1.0,0.00095,1.0,7.2944e-05,1.0,8.46e-05,0.0,1.8800000000000003e-05,1.0,5.64e-05,1.0,7.440000000000002e-05
mmlu-astronomy.val.100,gpt-4-1106-preview,1.0,0.0011799999999999,1.0,3.4200000000000005e-05,0.0,9.2e-05,0.0,0.00092,1.0,0.00092,1.0,0.0001139999999999,1.0,0.0011799999999999,0.0,8.846400000000001e-05,0.0,0.0001026,1.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.12e-05
hellaswag.val.7804,claude-v1,0.0,0.002232,0.0,8.34e-05,0.0,0.0002232,0.0,0.002232,0.0,0.002232,0.0,0.00028,0.0,0.00282,0.0,0.000215728,0.0,0.0002493,0.0,5.56e-05,0.0,0.0001668,0.0,0.0002216
mmlu-high-school-mathematics.val.171,claude-v1,0.0,0.000984,0.0,3.66e-05,0.0,9.84e-05,0.0,0.000984,0.0,0.000984,1.0,0.000124,1.0,0.00123,0.0,9.4672e-05,0.0,0.0001098,0.0,2.44e-05,0.0,7.32e-05,0.0,9.76e-05
hellaswag.val.2593,mistralai/mistral-7b-chat,0.0,2.12e-05,0.0,3.18e-05,1.0,8.560000000000001e-05,1.0,0.000856,1.0,0.000856,0.0,0.0001059999999999,1.0,0.0010999999999999,0.0,8.2256e-05,0.0,9.45e-05,0.0,2.12e-05,1.0,6.36e-05,1.0,8.400000000000001e-05
hellaswag.val.7573,mistralai/mistral-7b-chat,0.0,4.74e-05,0.0,7.110000000000001e-05,0.0,0.0001928,0.0,0.0019039999999999,0.0,0.001928,1.0,0.000237,1.0,0.00238,0.0,0.000183912,0.0,0.0002123999999999,0.0,4.74e-05,0.0,0.0001422,1.0,0.0001887999999999
mmlu-high-school-macroeconomics.val.329,gpt-4-1106-preview,1.0,0.001,0.0,2.97e-05,1.0,8e-05,1.0,0.0008,0.0,0.0008,0.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,0.0,1.98e-05,0.0,5.94e-05,1.0,7.92e-05
grade-school-math.dev.1927,gpt-4-1106-preview,0.5,0.01402,0.25,0.0002016,0.25,0.0008,0.5,0.00764,0.5,0.009248,0.5,0.000732,0.5,0.01402,0.25,0.00041128,0.75,0.0005364,0.25,9.44e-05,0.25,0.0003624,0.75,0.000576
grade-school-math.dev.1531,gpt-4-1106-preview,0.75,0.01302,0.25,0.0001853999999999,0.25,0.000804,0.25,0.006168,0.25,0.0079919999999999,0.25,0.000851,0.75,0.01302,0.25,0.000332904,0.25,0.0004851,0.25,8e-05,0.25,0.0003773999999999,0.25,0.000456
mmlu-high-school-psychology.val.358,WizardLM/WizardLM-13B-V1.2,1.0,3.21e-05,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,1.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
winogrande.dev.1241,claude-v1,1.0,0.000424,0.0,1.56e-05,1.0,4.24e-05,1.0,0.000424,1.0,0.000424,1.0,5.4000000000000005e-05,1.0,0.00056,1.0,4.0352e-05,0.0,4.6800000000000006e-05,1.0,1.04e-05,1.0,3.12e-05,0.0,4.16e-05
grade-school-math.dev.2865,gpt-4-1106-preview,0.5,0.00589,0.75,0.0001521,0.75,0.0005264,0.75,0.004208,0.75,0.005624,0.75,0.000466,0.5,0.00589,0.75,0.0003049679999999,0.75,0.0003564,0.25,8.26e-05,0.75,0.0002232,0.75,0.0003776
mmlu-college-medicine.val.100,WizardLM/WizardLM-13B-V1.2,1.0,2.43e-05,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,1.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
winogrande.dev.446,claude-v1,0.0,0.000376,0.0,1.3799999999999998e-05,1.0,3.76e-05,0.0,0.000376,0.0,0.000376,0.0,4.8e-05,0.0,0.00047,0.0,3.492e-05,0.0,4.14e-05,1.0,9.2e-06,1.0,2.76e-05,1.0,3.6e-05
mmlu-high-school-biology.val.29,WizardLM/WizardLM-13B-V1.2,1.0,3.45e-05,1.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,0.0,0.0001035,1.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
hellaswag.val.3025,mistralai/mistral-7b-chat,0.0,2.46e-05,1.0,3.69e-05,0.0,9.92e-05,1.0,0.000992,1.0,0.000992,1.0,0.000123,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001098,0.0,2.46e-05,0.0,7.379999999999999e-05,0.0,9.76e-05
hellaswag.val.5081,claude-v1,0.0,0.002056,0.0,7.65e-05,1.0,0.0002056,0.0,0.002056,1.0,0.002056,1.0,0.000258,1.0,0.00257,0.0,0.000198656,1.0,0.0002295,0.0,5.12e-05,0.0,0.0001536,1.0,0.000204
arc-challenge.test.604,meta/llama-2-70b-chat,1.0,0.0001061999999999,0.0,3.57e-05,1.0,9.600000000000002e-05,1.0,0.00096,1.0,0.00096,1.0,0.000121,1.0,0.0012,0.0,9.2344e-05,1.0,0.0001061999999999,0.0,2.3800000000000003e-05,0.0,7.14e-05,1.0,9.440000000000002e-05
mmlu-high-school-computer-science.val.94,gpt-4-1106-preview,1.0,0.00156,0.0,4.65e-05,0.0,0.0001248,0.0,0.001248,1.0,0.001248,0.0,0.000155,1.0,0.00156,0.0,0.00012028,0.0,0.0001395,0.0,3.1e-05,0.0,9.3e-05,0.0,0.000124
winogrande.dev.235,claude-v1,0.0,0.000408,0.0,1.5e-05,0.0,4.08e-05,0.0,0.000408,1.0,0.000408,0.0,5.2e-05,1.0,0.00054,0.0,3.880000000000001e-05,1.0,4.5e-05,0.0,1e-05,0.0,3e-05,0.0,3.92e-05
grade-school-math.dev.3864,gpt-4-1106-preview,0.75,0.0057199999999999,0.75,0.0001326,0.75,0.0005128,0.75,0.004288,0.75,0.0046,0.75,0.000358,0.75,0.0057199999999999,0.75,0.000296432,0.75,0.0003096,0.5,7.28e-05,0.75,0.0002292,0.75,0.0003656
mmlu-professional-accounting.val.229,meta/llama-2-70b-chat,0.0,9.72e-05,0.0,3.24e-05,0.0,8.720000000000002e-05,0.0,0.000872,1.0,0.000872,0.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,0.0,6.48e-05,0.0,8.560000000000002e-05
mmlu-high-school-macroeconomics.val.6,claude-v1,1.0,0.0008,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,1.0,1.98e-05,1.0,5.94e-05,1.0,7.92e-05
grade-school-math.dev.915,gpt-4-1106-preview,0.25,0.01329,0.25,0.0001706999999999,0.75,0.0007104,0.25,0.004608,0.25,0.006,0.25,0.000672,0.25,0.01329,0.25,0.000311176,0.25,0.0004554,0.25,6.620000000000001e-05,0.25,0.000249,0.25,0.0004816
hellaswag.val.3802,claude-v1,0.0,0.0026,0.0,9.689999999999998e-05,0.0,0.00026,0.0,0.0026,1.0,0.0026,1.0,0.000324,0.0,0.00328,0.0,0.000251424,0.0,0.0002907,0.0,6.48e-05,0.0,0.0001943999999999,0.0,0.0002584
winogrande.dev.1234,claude-v1,1.0,0.0003999999999999,0.0,1.4699999999999998e-05,0.0,4e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,1.0,4.9e-05,1.0,0.00053,1.0,3.8024e-05,0.0,4.410000000000001e-05,1.0,9.8e-06,1.0,2.94e-05,0.0,3.84e-05
grade-school-math.dev.7039,gpt-4-1106-preview,0.5,0.0137,0.25,0.0001857,0.75,0.0006448,0.75,0.00532,0.75,0.0068559999999999,0.25,0.000554,0.5,0.0137,0.25,0.000489656,0.25,0.0004527,0.25,0.000101,0.25,0.0003036,0.25,0.0004447999999999
mmlu-professional-law.val.1026,gpt-4-1106-preview,1.0,0.00265,1.0,7.92e-05,1.0,0.000212,1.0,0.00212,1.0,0.00212,1.0,0.000264,1.0,0.00265,0.0,0.000204864,0.0,0.0002376,0.0,5.280000000000001e-05,1.0,0.0001584,1.0,0.0002104
mmlu-professional-law.val.1294,claude-v1,0.0,0.00332,1.0,0.0001241999999999,0.0,0.000332,0.0,0.00332,0.0,0.00332,0.0,0.000414,1.0,0.00415,0.0,0.000321264,0.0,0.0003726,0.0,8.280000000000001e-05,0.0,0.0002483999999999,0.0,0.0003304
arc-challenge.test.436,meta/llama-2-70b-chat,1.0,8.82e-05,0.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,1.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
consensus_summary.dev.32,WizardLM/WizardLM-13B-V1.2,0.75,0.0001017,0.75,0.0001017,0.75,0.0002648,0.25,0.001904,0.75,0.003512,0.75,0.0003599999999999,0.75,0.00496,0.75,0.000262288,0.75,0.000396,0.25,6.2e-05,0.75,0.0001986,0.75,0.0002472
hellaswag.val.8261,claude-v1,1.0,0.001992,1.0,7.439999999999999e-05,0.0,0.0001992,1.0,0.001992,1.0,0.001992,0.0,0.00025,1.0,0.00252,1.0,0.000192448,1.0,0.0002232,1.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
mmlu-clinical-knowledge.val.140,meta/llama-2-70b-chat,0.0,9.9e-05,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,1.0,8.8e-05
grade-school-math.dev.6364,gpt-4-1106-preview,0.75,0.01012,0.25,0.0002028,0.75,0.0009199999999999,0.75,0.005912,0.75,0.007304,0.25,0.000572,0.75,0.01012,0.25,0.0004399919999999,0.25,0.0004086,0.25,0.0001254,0.75,0.0003306,0.25,0.0005424
mmlu-jurisprudence.val.19,meta/llama-2-70b-chat,0.0,6.48e-05,1.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
consensus_summary.dev.241,meta/code-llama-instruct-34b-chat,0.75,0.00024832,0.0,4.59e-05,1.0,0.0001448,1.0,0.001448,0.5,0.002696,1.0,0.000171,1.0,0.00184,0.75,0.00024832,0.75,0.0002357999999999,0.0,3.46e-05,0.75,0.0001086,0.75,0.000152
grade-school-math.dev.6960,gpt-4-1106-preview,0.25,0.0083,0.75,0.0001730999999999,0.25,0.0007336,0.25,0.00436,0.25,0.00604,0.25,0.00051,0.25,0.0083,0.25,0.000321264,0.25,0.0003195,0.25,0.0001016,0.75,0.0002742,0.75,0.0005
mmlu-high-school-statistics.val.193,gpt-4-1106-preview,0.0,0.00159,0.0,4.74e-05,0.0,0.0001272,0.0,0.001272,0.0,0.001272,0.0,0.000158,0.0,0.00159,0.0,0.000122608,0.0,0.0001422,0.0,3.160000000000001e-05,0.0,9.42e-05,0.0,0.0001264
mmlu-medical-genetics.val.94,meta/llama-2-70b-chat,0.0,7.83e-05,0.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-high-school-macroeconomics.val.298,claude-v1,1.0,0.000768,0.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,0.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-international-law.val.60,WizardLM/WizardLM-13B-V1.2,1.0,3.21e-05,1.0,3.21e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00112,0.0,8.380800000000001e-05,0.0,9.72e-05,1.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.640000000000001e-05
hellaswag.val.4368,claude-v2,1.0,0.002104,0.0,7.86e-05,1.0,0.0002104,1.0,0.002104,1.0,0.002104,1.0,0.0002639999999999,0.0,0.00266,0.0,0.000203312,0.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
hellaswag.val.742,mistralai/mistral-7b-chat,0.0,1.86e-05,1.0,2.79e-05,0.0,7.52e-05,0.0,0.000752,0.0,0.000752,1.0,9.5e-05,1.0,0.00097,0.0,7.2168e-05,1.0,8.370000000000002e-05,0.0,1.86e-05,0.0,5.58e-05,1.0,7.36e-05
mmlu-high-school-biology.val.62,WizardLM/WizardLM-13B-V1.2,0.0,3e-05,0.0,3e-05,0.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,0.0,9e-05,1.0,2e-05,0.0,6e-05,1.0,8e-05
mmlu-virology.val.77,gpt-4-1106-preview,0.0,0.00078,0.0,2.22e-05,0.0,6e-05,0.0,0.0006,0.0,0.0006,0.0,7.4e-05,0.0,0.00078,0.0,5.7424e-05,0.0,6.66e-05,1.0,1.48e-05,0.0,4.44e-05,0.0,5.84e-05
hellaswag.val.2072,mistralai/mistral-7b-chat,0.0,1.98e-05,1.0,2.97e-05,0.0,8e-05,1.0,0.0008,0.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,1.0,8.819999999999999e-05,0.0,1.98e-05,0.0,5.94e-05,1.0,7.840000000000001e-05
mmlu-high-school-macroeconomics.val.378,WizardLM/WizardLM-13B-V1.2,1.0,3.57e-05,1.0,3.57e-05,0.0,9.600000000000002e-05,0.0,0.00096,0.0,0.00096,1.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001071,1.0,2.3800000000000003e-05,1.0,7.14e-05,1.0,9.52e-05
grade-school-math.dev.1022,gpt-4-1106-preview,0.75,0.00782,0.75,0.0001598999999999,0.75,0.0004576,0.75,0.005536,0.75,0.0052,0.75,0.000579,0.75,0.00782,0.75,0.000330576,0.75,0.0003222,0.25,9.12e-05,0.25,0.0002646,0.75,0.0004096
hellaswag.val.6258,claude-v1,0.0,0.002064,0.0,7.71e-05,0.0,0.0002064,0.0,0.002064,0.0,0.002064,0.0,0.000259,0.0,0.00258,0.0,0.0001994319999999,0.0,0.0002304,0.0,5.14e-05,0.0,0.0001542,1.0,0.0002048
mmlu-high-school-psychology.val.66,WizardLM/WizardLM-13B-V1.2,0.0,2.88e-05,0.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,0.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,0.0,5.76e-05,1.0,7.68e-05
hellaswag.val.3292,claude-v1,1.0,0.0017439999999999,1.0,6.51e-05,1.0,0.0001768,1.0,0.0017439999999999,0.0,0.0017439999999999,0.0,0.000217,1.0,0.00221,1.0,0.0001683919999999,0.0,0.0001952999999999,1.0,4.340000000000001e-05,1.0,0.0001302,1.0,0.0001728
grade-school-math.dev.4377,meta/llama-2-70b-chat,0.25,0.0004032,0.25,0.0002079,0.75,0.0007056,0.25,0.006264,0.25,0.006096,0.25,0.000709,0.75,0.00909,0.25,0.000347648,0.25,0.0004032,0.25,7.780000000000001e-05,0.5,0.000333,0.25,0.000356
mmlu-professional-law.val.735,gpt-4-1106-preview,1.0,0.00234,1.0,6.9e-05,0.0,0.0001848,1.0,0.001848,1.0,0.001848,1.0,0.000232,1.0,0.00234,0.0,0.0001784799999999,0.0,0.000207,0.0,4.600000000000001e-05,1.0,0.000138,1.0,0.0001832
hellaswag.val.5658,claude-v1,1.0,0.0018239999999999,1.0,6.81e-05,1.0,0.0001824,1.0,0.0018239999999999,0.0,0.0018239999999999,0.0,0.0002289999999999,1.0,0.00231,1.0,0.0001761519999999,0.0,0.0002042999999999,1.0,4.5400000000000006e-05,1.0,0.0001362,1.0,0.0001808
hellaswag.val.7557,meta/code-llama-instruct-34b-chat,0.0,0.00020952,0.0,8.069999999999998e-05,1.0,0.0002192,0.0,0.002168,1.0,0.002168,1.0,0.000272,1.0,0.00274,0.0,0.00020952,0.0,0.000243,0.0,5.4000000000000005e-05,0.0,0.0001613999999999,1.0,0.0002152
mmlu-international-law.val.102,gpt-4-1106-preview,1.0,0.00104,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00104,0.0,7.76e-05,0.0,9e-05,1.0,2e-05,1.0,6e-05,1.0,8e-05
hellaswag.val.6108,claude-v1,0.0,0.002224,0.0,8.31e-05,0.0,0.0002248,0.0,0.002224,0.0,0.002224,1.0,0.000279,0.0,0.00278,0.0,0.000214952,1.0,0.0002493,0.0,5.5400000000000005e-05,0.0,0.0001662,1.0,0.0002208
hellaswag.val.4426,claude-v1,0.0,0.001872,0.0,6.989999999999999e-05,1.0,0.0001872,0.0,0.001872,1.0,0.001872,1.0,0.000233,1.0,0.00234,0.0,0.000180808,1.0,0.0002088,0.0,4.660000000000001e-05,0.0,0.0001397999999999,1.0,0.0001856
hellaswag.val.3043,claude-v1,0.0,0.001008,0.0,3.72e-05,1.0,0.0001008,0.0,0.001008,1.0,0.001008,0.0,0.000125,0.0,0.00126,1.0,9.7e-05,1.0,0.0001116,1.0,2.5e-05,1.0,7.5e-05,0.0,9.92e-05
grade-school-math.dev.704,meta/llama-2-70b-chat,0.25,0.000396,0.75,0.0001356,0.75,0.0005512,0.75,0.005008,0.5,0.006208,0.75,0.000544,0.75,0.00818,0.75,0.000318936,0.25,0.000396,0.25,8.66e-05,0.5,0.0002826,0.5,0.0003808
hellaswag.val.6244,claude-v1,1.0,0.002304,0.0,8.61e-05,1.0,0.0002328,1.0,0.002304,1.0,0.002304,0.0,0.000289,1.0,0.00288,0.0,0.000222712,1.0,0.0002583,0.0,5.7400000000000006e-05,0.0,0.0001722,1.0,0.0002288
grade-school-math.dev.2021,gpt-4-1106-preview,0.75,0.00561,0.5,0.0001293,0.75,0.0005208,0.75,0.004632,0.75,0.004824,0.5,0.00038,0.75,0.00561,0.75,0.000326696,0.75,0.0004041,0.25,7.16e-05,0.5,0.0002082,0.75,0.0003376
hellaswag.val.2798,gpt-4-1106-preview,1.0,0.00127,1.0,3.78e-05,1.0,0.0001016,1.0,0.001016,1.0,0.001016,1.0,0.000128,1.0,0.00127,0.0,9.7776e-05,1.0,0.0001125,0.0,2.52e-05,1.0,7.56e-05,1.0,0.0001
grade-school-math.dev.927,gpt-4-1106-preview,0.5,0.00727,0.25,0.0001707,0.25,0.0006584,0.25,0.004928,0.25,0.0063679999999999,0.25,0.000539,0.5,0.00727,0.25,0.000305744,0.25,0.0003528,0.25,9.04e-05,1.0,0.0001962,0.25,0.0003568
mmlu-formal-logic.val.96,gpt-4-1106-preview,0.0,0.00094,0.0,2.79e-05,1.0,7.52e-05,0.0,0.000752,0.0,0.000752,0.0,9.3e-05,0.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,0.0,5.58e-05,0.0,7.439999999999999e-05
mmlu-human-aging.val.180,meta/llama-2-70b-chat,0.0,6.75e-05,0.0,2.25e-05,1.0,6.08e-05,1.0,0.000608,1.0,0.000608,1.0,7.5e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,1.0,4.5e-05,1.0,5.92e-05
hellaswag.val.1955,claude-v1,1.0,0.000976,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,1.0,0.00125,0.0,9.3896e-05,1.0,0.000108,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
hellaswag.val.1614,claude-v1,0.0,0.00104,1.0,3.8700000000000006e-05,1.0,0.000104,0.0,0.00104,0.0,0.00104,1.0,0.0001309999999999,1.0,0.0013,0.0,0.000100104,0.0,0.0001152,0.0,2.58e-05,1.0,7.740000000000001e-05,0.0,0.0001024
grade-school-math.dev.5958,gpt-4-1106-preview,0.75,0.0078099999999999,0.75,0.000159,0.75,0.0004663999999999,0.75,0.004328,0.75,0.004928,0.75,0.000505,0.75,0.0078099999999999,0.5,0.0003662719999999,0.75,0.000396,0.75,8.840000000000001e-05,0.5,0.000279,0.75,0.0004008
hellaswag.val.1554,claude-v1,1.0,0.001248,1.0,4.65e-05,1.0,0.0001248,1.0,0.001248,1.0,0.001272,1.0,0.000157,1.0,0.00156,0.0,0.00012028,1.0,0.0001395,1.0,3.1e-05,1.0,9.3e-05,1.0,0.0001231999999999
hellaswag.val.128,gpt-4-1106-preview,1.0,0.00106,0.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,1.0,9.36e-05,0.0,2.1e-05,0.0,6.3e-05,1.0,8.320000000000002e-05
consensus_summary.dev.105,gpt-4-1106-preview,1.0,0.00259,0.75,0.0001109999999999,1.0,0.0002024,1.0,0.001952,0.75,0.004592,0.75,0.000388,1.0,0.00259,0.75,0.000232024,0.75,0.0003258,0.75,8.34e-05,0.75,0.0002045999999999,0.75,0.000236
hellaswag.val.6255,claude-v1,1.0,0.0018239999999999,0.0,6.78e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.001848,1.0,0.000227,1.0,0.00228,0.0,0.0001761519999999,1.0,0.0002033999999999,0.0,4.5400000000000006e-05,0.0,0.0001362,1.0,0.0001808
hellaswag.val.335,mistralai/mistral-7b-chat,0.0,2.18e-05,0.0,3.27e-05,1.0,8.800000000000001e-05,1.0,0.00088,1.0,0.00088,1.0,0.0001089999999999,1.0,0.0011,0.0,8.4584e-05,1.0,9.72e-05,0.0,2.18e-05,1.0,6.54e-05,1.0,8.640000000000001e-05
mmlu-computer-security.val.58,WizardLM/WizardLM-13B-V1.2,0.0,2.4e-05,0.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
mmlu-professional-law.val.1258,gpt-4-1106-preview,1.0,0.00202,0.0,5.94e-05,0.0,0.0001592,0.0,0.0015919999999999,0.0,0.0015919999999999,0.0,0.0001999999999999,1.0,0.00202,0.0,0.000153648,0.0,0.0001782,1.0,3.960000000000001e-05,0.0,0.0001188,1.0,0.0001576
mmlu-professional-law.val.1226,claude-v1,0.0,0.002568,0.0,9.6e-05,1.0,0.0002568,0.0,0.002568,0.0,0.002568,0.0,0.0003199999999999,1.0,0.00321,0.0,0.00024832,0.0,0.0002879999999999,1.0,6.4e-05,1.0,0.0001919999999999,1.0,0.0002552
hellaswag.val.1535,meta/llama-2-70b-chat,0.0,9.72e-05,0.0,3.27e-05,0.0,8.800000000000001e-05,1.0,0.00088,0.0,0.00088,0.0,0.0001089999999999,0.0,0.0011,1.0,8.4584e-05,0.0,9.72e-05,1.0,2.18e-05,0.0,6.54e-05,0.0,8.640000000000001e-05
mmlu-professional-law.val.590,meta/llama-2-70b-chat,0.0,0.0003096,0.0,0.0001032,0.0,0.000276,0.0,0.00276,0.0,0.00276,0.0,0.000344,0.0,0.00345,0.0,0.0002669439999999,0.0,0.0003096,0.0,6.88e-05,0.0,0.0002064,0.0,0.0002752
grade-school-math.dev.6105,gpt-4-1106-preview,0.5,0.00701,0.5,0.0001449,0.75,0.0005704,0.5,0.004432,0.75,0.005008,0.5,0.00053,0.5,0.00701,0.5,0.00030652,0.75,0.0004302,0.5,7.68e-05,0.5,0.0002262,0.5,0.000384
mmlu-management.val.93,claude-v2,0.0,0.000632,1.0,2.3400000000000003e-05,0.0,6.32e-05,0.0,0.000632,0.0,0.000632,1.0,7.8e-05,0.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,1.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,0.0,6.240000000000001e-05
hellaswag.val.978,mistralai/mistral-7b-chat,0.0,2.6600000000000003e-05,1.0,3.96e-05,1.0,0.0001072,1.0,0.001072,1.0,0.001072,1.0,0.000133,1.0,0.00134,0.0,0.000103208,1.0,0.0001188,0.0,2.6600000000000003e-05,0.0,7.98e-05,0.0,0.0001056
mmlu-high-school-psychology.val.276,claude-v1,1.0,0.000784,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00101,0.0,7.5272e-05,0.0,8.730000000000001e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.680000000000001e-05
mmlu-professional-accounting.val.101,meta/llama-2-70b-chat,0.0,0.0001035,1.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,0.0,0.0001035,0.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
mmlu-moral-scenarios.val.460,claude-v1,1.0,0.001168,0.0,4.35e-05,1.0,0.0001168,1.0,0.001168,1.0,0.001168,1.0,0.000147,1.0,0.00146,0.0,0.00011252,0.0,0.0001305,0.0,2.9e-05,1.0,8.7e-05,1.0,0.0001152
grade-school-math.dev.3331,meta/llama-2-70b-chat,0.25,0.0006426,0.5,0.0002079,0.75,0.000956,0.75,0.007112,0.75,0.008,0.75,0.000798,0.75,0.01219,0.25,0.000484224,0.25,0.0006426,0.25,0.0001234,0.25,0.0003852,0.75,0.0005968
mmlu-professional-medicine.val.137,claude-v1,1.0,0.001488,0.0,5.55e-05,1.0,0.0001488,1.0,0.001488,1.0,0.001488,1.0,0.000187,1.0,0.00189,0.0,0.00014356,0.0,0.0001665,1.0,3.7000000000000005e-05,1.0,0.000111,1.0,0.0001472
mmlu-professional-law.val.1183,claude-v1,1.0,0.001928,0.0,7.199999999999999e-05,0.0,0.0001928,1.0,0.001928,0.0,0.001928,1.0,0.00024,1.0,0.00241,0.0,0.00018624,0.0,0.000216,1.0,4.8e-05,0.0,0.0001439999999999,1.0,0.0001911999999999
mmlu-professional-law.val.1326,claude-v1,1.0,0.002216,1.0,8.280000000000001e-05,1.0,0.0002216,1.0,0.002216,1.0,0.002216,1.0,0.000276,1.0,0.00277,0.0,0.0002141759999999,0.0,0.0002483999999999,0.0,5.520000000000001e-05,1.0,0.0001656,1.0,0.00022
grade-school-math.dev.4090,gpt-4-1106-preview,0.75,0.00574,0.75,0.0001356,0.75,0.0004928,0.75,0.003344,0.75,0.005,0.75,0.000456,0.75,0.00574,0.75,0.000255304,0.75,0.0003618,0.75,9.36e-05,0.75,0.0002435999999999,0.75,0.0003272
mmlu-human-aging.val.104,meta/llama-2-70b-chat,0.0,7.290000000000001e-05,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
mmlu-security-studies.val.219,meta/llama-2-70b-chat,0.0,0.0001008,0.0,3.3600000000000004e-05,1.0,9.04e-05,0.0,0.000904,1.0,0.000904,1.0,0.000112,1.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,1.0,8.96e-05
hellaswag.val.9743,claude-v1,1.0,0.00212,1.0,7.89e-05,0.0,0.000212,1.0,0.00212,0.0,0.00212,0.0,0.000266,1.0,0.00268,0.0,0.000204864,1.0,0.0002376,1.0,5.280000000000001e-05,1.0,0.0001584,1.0,0.0002104
mmlu-prehistory.val.75,claude-v1,1.0,0.000704,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.0008799999999999,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-high-school-biology.val.290,mistralai/mistral-7b-chat,0.0,1.7e-05,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.720000000000001e-05
hellaswag.val.3889,claude-v1,0.0,0.001624,0.0,6.06e-05,0.0,0.0001624,0.0,0.001624,0.0,0.001624,0.0,0.000204,0.0,0.00206,0.0,0.000156752,0.0,0.0001809,0.0,4.0400000000000006e-05,0.0,0.0001212,0.0,0.0001608
hellaswag.val.3173,claude-v1,1.0,0.000976,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000123,1.0,0.00125,1.0,9.3896e-05,0.0,0.000108,1.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
mmlu-moral-scenarios.val.35,WizardLM/WizardLM-13B-V1.2,0.0,3.96e-05,0.0,3.96e-05,1.0,0.0001064,1.0,0.0010639999999999,1.0,0.0010639999999999,0.0,0.0001319999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001188,0.0,2.64e-05,1.0,7.92e-05,1.0,0.0001048
mmlu-clinical-knowledge.val.193,gpt-4-1106-preview,1.0,0.0009299999999999,1.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.2e-05,1.0,0.0009299999999999,0.0,7.139200000000001e-05,0.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
hellaswag.val.2319,mistralai/mistral-7b-chat,0.0,2.9800000000000003e-05,0.0,4.44e-05,1.0,0.00012,1.0,0.0012,1.0,0.0012,1.0,0.000149,1.0,0.0015,0.0,0.000115624,1.0,0.0001332,0.0,2.9800000000000003e-05,1.0,8.94e-05,1.0,0.0001184
hellaswag.val.5933,claude-v1,1.0,0.0019119999999999,1.0,7.11e-05,1.0,0.0001912,1.0,0.0019119999999999,0.0,0.0019119999999999,1.0,0.0002379999999999,1.0,0.00239,1.0,0.000184688,0.0,0.0002142,1.0,4.7600000000000005e-05,1.0,0.0001428,1.0,0.0001896
hellaswag.val.6724,claude-v1,1.0,0.001952,1.0,7.29e-05,0.0,0.0001976,1.0,0.001952,0.0,0.001952,0.0,0.000243,1.0,0.00247,1.0,0.000188568,0.0,0.0002187,1.0,4.860000000000001e-05,0.0,0.0001458,1.0,0.0001936
grade-school-math.dev.6764,gpt-4-1106-preview,0.75,0.00932,0.25,0.0001515,0.25,0.000784,0.5,0.004504,0.75,0.005104,0.75,0.000396,0.75,0.00932,0.25,0.000296432,0.25,0.0003447,0.25,8.02e-05,0.25,0.0002406,0.25,0.0003976
grade-school-math.dev.391,gpt-4-1106-preview,0.75,0.00723,0.75,0.000144,0.25,0.0002544,0.75,0.0042959999999999,0.5,0.0060479999999999,0.25,0.00053,0.75,0.00723,0.25,0.00026384,0.25,0.0003465,0.25,6.560000000000001e-05,0.25,0.0002201999999999,0.25,0.0002544
hellaswag.val.9660,claude-v1,0.0,0.002048,1.0,7.649999999999999e-05,0.0,0.0002048,0.0,0.002048,1.0,0.002048,0.0,0.000257,1.0,0.00259,1.0,0.00019788,0.0,0.0002295,1.0,5.1000000000000006e-05,1.0,0.0001529999999999,1.0,0.0002032
abstract2title.test.100,WizardLM/WizardLM-13B-V1.2,1.0,7.38e-05,1.0,7.38e-05,1.0,0.0002568,1.0,0.00276,1.0,0.002832,1.0,0.000282,1.0,0.00306,1.0,0.00018236,0.0,0.0004914,1.0,5e-05,1.0,0.0001494,1.0,0.000196
mmlu-moral-scenarios.val.636,WizardLM/WizardLM-13B-V1.2,0.0,3.96e-05,0.0,3.96e-05,0.0,0.0001064,0.0,0.0010639999999999,0.0,0.0010639999999999,0.0,0.0001339999999999,1.0,0.0013599999999999,0.0,0.000102432,0.0,0.0001188,0.0,2.64e-05,0.0,7.92e-05,0.0,0.0001055999999999
mbpp.dev.241,gpt-4-1106-preview,1.0,0.0107,0.0,7.469999999999999e-05,0.0,0.0001336,1.0,0.0017439999999999,0.0,0.004624,0.0,0.000259,1.0,0.0107,0.0,6.2856e-05,0.0,0.0001376999999999,0.0,2.48e-05,1.0,8.64e-05,1.0,0.0002304
mmlu-human-sexuality.val.127,WizardLM/WizardLM-13B-V1.2,1.0,2.43e-05,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,0.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,1.0,1.62e-05,1.0,4.86e-05,1.0,6.48e-05
hellaswag.val.6672,claude-v1,1.0,0.0018399999999999,0.0,6.869999999999999e-05,1.0,0.0001864,1.0,0.0018399999999999,1.0,0.0018399999999999,0.0,0.0002289999999999,1.0,0.0023,0.0,0.000177704,0.0,0.0002061,0.0,4.580000000000001e-05,0.0,0.0001373999999999,1.0,0.0001832
grade-school-math.dev.873,meta/llama-2-70b-chat,0.5,0.0004238999999999,0.25,0.0001494,0.75,0.0005736,0.75,0.005712,0.75,0.005256,0.75,0.000523,0.75,0.0102,0.75,0.000315832,0.5,0.0004238999999999,0.75,0.0001088,0.5,0.0002568,0.5,0.0004016
hellaswag.val.4415,claude-v1,1.0,0.001952,0.0,7.29e-05,1.0,0.0001952,1.0,0.001952,1.0,0.001952,1.0,0.000245,1.0,0.00247,0.0,0.000188568,1.0,0.0002178,0.0,4.860000000000001e-05,1.0,0.0001458,1.0,0.0001936
consensus_summary.dev.305,WizardLM/WizardLM-13B-V1.2,0.75,0.0001119,0.75,0.0001119,0.75,0.0004448,0.0,0.001976,0.75,0.005768,0.75,0.000399,0.75,0.00607,0.75,0.00030652,0.75,0.0003555,0.75,8.36e-05,0.75,0.0002076,0.75,0.0002504
hellaswag.val.1429,mistralai/mistral-7b-chat,0.0,2.28e-05,0.0,3.4200000000000005e-05,1.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.000116,1.0,0.0011799999999999,0.0,8.846400000000001e-05,1.0,0.0001016999999999,0.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
winogrande.dev.454,claude-v1,0.0,0.000376,0.0,1.3799999999999998e-05,1.0,3.76e-05,0.0,0.000376,0.0,0.000376,1.0,4.8e-05,1.0,0.0005,1.0,3.5696e-05,0.0,4.14e-05,1.0,9.2e-06,0.0,2.76e-05,1.0,3.68e-05
mmlu-professional-law.val.1469,WizardLM/WizardLM-13B-V1.2,0.0,6.3e-05,0.0,6.3e-05,0.0,0.0001688,0.0,0.0016879999999999,0.0,0.0016879999999999,0.0,0.0002119999999999,0.0,0.00214,0.0,0.00016296,0.0,0.000189,0.0,4.2e-05,0.0,0.000126,1.0,0.000168
mmlu-management.val.92,meta/llama-2-70b-chat,0.0,5.940000000000001e-05,0.0,1.98e-05,1.0,5.36e-05,1.0,0.000536,1.0,0.000536,1.0,6.599999999999999e-05,1.0,0.00067,0.0,5.1216000000000006e-05,0.0,5.940000000000001e-05,0.0,1.32e-05,1.0,3.96e-05,1.0,5.2e-05
grade-school-math.dev.4509,gpt-4-1106-preview,0.5,0.00904,0.25,0.0001998,0.75,0.0007592,0.75,0.005552,0.75,0.00764,0.75,0.000612,0.5,0.00904,0.25,0.00046172,0.25,0.0004662,0.25,0.0001064,0.25,0.0003156,0.75,0.0004608
mmlu-professional-law.val.36,gpt-4-1106-preview,0.0,0.0041,0.0,0.0001217999999999,0.0,0.0003256,0.0,0.003256,0.0,0.003256,0.0,0.000408,0.0,0.0041,0.0,0.000315056,0.0,0.0003654,1.0,8.120000000000001e-05,0.0,0.0002435999999999,0.0,0.000324
mmlu-professional-law.val.302,gpt-4-1106-preview,0.0,0.0026,0.0,7.769999999999999e-05,0.0,0.000208,0.0,0.00208,0.0,0.00208,0.0,0.000259,0.0,0.0026,0.0,0.000200984,0.0,0.0002331,0.0,5.1800000000000005e-05,0.0,0.0001553999999999,0.0,0.0002064
hellaswag.val.5413,claude-v1,1.0,0.001808,0.0,6.749999999999999e-05,0.0,0.0001832,1.0,0.001808,1.0,0.001808,0.0,0.000227,1.0,0.00229,0.0,0.0001746,0.0,0.0002025,0.0,4.5e-05,0.0,0.0001349999999999,1.0,0.0001792
mmlu-clinical-knowledge.val.46,meta/llama-2-70b-chat,0.0,7.740000000000001e-05,0.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
mmlu-miscellaneous.val.723,WizardLM/WizardLM-13B-V1.2,1.0,2.28e-05,1.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.0008,0.0,5.8976e-05,0.0,6.840000000000001e-05,1.0,1.52e-05,1.0,4.56e-05,1.0,6.08e-05
mmlu-college-biology.val.63,claude-v1,0.0,0.00092,0.0,3.4200000000000005e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,0.0,0.0001139999999999,0.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001026,1.0,2.28e-05,0.0,6.840000000000001e-05,0.0,9.040000000000002e-05
hellaswag.val.1509,gpt-4-1106-preview,0.0,0.00136,0.0,4.05e-05,0.0,0.0001088,0.0,0.001088,0.0,0.001088,0.0,0.000137,0.0,0.00136,0.0,0.0001047599999999,0.0,0.0001206,1.0,2.7e-05,0.0,8.1e-05,0.0,0.0001072
hellaswag.val.4033,meta/llama-2-70b-chat,1.0,0.0002232,0.0,7.439999999999999e-05,0.0,0.0001992,0.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00249,0.0,0.000192448,1.0,0.0002232,0.0,4.9600000000000006e-05,0.0,0.0001487999999999,1.0,0.0001976
mmlu-high-school-physics.val.100,gpt-4-1106-preview,1.0,0.00177,0.0,5.28e-05,0.0,0.0001416,0.0,0.001416,0.0,0.001416,0.0,0.000176,1.0,0.00177,0.0,0.000136576,0.0,0.0001583999999999,1.0,3.520000000000001e-05,0.0,0.0001055999999999,0.0,0.0001408
mmlu-professional-psychology.val.446,gpt-4-1106-preview,0.0,0.00129,0.0,3.84e-05,0.0,0.0001032,0.0,0.001032,0.0,0.001032,0.0,0.000128,0.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,0.0,2.56e-05,0.0,7.68e-05,0.0,0.0001016
hellaswag.val.4102,claude-v1,1.0,0.002176,1.0,8.13e-05,0.0,0.0002176,1.0,0.002176,1.0,0.002176,0.0,0.0002729999999999,1.0,0.00275,0.0,0.0002102959999999,1.0,0.0002439,1.0,5.420000000000001e-05,1.0,0.0001626,1.0,0.000216
hellaswag.val.7289,claude-v1,0.0,0.001816,0.0,6.780000000000001e-05,1.0,0.0001816,0.0,0.001816,1.0,0.001816,1.0,0.0002279999999999,1.0,0.00227,0.0,0.000175376,1.0,0.0002033999999999,0.0,4.520000000000001e-05,0.0,0.0001356,1.0,0.00018
hellaswag.val.9705,claude-v1,1.0,0.002008,0.0,7.5e-05,1.0,0.0002008,1.0,0.002008,1.0,0.002008,1.0,0.000252,1.0,0.00254,0.0,0.000194,0.0,0.000225,0.0,5e-05,0.0,0.00015,0.0,0.0001992
mmlu-philosophy.val.289,gpt-4-1106-preview,1.0,0.00214,1.0,6.39e-05,1.0,0.0001712,1.0,0.001712,0.0,0.001712,1.0,0.000213,1.0,0.00214,0.0,0.000165288,0.0,0.0001917,1.0,4.2600000000000005e-05,1.0,0.0001278,1.0,0.0001704
grade-school-math.dev.1359,claude-v1,0.75,0.004296,0.25,0.0001563,0.75,0.0004464,0.75,0.004296,0.75,0.004248,0.75,0.000454,0.5,0.00798,0.25,0.000352304,0.75,0.0003419999999999,0.25,7.920000000000001e-05,0.75,0.0002352,0.75,0.000348
mmlu-miscellaneous.val.285,meta/llama-2-70b-chat,0.0,8.91e-05,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.00103,0.0,7.682400000000001e-05,0.0,8.91e-05,0.0,1.98e-05,1.0,5.94e-05,1.0,7.840000000000001e-05
grade-school-math.dev.4461,gpt-4-1106-preview,0.5,0.0096,0.25,0.000141,0.75,0.000588,0.75,0.004488,0.75,0.006312,0.5,0.000528,0.5,0.0096,0.25,0.000394208,0.25,0.0003519,0.5,8.280000000000001e-05,0.25,0.0003414,0.5,0.0004136
mmlu-logical-fallacies.val.98,zero-one-ai/Yi-34B-Chat,1.0,7.36e-05,0.0,2.79e-05,0.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.3e-05,1.0,0.00097,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.36e-05
arc-challenge.val.203,meta/llama-2-70b-chat,1.0,6.03e-05,1.0,2.01e-05,1.0,5.44e-05,1.0,0.000544,1.0,0.000544,1.0,6.9e-05,1.0,0.00071,0.0,5.1992000000000006e-05,1.0,6.03e-05,0.0,1.34e-05,1.0,4.02e-05,1.0,5.28e-05
mmlu-high-school-world-history.val.4,meta/llama-2-70b-chat,0.0,0.0002691,1.0,8.97e-05,1.0,0.00024,1.0,0.0024,1.0,0.0024,1.0,0.000299,1.0,0.003,0.0,0.000232024,0.0,0.0002691,0.0,5.980000000000001e-05,1.0,0.0001794,1.0,0.0002384
mmlu-professional-law.val.950,zero-one-ai/Yi-34B-Chat,0.0,0.0001848,0.0,6.93e-05,0.0,0.0001856,0.0,0.001856,0.0,0.001856,0.0,0.000231,0.0,0.00232,0.0,0.000179256,0.0,0.0002078999999999,0.0,4.6200000000000005e-05,0.0,0.0001386,0.0,0.0001848
winogrande.dev.172,claude-v1,1.0,0.000472,1.0,1.7400000000000003e-05,1.0,4.72e-05,1.0,0.000472,1.0,0.000472,1.0,5.8e-05,1.0,0.0005899999999999,0.0,4.500800000000001e-05,0.0,5.22e-05,1.0,1.16e-05,1.0,3.4800000000000006e-05,1.0,4.64e-05
mmlu-moral-scenarios.val.769,claude-v1,1.0,0.0011279999999999,0.0,4.2e-05,1.0,0.0001128,1.0,0.0011279999999999,1.0,0.0011279999999999,1.0,0.0001419999999999,1.0,0.00141,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,1.0,8.4e-05,1.0,0.0001112
mmlu-professional-law.val.837,gpt-4-1106-preview,1.0,0.00266,0.0,7.95e-05,0.0,0.0002128,0.0,0.002128,0.0,0.002128,0.0,0.000265,1.0,0.00266,0.0,0.00020564,0.0,0.0002385,1.0,5.300000000000001e-05,0.0,0.000159,1.0,0.0002112
hellaswag.val.9978,meta/llama-2-70b-chat,1.0,0.0002322,0.0,7.74e-05,1.0,0.0002072,0.0,0.002072,1.0,0.002072,0.0,0.00026,1.0,0.00259,0.0,0.000200208,1.0,0.0002322,0.0,5.160000000000001e-05,0.0,0.0001548,1.0,0.0002056
abstract2title.test.74,gpt-4-1106-preview,1.0,0.00498,1.0,0.0001796999999999,1.0,0.000432,1.0,0.004176,1.0,0.004224,1.0,0.000465,1.0,0.00498,1.0,0.000342216,1.0,0.0005148,1.0,8.680000000000001e-05,1.0,0.0002681999999999,1.0,0.0003608
mmlu-high-school-statistics.val.53,claude-v1,1.0,0.001408,1.0,5.25e-05,1.0,0.0001408,1.0,0.001408,1.0,0.001408,0.0,0.000175,1.0,0.00176,0.0,0.0001357999999999,0.0,0.0001575,0.0,3.5000000000000004e-05,1.0,0.000105,1.0,0.00014
mmlu-professional-law.val.597,meta/llama-2-70b-chat,0.0,0.0002295,0.0,7.649999999999999e-05,0.0,0.0002048,1.0,0.002048,0.0,0.002048,0.0,0.000255,0.0,0.00256,0.0,0.00019788,0.0,0.0002295,0.0,5.1000000000000006e-05,1.0,0.0001529999999999,0.0,0.0002032
mmlu-machine-learning.val.103,WizardLM/WizardLM-13B-V1.2,0.0,8.31e-05,0.0,8.31e-05,1.0,0.0002224,0.0,0.002224,0.0,0.002224,0.0,0.000277,1.0,0.00278,0.0,0.000214952,0.0,0.0002493,0.0,5.5400000000000005e-05,0.0,0.0001662,0.0,0.0002216
mmlu-professional-law.val.1430,gpt-4-1106-preview,0.0,0.00299,1.0,8.94e-05,0.0,0.0002392,0.0,0.002392,0.0,0.002392,0.0,0.000298,0.0,0.00299,0.0,0.0002312479999999,0.0,0.0002682,0.0,5.9600000000000005e-05,0.0,0.0001788,0.0,0.0002376
mmlu-jurisprudence.val.33,gpt-4-1106-preview,1.0,0.00109,0.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
mmlu-prehistory.val.37,meta/llama-2-70b-chat,0.0,8.370000000000002e-05,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.3e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.36e-05
mmlu-high-school-world-history.val.146,claude-v1,1.0,0.003296,1.0,0.0001233,1.0,0.0003296,1.0,0.003296,1.0,0.003296,1.0,0.000411,1.0,0.0041199999999999,0.0,0.000318936,0.0,0.0003699,1.0,8.22e-05,1.0,0.0002466,1.0,0.000328
hellaswag.val.4260,claude-v1,1.0,0.001768,0.0,6.599999999999999e-05,1.0,0.0001792,1.0,0.001768,1.0,0.001768,1.0,0.00022,1.0,0.00221,0.0,0.00017072,1.0,0.0001971,0.0,4.4000000000000006e-05,1.0,0.0001319999999999,1.0,0.0001752
grade-school-math.dev.4167,gpt-4-1106-preview,0.75,0.01274,0.5,0.0001643999999999,0.25,0.0008079999999999,0.25,0.00604,0.25,0.01,0.25,0.0005679999999999,0.75,0.01274,0.25,0.000392656,0.25,0.0004824,0.25,0.0001006,0.75,0.0002676,0.5,0.0002904
grade-school-math.dev.2125,gpt-4-1106-preview,0.75,0.00792,0.25,0.0001118999999999,0.75,0.0004944,0.25,0.00264,0.75,0.005688,0.75,0.000383,0.75,0.00792,0.25,0.000228144,0.25,0.0002988,0.25,7.12e-05,0.25,0.0003083999999999,0.75,0.0003511999999999
mmlu-philosophy.val.105,gpt-4-1106-preview,0.0,0.00111,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,0.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,0.0,8.8e-05
mmlu-high-school-microeconomics.val.4,meta/llama-2-70b-chat,1.0,8.1e-05,0.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,1.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
mmlu-high-school-macroeconomics.val.36,gpt-4-1106-preview,0.0,0.00116,0.0,3.45e-05,0.0,9.28e-05,0.0,0.000928,0.0,0.000928,0.0,0.0001149999999999,0.0,0.00116,0.0,8.924e-05,0.0,0.0001035,1.0,2.3e-05,0.0,6.9e-05,0.0,9.2e-05
arc-challenge.test.289,WizardLM/WizardLM-13B-V1.2,0.0,1.89e-05,0.0,1.89e-05,1.0,5.12e-05,1.0,0.000512,1.0,0.000512,1.0,6.3e-05,1.0,0.0006399999999999,0.0,4.8888e-05,0.0,5.67e-05,0.0,1.26e-05,1.0,3.78e-05,1.0,4.9600000000000006e-05
mmlu-security-studies.val.8,claude-v1,1.0,0.00116,1.0,4.32e-05,0.0,0.000116,1.0,0.00116,1.0,0.00116,1.0,0.000144,1.0,0.00148,0.0,0.000111744,0.0,0.0001295999999999,0.0,2.88e-05,1.0,8.64e-05,1.0,0.0001144
mmlu-security-studies.val.100,meta/llama-2-70b-chat,0.0,0.0001314,1.0,4.38e-05,1.0,0.0001176,1.0,0.001176,1.0,0.001176,1.0,0.000146,1.0,0.00147,0.0,0.000113296,0.0,0.0001314,0.0,2.92e-05,1.0,8.759999999999999e-05,1.0,0.000116
mmlu-virology.val.13,gpt-4-1106-preview,0.0,0.00085,0.0,2.52e-05,0.0,6.800000000000001e-05,0.0,0.00068,0.0,0.00068,0.0,8.4e-05,0.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,1.0,1.6800000000000002e-05,0.0,5.04e-05,0.0,6.720000000000001e-05
mmlu-professional-law.val.1069,gpt-4-1106-preview,0.0,0.00174,0.0,5.1e-05,0.0,0.0001368,0.0,0.0013679999999999,0.0,0.0013679999999999,0.0,0.0001699999999999,0.0,0.00174,0.0,0.0001319199999999,0.0,0.000153,0.0,3.4000000000000007e-05,0.0,0.000102,0.0,0.0001352
grade-school-math.dev.5990,gpt-4-1106-preview,0.5,0.00761,0.25,0.0001467,0.75,0.0005656,0.75,0.004,0.75,0.0071439999999999,0.75,0.000467,0.5,0.00761,0.25,0.000282464,0.75,0.0003627,0.25,7.400000000000001e-05,0.25,0.0003396,0.25,0.0002248
grade-school-math.dev.6941,gpt-4-1106-preview,0.75,0.00646,0.75,0.0001227,0.75,0.0004904,0.75,0.004472,0.5,0.004784,0.75,0.0005239999999999,0.75,0.00646,0.75,0.000336784,0.5,0.000342,0.25,9.02e-05,0.5,0.0002111999999999,0.5,0.0003352
mmlu-professional-law.val.714,gpt-4-1106-preview,1.0,0.00302,1.0,9.03e-05,1.0,0.0002416,1.0,0.002416,1.0,0.002416,1.0,0.000301,1.0,0.00302,0.0,0.000233576,0.0,0.0002709,0.0,6.0200000000000006e-05,1.0,0.0001806,1.0,0.00024
mmlu-conceptual-physics.val.142,gpt-4-1106-preview,1.0,0.00081,0.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,0.0,8.2e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.400000000000001e-05
grade-school-math.dev.6826,meta/llama-2-70b-chat,0.5,0.0004239,0.75,0.0001758,0.25,0.000724,0.75,0.005176,0.5,0.005968,0.75,0.000627,0.5,0.00911,0.25,0.000439216,0.5,0.0004239,0.75,9.22e-05,0.25,0.0002772,0.25,0.000244
mmlu-marketing.val.47,meta/llama-2-70b-chat,0.0,6.48e-05,1.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00076,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
hellaswag.val.2795,mistralai/mistral-7b-chat,0.0,2.56e-05,1.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.9328e-05,1.0,0.0001143,0.0,2.56e-05,0.0,7.68e-05,1.0,0.0001016
mmlu-prehistory.val.79,meta/llama-2-70b-chat,0.0,7.38e-05,0.0,2.46e-05,0.0,6.64e-05,0.0,0.000664,0.0,0.000664,0.0,8.2e-05,0.0,0.00086,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,0.0,4.92e-05,0.0,6.56e-05
mmlu-logical-fallacies.val.105,meta/llama-2-70b-chat,0.0,6.75e-05,1.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,1.0,7.5e-05,0.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,0.0,6e-05
hellaswag.val.7041,claude-v1,0.0,0.002176,0.0,8.13e-05,0.0,0.00022,0.0,0.002176,0.0,0.002176,0.0,0.0002729999999999,0.0,0.00275,0.0,0.0002102959999999,0.0,0.0002439,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-sociology.val.134,meta/llama-2-70b-chat,0.0,6.840000000000001e-05,0.0,2.28e-05,0.0,6.16e-05,1.0,0.000616,0.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,1.0,1.52e-05,1.0,4.56e-05,0.0,6.08e-05
grade-school-math.dev.3902,meta/llama-2-70b-chat,0.5,0.0004167,0.5,0.0001952999999999,0.25,0.0006392,0.75,0.005048,0.75,0.00584,0.25,0.000559,0.75,0.0093099999999999,0.25,0.000415936,0.5,0.0004167,0.25,0.000106,0.25,0.0003528,0.5,0.0004456
hellaswag.val.9991,claude-v1,0.0,0.002016,0.0,7.53e-05,0.0,0.0002016,0.0,0.002016,0.0,0.002016,1.0,0.0002529999999999,1.0,0.00252,0.0,0.000194776,1.0,0.0002259,0.0,5.020000000000001e-05,0.0,0.0001506,1.0,0.0002
mmlu-professional-law.val.641,meta/llama-2-70b-chat,0.0,0.0001683,0.0,5.61e-05,1.0,0.0001504,1.0,0.001504,1.0,0.001504,1.0,0.000187,0.0,0.00188,0.0,0.000145112,0.0,0.0001683,0.0,3.74e-05,0.0,0.0001122,1.0,0.0001496
mmlu-jurisprudence.val.54,zero-one-ai/Yi-34B-Chat,0.0,0.0001032,0.0,3.8700000000000006e-05,0.0,0.000104,0.0,0.00104,0.0,0.00104,0.0,0.000129,0.0,0.0013,0.0,0.000100104,0.0,0.0001161,1.0,2.58e-05,0.0,7.740000000000001e-05,0.0,0.0001032
hellaswag.val.9361,claude-v1,0.0,0.00204,1.0,7.62e-05,0.0,0.000204,0.0,0.00204,0.0,0.002064,0.0,0.000254,1.0,0.00258,1.0,0.0001971039999999,1.0,0.0002286,1.0,5.080000000000001e-05,1.0,0.0001524,1.0,0.0002024
mmlu-professional-law.val.1394,meta/llama-2-70b-chat,0.0,0.0003132,0.0,0.0001044,0.0,0.0002792,0.0,0.002792,0.0,0.002792,0.0,0.000348,0.0,0.00349,0.0,0.0002700479999999,0.0,0.0003132,1.0,6.96e-05,0.0,0.0002088,0.0,0.0002776
hellaswag.val.6078,claude-v1,0.0,0.002304,1.0,8.61e-05,0.0,0.0002328,0.0,0.002304,0.0,0.002304,0.0,0.000289,1.0,0.00291,1.0,0.000222712,1.0,0.0002583,1.0,5.7400000000000006e-05,1.0,0.0001722,1.0,0.0002288
mmlu-high-school-european-history.val.2,gpt-4-1106-preview,1.0,0.00366,0.0,0.0001094999999999,1.0,0.0002928,1.0,0.002928,1.0,0.002928,1.0,0.000365,1.0,0.00366,0.0,0.00028324,0.0,0.0003284999999999,0.0,7.3e-05,1.0,0.0002189999999999,1.0,0.0002912
mmlu-high-school-psychology.val.456,WizardLM/WizardLM-13B-V1.2,1.0,3.24e-05,1.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,1.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.640000000000001e-05
winogrande.dev.1049,claude-v1,0.0,0.000448,0.0,1.65e-05,0.0,4.4800000000000005e-05,0.0,0.000448,0.0,0.000448,0.0,5.7e-05,1.0,0.00059,1.0,4.2680000000000005e-05,0.0,4.95e-05,1.0,1.1e-05,0.0,3.3e-05,1.0,4.3200000000000007e-05
hellaswag.val.8239,meta/code-llama-instruct-34b-chat,0.0,0.000204864,0.0,7.89e-05,0.0,0.0002144,0.0,0.00212,0.0,0.002144,1.0,0.000266,1.0,0.00265,0.0,0.000204864,0.0,0.0002367,0.0,5.280000000000001e-05,0.0,0.0001584,0.0,0.0002104
mmlu-global-facts.val.38,gpt-4-1106-preview,1.0,0.001,1.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,0.0,0.000776,1.0,9.6e-05,1.0,0.001,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.68e-05
grade-school-math.dev.4324,meta/llama-2-70b-chat,0.25,0.0003951,0.75,0.0001467,0.75,0.0006168,0.75,0.0047279999999999,0.75,0.005112,0.75,0.000442,0.75,0.01065,0.75,0.000339888,0.25,0.0003951,0.25,6.9e-05,0.25,0.0002789999999999,0.25,0.000436
mmlu-human-aging.val.105,claude-v1,1.0,0.000632,1.0,2.3400000000000003e-05,1.0,6.32e-05,1.0,0.000632,1.0,0.000632,1.0,7.8e-05,1.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,1.0,4.6800000000000006e-05,1.0,6.16e-05
grade-school-math.dev.611,gpt-4-1106-preview,0.75,0.00917,0.25,0.0001307999999999,0.25,0.0002608,0.75,0.005464,0.75,0.005416,0.75,0.000483,0.75,0.00917,0.25,0.00034144,0.75,0.0003995999999999,0.25,8.740000000000001e-05,0.75,0.0002316,0.75,0.0003464
mmlu-professional-law.val.1010,claude-v1,1.0,0.00092,1.0,3.4200000000000005e-05,1.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.0001139999999999,1.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001026,0.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
mmlu-professional-psychology.val.218,meta/llama-2-70b-chat,0.0,0.0001035,0.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.00116,0.0,8.924e-05,0.0,0.0001035,0.0,2.3e-05,1.0,6.9e-05,1.0,9.12e-05
hellaswag.val.3579,claude-v1,0.0,0.00208,0.0,7.739999999999998e-05,0.0,0.0002104,0.0,0.00208,1.0,0.00208,1.0,0.000261,1.0,0.00263,0.0,0.000200984,0.0,0.0002331,0.0,5.1800000000000005e-05,1.0,0.0001553999999999,1.0,0.0002064
grade-school-math.dev.4543,meta/llama-2-70b-chat,0.75,0.0004023,0.75,0.0001512,0.5,0.000668,0.75,0.004352,0.75,0.0054319999999999,0.5,0.000536,0.5,0.00799,0.75,0.000312728,0.75,0.0004023,0.5,9.84e-05,0.5,0.0002729999999999,0.5,0.0003456
grade-school-math.dev.7054,gpt-4-1106-preview,0.75,0.0089,0.75,0.0001716,0.75,0.0007888,0.75,0.004648,0.75,0.00592,0.75,0.0005549999999999,0.75,0.0089,0.25,0.000301864,0.25,0.0004518,0.25,9.3e-05,0.25,0.0002412,0.75,0.0004048
mmlu-professional-psychology.val.184,WizardLM/WizardLM-13B-V1.2,0.0,3.57e-05,0.0,3.57e-05,0.0,9.600000000000002e-05,0.0,0.00096,1.0,0.00096,0.0,0.0001189999999999,1.0,0.00123,0.0,9.2344e-05,0.0,0.0001071,0.0,2.3800000000000003e-05,0.0,7.14e-05,0.0,9.52e-05
mmlu-high-school-chemistry.val.140,gpt-4-1106-preview,1.0,0.00137,0.0,4.08e-05,0.0,0.0001096,1.0,0.001096,0.0,0.001096,0.0,0.000136,1.0,0.00137,0.0,0.000105536,0.0,0.0001224,0.0,2.72e-05,1.0,8.159999999999999e-05,0.0,0.000108
hellaswag.val.8898,claude-v1,0.0,0.001776,0.0,6.63e-05,0.0,0.0001776,0.0,0.001776,0.0,0.0018,1.0,0.000223,1.0,0.00225,0.0,0.000171496,0.0,0.0001988999999999,0.0,4.420000000000001e-05,1.0,0.0001326,0.0,0.000176
mmlu-professional-psychology.val.584,claude-v1,0.0,0.001872,0.0,6.989999999999999e-05,0.0,0.0001872,0.0,0.001872,0.0,0.001872,0.0,0.000233,0.0,0.00234,0.0,0.000180808,0.0,0.0002097,0.0,4.660000000000001e-05,0.0,0.0001397999999999,0.0,0.0001856
grade-school-math.dev.919,meta/llama-2-70b-chat,0.25,0.0003798,0.25,0.0001593,0.5,0.0006192,0.75,0.0051839999999999,0.5,0.006408,0.5,0.000534,0.75,0.00831,0.25,0.00035308,0.25,0.0003798,0.25,7.92e-05,0.25,0.000237,0.75,0.000392
mmlu-college-biology.val.50,WizardLM/WizardLM-13B-V1.2,1.0,3.57e-05,1.0,3.57e-05,1.0,9.600000000000002e-05,1.0,0.00096,1.0,0.00096,1.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001071,1.0,2.3800000000000003e-05,1.0,7.14e-05,1.0,9.52e-05
mmlu-moral-scenarios.val.756,claude-v1,1.0,0.001104,0.0,4.11e-05,1.0,0.0001104,1.0,0.001104,1.0,0.001104,1.0,0.000139,1.0,0.00138,0.0,0.000106312,0.0,0.0001233,0.0,2.74e-05,1.0,8.22e-05,1.0,0.0001088
mmlu-high-school-macroeconomics.val.222,gpt-4-1106-preview,1.0,0.00096,0.0,2.85e-05,0.0,7.680000000000001e-05,0.0,0.000768,0.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,0.0,7.6e-05
grade-school-math.dev.1234,meta/llama-2-70b-chat,0.25,0.0004194,0.5,0.0001373999999999,0.5,0.00046,0.75,0.00472,0.75,0.004792,0.5,0.000552,0.75,0.00959,0.75,0.000369376,0.25,0.0004194,0.25,8.640000000000001e-05,0.75,0.0002652,0.5,0.0003432
hellaswag.val.143,mistralai/mistral-7b-chat,0.0,1.48e-05,0.0,2.22e-05,0.0,6e-05,0.0,0.0006,0.0,0.0006,0.0,7.6e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.57e-05,0.0,1.48e-05,0.0,4.44e-05,0.0,5.84e-05
mmlu-professional-law.val.424,claude-v1,0.0,0.002456,1.0,9.18e-05,1.0,0.0002456,0.0,0.002456,0.0,0.002456,1.0,0.000306,1.0,0.0031,0.0,0.0002374559999999,0.0,0.0002754,1.0,6.120000000000001e-05,1.0,0.0001836,1.0,0.000244
mmlu-philosophy.val.290,meta/llama-2-70b-chat,0.0,8.82e-05,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
mmlu-electrical-engineering.val.17,meta/llama-2-70b-chat,0.0,6.57e-05,1.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.500000000000001e-05,1.0,0.00077,0.0,5.6648e-05,0.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
mmlu-high-school-microeconomics.val.218,claude-v1,1.0,0.000976,1.0,3.63e-05,0.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,0.0,0.00122,0.0,9.3896e-05,0.0,0.0001089,1.0,2.42e-05,0.0,7.259999999999999e-05,1.0,9.68e-05
mmlu-clinical-knowledge.val.236,gpt-4-1106-preview,1.0,0.00075,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,1.0,4.44e-05,1.0,5.92e-05
mmlu-conceptual-physics.val.46,gpt-4-1106-preview,1.0,0.00075,1.0,2.22e-05,0.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,0.0,4.44e-05,1.0,5.84e-05
grade-school-math.dev.2592,gpt-4-1106-preview,0.75,0.01045,0.5,0.0001569,0.75,0.0006152,0.75,0.004904,0.75,0.006512,0.75,0.000632,0.75,0.01045,0.25,0.0003608399999999,0.75,0.0004122,0.25,0.0001078,0.25,0.000279,0.75,0.0004888
mmlu-professional-law.val.388,gpt-4-1106-preview,1.0,0.00253,0.0,7.56e-05,1.0,0.0002024,1.0,0.002024,1.0,0.002024,1.0,0.000252,1.0,0.00253,0.0,0.000195552,0.0,0.0002267999999999,0.0,5.0400000000000005e-05,0.0,0.0001512,1.0,0.0002008
mmlu-professional-law.val.1332,gpt-4-1106-preview,1.0,0.00435,1.0,0.0001302,1.0,0.000348,1.0,0.00348,1.0,0.00348,1.0,0.000434,1.0,0.00435,0.0,0.000336784,0.0,0.0003906,0.0,8.68e-05,1.0,0.0002604,1.0,0.0003472
mmlu-high-school-biology.val.121,meta/llama-2-70b-chat,0.0,7.56e-05,1.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
hellaswag.val.2598,mistralai/mistral-7b-chat,0.0,2.84e-05,0.0,4.23e-05,0.0,0.0001144,0.0,0.0011439999999999,0.0,0.0011439999999999,0.0,0.0001419999999999,1.0,0.00143,0.0,0.000110192,0.0,0.0001269,0.0,2.84e-05,0.0,8.52e-05,0.0,0.0001128
hellaswag.val.8132,meta/llama-2-70b-chat,1.0,0.0002259,0.0,7.56e-05,1.0,0.0002048,1.0,0.002024,1.0,0.002024,1.0,0.000252,1.0,0.00256,0.0,0.000195552,1.0,0.0002259,0.0,5.0400000000000005e-05,0.0,0.0001512,1.0,0.0002008
hellaswag.val.9655,meta/code-llama-instruct-34b-chat,0.0,0.00019012,0.0,7.35e-05,0.0,0.0001992,1.0,0.001968,1.0,0.001968,1.0,0.000245,0.0,0.00249,0.0,0.00019012,1.0,0.0002196,0.0,4.9000000000000005e-05,1.0,0.000147,0.0,0.0001952
mmlu-econometrics.val.96,gpt-4-1106-preview,1.0,0.00172,1.0,5.13e-05,0.0,0.0001376,1.0,0.0013759999999999,0.0,0.0013759999999999,1.0,0.0001709999999999,1.0,0.00172,0.0,0.000132696,0.0,0.0001538999999999,1.0,3.4200000000000005e-05,1.0,0.0001026,0.0,0.0001368
mmlu-conceptual-physics.val.105,gpt-4-1106-preview,1.0,0.00071,0.0,2.1e-05,0.0,5.6800000000000005e-05,1.0,0.000568,1.0,0.000568,0.0,7e-05,1.0,0.00071,0.0,5.432e-05,0.0,6.3e-05,0.0,1.4e-05,0.0,4.2e-05,0.0,5.520000000000001e-05
hellaswag.val.2437,claude-v1,0.0,0.00092,0.0,3.3900000000000004e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,0.0,0.000116,0.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001016999999999,0.0,2.28e-05,0.0,6.840000000000001e-05,0.0,9.040000000000002e-05
mmlu-college-mathematics.val.60,gpt-4-1106-preview,1.0,0.00077,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,0.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,0.0,4.5e-05,1.0,6.08e-05
grade-school-math.dev.6052,meta/llama-2-70b-chat,0.5,0.0003186,0.5,0.0001347,0.5,0.0005264,0.5,0.005,0.5,0.005672,0.75,0.000405,0.75,0.00652,0.5,0.000355408,0.5,0.0003186,0.25,9.100000000000002e-05,0.75,0.0002562,0.75,0.0004056
mmlu-formal-logic.val.22,gpt-4-1106-preview,0.0,0.00152,0.0,4.53e-05,1.0,0.0001216,0.0,0.0012159999999999,1.0,0.0012159999999999,0.0,0.0001509999999999,0.0,0.00152,0.0,0.000117176,0.0,0.0001359,0.0,3.02e-05,0.0,8.999999999999999e-05,0.0,0.0001208
mmlu-high-school-world-history.val.129,claude-v1,1.0,0.003808,1.0,0.0001425,1.0,0.0003808,1.0,0.003808,1.0,0.003808,1.0,0.000475,1.0,0.0047599999999999,0.0,0.0003686,0.0,0.0004275,1.0,9.5e-05,1.0,0.000285,1.0,0.0003792
grade-school-math.dev.3189,gpt-4-1106-preview,0.5,0.0077699999999999,0.5,0.0001532999999999,0.75,0.0004896,0.75,0.004488,0.5,0.00516,0.5,0.000496,0.5,0.0077699999999999,0.25,0.000304968,0.75,0.0003537,0.5,8.400000000000001e-05,0.5,0.0002568,0.25,0.0002272
mmlu-professional-law.val.1056,gpt-4-1106-preview,1.0,0.00396,0.0,0.0001185,1.0,0.0003168,1.0,0.003168,1.0,0.003192,0.0,0.000395,1.0,0.00396,0.0,0.00030652,0.0,0.0003554999999999,0.0,7.900000000000001e-05,0.0,0.000237,1.0,0.000316
arc-challenge.test.701,meta/code-llama-instruct-34b-chat,0.0,7.139200000000001e-05,1.0,2.76e-05,1.0,7.44e-05,1.0,0.000744,1.0,0.000744,1.0,9.4e-05,1.0,0.00096,0.0,7.139200000000001e-05,1.0,8.280000000000001e-05,0.0,1.84e-05,1.0,5.52e-05,1.0,7.280000000000001e-05
mmlu-security-studies.val.218,claude-v1,1.0,0.002384,0.0,8.91e-05,1.0,0.0002384,1.0,0.002384,1.0,0.002384,1.0,0.000297,1.0,0.00298,0.0,0.000230472,0.0,0.0002673,0.0,5.94e-05,1.0,0.0001782,1.0,0.0002367999999999
hellaswag.val.814,mistralai/mistral-7b-chat,0.0,2.32e-05,0.0,3.45e-05,1.0,9.36e-05,0.0,0.000936,0.0,0.000936,0.0,0.000118,0.0,0.00117,0.0,9.0016e-05,0.0,0.0001035,0.0,2.32e-05,0.0,6.96e-05,0.0,9.2e-05
arc-challenge.test.826,gpt-4-1106-preview,1.0,0.00066,0.0,1.86e-05,1.0,5.04e-05,1.0,0.000504,0.0,0.000504,0.0,6.2e-05,1.0,0.00066,0.0,4.8112e-05,0.0,5.58e-05,0.0,1.24e-05,0.0,3.72e-05,0.0,4.88e-05
mmlu-philosophy.val.91,gpt-4-1106-preview,1.0,0.00192,0.0,5.73e-05,1.0,0.0001536,1.0,0.001536,1.0,0.001536,1.0,0.000191,1.0,0.00192,0.0,0.000148216,0.0,0.0001718999999999,0.0,3.820000000000001e-05,1.0,0.0001146,1.0,0.000152
arc-challenge.test.854,gpt-4-1106-preview,1.0,0.00133,0.0,3.96e-05,0.0,0.0001064,1.0,0.0010639999999999,1.0,0.0010639999999999,0.0,0.0001339999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001188,0.0,2.62e-05,1.0,7.92e-05,1.0,0.0001048
grade-school-math.dev.6543,gpt-4-1106-preview,0.75,0.00621,0.75,0.0001428,0.75,0.0002904,0.75,0.004608,0.75,0.005112,0.75,0.000589,0.75,0.00621,0.5,0.000308072,0.75,0.0003888,0.25,8.5e-05,0.75,0.0002561999999999,0.75,0.0004024
grade-school-math.dev.2112,gpt-4-1106-preview,0.75,0.00966,0.25,0.0001724999999999,0.25,0.0007199999999999,0.25,0.007008,0.25,0.008448,0.25,0.000639,0.75,0.00966,0.25,0.000342992,0.75,0.0004112999999999,0.25,8.060000000000001e-05,0.25,0.0002892,0.25,0.000424
arc-challenge.test.767,WizardLM/WizardLM-13B-V1.2,1.0,3.12e-05,1.0,3.12e-05,1.0,8.400000000000001e-05,1.0,0.00084,1.0,0.00084,1.0,0.000106,1.0,0.00108,1.0,8.0704e-05,1.0,9.36e-05,1.0,2.08e-05,1.0,6.24e-05,1.0,8.240000000000001e-05
hellaswag.val.2919,mistralai/mistral-7b-chat,0.0,3.2200000000000003e-05,0.0,4.8e-05,1.0,0.0001296,0.0,0.0012959999999999,0.0,0.0012959999999999,1.0,0.0001629999999999,1.0,0.00165,0.0,0.000124936,0.0,0.0001439999999999,0.0,3.2200000000000003e-05,0.0,9.66e-05,0.0,0.000128
mmlu-moral-scenarios.val.650,claude-v1,0.0,0.0011279999999999,0.0,4.2e-05,0.0,0.0001128,0.0,0.0011279999999999,0.0,0.0011279999999999,1.0,0.00014,0.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,0.0,8.4e-05,0.0,0.000112
mmlu-professional-law.val.267,claude-v1,1.0,0.00188,1.0,7.02e-05,1.0,0.000188,1.0,0.00188,1.0,0.00188,1.0,0.000236,1.0,0.00235,0.0,0.000181584,0.0,0.0002106,0.0,4.6800000000000006e-05,1.0,0.0001404,1.0,0.0001864
mmlu-professional-accounting.val.239,gpt-4-1106-preview,1.0,0.00091,0.0,2.7e-05,0.0,7.280000000000001e-05,0.0,0.000728,0.0,0.000728,0.0,8.999999999999999e-05,1.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,0.0,5.4e-05,0.0,7.2e-05
mmlu-professional-law.val.1124,meta/llama-2-70b-chat,0.0,0.0001467,0.0,4.89e-05,0.0,0.0001312,0.0,0.001312,0.0,0.001312,1.0,0.000163,0.0,0.00167,0.0,0.0001264879999999,0.0,0.0001467,0.0,3.2600000000000006e-05,0.0,9.78e-05,0.0,0.0001296
grade-school-math.dev.2573,gpt-4-1106-preview,0.75,0.00777,0.75,0.0001566,0.75,0.0006263999999999,0.75,0.00456,0.75,0.0054959999999999,0.75,0.0005579999999999,0.75,0.00777,0.25,0.000585104,0.75,0.0004374,0.25,0.0001122,0.75,0.0002676,0.75,0.0003664
hellaswag.val.3100,meta/llama-2-70b-chat,1.0,9.81e-05,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,1.0,9.81e-05,0.0,2.2e-05,0.0,6.6e-05,1.0,8.72e-05
mmlu-professional-psychology.val.538,meta/llama-2-70b-chat,0.0,6.48e-05,0.0,2.16e-05,0.0,5.84e-05,1.0,0.000584,1.0,0.000584,0.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,1.0,4.32e-05,1.0,5.76e-05
grade-school-math.dev.1567,gpt-4-1106-preview,0.75,0.01411,0.25,0.0001539,0.75,0.0007447999999999,0.75,0.006248,0.75,0.006176,0.75,0.000638,0.75,0.01411,0.25,0.00041904,0.25,0.0004284,0.25,0.0001012,0.25,0.000381,0.25,0.0004424
grade-school-math.dev.1638,gpt-4-1106-preview,0.75,0.01069,0.75,0.0001392,0.75,0.0006296,0.75,0.005048,0.75,0.0057199999999999,0.25,0.000577,0.75,0.01069,0.25,0.000320488,0.5,0.0003699,0.25,0.0001372,0.75,0.0002808,0.75,0.0004232
hellaswag.val.9236,claude-v1,0.0,0.002144,0.0,8.01e-05,0.0,0.0002168,0.0,0.002144,0.0,0.002144,1.0,0.000269,0.0,0.00271,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,0.0,0.0002128
mmlu-abstract-algebra.val.46,gpt-4-1106-preview,1.0,0.00104,0.0,3.09e-05,0.0,8.320000000000002e-05,0.0,0.000832,0.0,0.000832,0.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,0.0,6.18e-05,0.0,8.240000000000001e-05
arc-challenge.test.534,meta/llama-2-70b-chat,1.0,0.0001619999999999,1.0,5.43e-05,0.0,0.0001456,1.0,0.0014559999999999,1.0,0.0014559999999999,1.0,0.0001809999999999,1.0,0.00182,0.0,0.0001404559999999,1.0,0.0001619999999999,0.0,3.6200000000000006e-05,1.0,0.0001086,1.0,0.000144
grade-school-math.dev.1703,gpt-4-1106-preview,0.75,0.00936,0.5,0.0001491,0.5,0.0005544,0.5,0.004704,0.75,0.00492,0.5,0.000441,0.75,0.00936,0.25,0.000268496,0.75,0.000351,0.25,6.840000000000001e-05,0.25,0.0002154,0.5,0.0003488
mbpp.dev.68,meta/llama-2-70b-chat,1.0,0.0001997999999999,1.0,4.62e-05,1.0,0.0002496,1.0,0.001416,1.0,0.004344,1.0,0.000184,1.0,0.00513,0.0,9.312e-05,1.0,0.0001997999999999,1.0,4.14e-05,1.0,7.26e-05,1.0,0.0001
mmlu-miscellaneous.val.557,meta/llama-2-70b-chat,0.0,7.38e-05,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00083,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
hellaswag.val.1092,claude-v1,0.0,0.001032,0.0,3.84e-05,0.0,0.0001032,0.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.9328e-05,0.0,0.0001143,0.0,2.56e-05,0.0,7.68e-05,0.0,0.0001016
mmlu-professional-law.val.1129,WizardLM/WizardLM-13B-V1.2,0.0,9.96e-05,0.0,9.96e-05,1.0,0.0002664,1.0,0.002664,1.0,0.002664,1.0,0.000332,1.0,0.00333,0.0,0.000257632,0.0,0.0002988,1.0,6.64e-05,1.0,0.0001992,1.0,0.0002648
mbpp.dev.60,meta/llama-2-70b-chat,0.0,0.0001575,0.0,6.599999999999999e-05,0.0,0.0004927999999999,0.0,0.00368,0.0,0.0062959999999999,0.0,0.000363,1.0,0.01117,1.0,0.0001047599999999,0.0,0.0001575,0.0,2.72e-05,0.0,0.0001962,0.0,9.36e-05
grade-school-math.dev.179,claude-v1,0.75,0.0063999999999999,0.25,0.0001820999999999,0.5,0.0007072,0.75,0.0063999999999999,0.5,0.007552,0.5,0.000543,0.5,0.0101,0.25,0.000355408,0.75,0.0003942,0.25,8.060000000000001e-05,0.75,0.0003174,0.75,0.0004512
winogrande.dev.1155,claude-v1,1.0,0.000448,1.0,1.62e-05,1.0,4.4800000000000005e-05,1.0,0.000448,1.0,0.000448,1.0,5.7e-05,1.0,0.00059,1.0,4.2680000000000005e-05,0.0,4.86e-05,1.0,1.1e-05,1.0,3.3e-05,1.0,4.3200000000000007e-05
mmlu-moral-disputes.val.187,gpt-4-1106-preview,1.0,0.00111,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,1.0,6.6e-05,1.0,8.8e-05
mmlu-miscellaneous.val.266,zero-one-ai/Yi-34B-Chat,1.0,5.76e-05,0.0,2.19e-05,0.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,1.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
mmlu-professional-medicine.val.133,gpt-4-1106-preview,1.0,0.00204,0.0,6e-05,1.0,0.0001608,1.0,0.0016079999999999,1.0,0.0016079999999999,1.0,0.0002019999999999,1.0,0.00204,0.0,0.0001551999999999,0.0,0.00018,0.0,4e-05,1.0,0.00012,1.0,0.0001592
hellaswag.val.2065,meta/llama-2-70b-chat,0.0,0.0001008,0.0,3.39e-05,0.0,9.120000000000002e-05,0.0,0.000912,0.0,0.000912,0.0,0.000113,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001008,0.0,2.2600000000000004e-05,0.0,6.78e-05,0.0,8.960000000000002e-05
grade-school-math.dev.1915,gpt-4-1106-preview,0.5,0.00778,0.75,0.0001758,0.75,0.0006032,0.75,0.004376,0.5,0.006848,0.75,0.000434,0.5,0.00778,0.75,0.00026772,0.75,0.0003429,0.75,7.8e-05,0.75,0.000276,0.75,0.0003888
mmlu-professional-law.val.1401,claude-v1,0.0,0.002688,0.0,0.0001004999999999,0.0,0.0002688,0.0,0.002688,0.0,0.002688,1.0,0.000335,0.0,0.00336,0.0,0.00025996,0.0,0.0003014999999999,1.0,6.7e-05,0.0,0.0002009999999999,0.0,0.0002672
arc-challenge.test.281,gpt-4-1106-preview,1.0,0.00122,0.0,3.54e-05,1.0,9.52e-05,1.0,0.000952,1.0,0.000952,0.0,0.00012,1.0,0.00122,0.0,9.1568e-05,1.0,0.0001062,0.0,2.36e-05,0.0,7.08e-05,1.0,9.36e-05
winogrande.dev.1151,claude-v1,0.0,0.000408,0.0,1.5e-05,1.0,4.08e-05,0.0,0.000408,0.0,0.000408,1.0,5e-05,1.0,0.00054,0.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,1.0,3e-05,1.0,3.92e-05
grade-school-math.dev.3420,gpt-4-1106-preview,0.75,0.00834,0.75,0.0001431,0.75,0.0006864,0.5,0.005784,0.75,0.0066,0.25,0.000455,0.75,0.00834,0.25,0.00035696,0.25,0.0003932999999999,0.25,9.880000000000002e-05,0.25,0.0002507999999999,0.75,0.0004192
winogrande.dev.479,claude-v1,1.0,0.000376,1.0,1.3799999999999998e-05,1.0,3.76e-05,1.0,0.000376,1.0,0.000376,1.0,4.8e-05,1.0,0.00047,0.0,3.5696e-05,1.0,4.14e-05,0.0,9.2e-06,1.0,2.76e-05,1.0,3.6e-05
hellaswag.val.3134,mistralai/mistral-7b-chat,0.0,2.88e-05,0.0,4.32e-05,1.0,0.000116,1.0,0.00116,1.0,0.00116,1.0,0.000146,1.0,0.00145,0.0,0.000111744,1.0,0.0001286999999999,0.0,2.88e-05,1.0,8.64e-05,1.0,0.0001144
mmlu-professional-law.val.309,gpt-4-1106-preview,1.0,0.00319,0.0,9.54e-05,1.0,0.0002552,1.0,0.002552,0.0,0.002552,1.0,0.000318,1.0,0.00319,0.0,0.000246768,0.0,0.0002862,1.0,6.36e-05,1.0,0.0001908,1.0,0.0002544
hellaswag.val.9164,claude-v1,1.0,0.002136,1.0,7.98e-05,1.0,0.0002136,1.0,0.002136,1.0,0.002136,1.0,0.000268,1.0,0.0027,0.0,0.0002064159999999,1.0,0.0002394,0.0,5.3200000000000006e-05,0.0,0.0001596,1.0,0.000212
mmlu-professional-law.val.1328,gpt-4-1106-preview,1.0,0.00225,0.0,6.72e-05,1.0,0.00018,1.0,0.0018,0.0,0.0018,0.0,0.000224,1.0,0.00225,0.0,0.0001738239999999,0.0,0.0002016,1.0,4.480000000000001e-05,0.0,0.0001344,1.0,0.0001784
hellaswag.val.1303,mistralai/mistral-7b-chat,1.0,2.44e-05,1.0,3.63e-05,1.0,9.84e-05,1.0,0.000984,0.0,0.000984,1.0,0.000122,0.0,0.00123,1.0,9.4672e-05,0.0,0.0001088999999999,1.0,2.44e-05,1.0,7.32e-05,1.0,9.68e-05
mmlu-professional-law.val.1207,claude-v1,0.0,0.0013759999999999,0.0,5.13e-05,0.0,0.0001376,0.0,0.0013759999999999,0.0,0.0013759999999999,0.0,0.0001709999999999,0.0,0.00172,0.0,0.000132696,0.0,0.0001538999999999,0.0,3.4200000000000005e-05,0.0,0.0001026,0.0,0.000136
winogrande.dev.533,WizardLM/WizardLM-13B-V1.2,1.0,1.7400000000000003e-05,1.0,1.7400000000000003e-05,1.0,4.72e-05,1.0,0.000472,1.0,0.000472,1.0,5.8e-05,1.0,0.0005899999999999,0.0,4.500800000000001e-05,1.0,5.13e-05,0.0,1.16e-05,0.0,3.4800000000000006e-05,1.0,4.56e-05
hellaswag.val.8992,claude-v1,1.0,0.002144,0.0,8.01e-05,1.0,0.0002144,1.0,0.002144,1.0,0.002144,1.0,0.000267,1.0,0.00268,0.0,0.000207192,1.0,0.0002394,0.0,5.34e-05,1.0,0.0001602,1.0,0.0002128
mmlu-moral-scenarios.val.672,claude-v1,0.0,0.0011279999999999,1.0,4.2e-05,0.0,0.0001128,0.0,0.0011279999999999,0.0,0.0011279999999999,0.0,0.00014,1.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,0.0,8.4e-05,0.0,0.0001112
consensus_summary.dev.78,meta/code-llama-instruct-34b-chat,0.75,0.000251424,0.25,6.54e-05,1.0,0.0002423999999999,0.25,0.001776,0.75,0.005664,0.75,0.0003599999999999,0.25,0.00222,0.75,0.000251424,0.0,0.0002421,0.25,4.72e-05,0.75,0.0001914,0.75,0.0002032
mmlu-prehistory.val.22,gpt-4-1106-preview,1.0,0.0007599999999999,1.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,0.0,7.7e-05,1.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,0.0,5.92e-05
consensus_summary.dev.187,meta/code-llama-instruct-34b-chat,0.75,0.000218056,0.75,8.73e-05,0.75,0.0002223999999999,1.0,0.0016719999999999,0.75,0.002824,0.75,0.000282,0.75,0.00353,0.75,0.000218056,0.75,0.0002663999999999,0.75,6.14e-05,0.75,0.0001572,0.75,0.0002056
mmlu-professional-psychology.val.410,gpt-4-1106-preview,1.0,0.0013,1.0,3.8700000000000006e-05,1.0,0.000104,1.0,0.00104,1.0,0.00104,1.0,0.000129,1.0,0.0013,0.0,0.000100104,0.0,0.0001161,0.0,2.58e-05,1.0,7.740000000000001e-05,1.0,0.0001024
grade-school-math.dev.3476,gpt-4-1106-preview,0.5,0.00531,0.75,0.0001233,0.75,0.0004368,0.5,0.003864,0.5,0.004776,0.25,0.000283,0.5,0.00531,0.75,0.000261512,0.75,0.0003015,0.75,7.48e-05,0.75,0.0002279999999999,0.75,0.0003288
mmlu-prehistory.val.179,claude-v1,1.0,0.000664,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00086,0.0,6.3632e-05,1.0,7.29e-05,1.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
mmlu-business-ethics.val.64,meta/llama-2-70b-chat,0.0,0.0001188,0.0,3.96e-05,1.0,0.0001064,1.0,0.0010639999999999,1.0,0.0010639999999999,1.0,0.0001319999999999,1.0,0.00133,0.0,0.000102432,0.0,0.0001188,0.0,2.64e-05,1.0,7.92e-05,1.0,0.0001055999999999
grade-school-math.dev.211,gpt-4-1106-preview,0.75,0.01206,0.25,0.0001716,0.75,0.0007872,0.75,0.007752,0.5,0.007584,0.75,0.0008389999999999,0.75,0.01206,0.25,0.000443096,0.25,0.0005391,0.5,0.000113,0.25,0.0003306,0.25,0.0004504
mmlu-high-school-mathematics.val.34,meta/code-llama-instruct-34b-chat,0.0,9.0016e-05,0.0,3.48e-05,0.0,9.36e-05,0.0,0.000936,0.0,0.000936,1.0,0.000118,0.0,0.00117,0.0,9.0016e-05,0.0,0.0001044,0.0,2.32e-05,0.0,6.96e-05,0.0,9.28e-05
mmlu-high-school-us-history.val.133,claude-v1,1.0,0.002872,0.0,0.0001074,1.0,0.0002872,1.0,0.002872,1.0,0.002872,1.0,0.0003599999999999,1.0,0.00362,0.0,0.000277808,0.0,0.0003222,0.0,7.16e-05,1.0,0.0002148,1.0,0.0002856
arc-challenge.test.687,gpt-4-1106-preview,1.0,0.00063,1.0,1.77e-05,1.0,4.8e-05,1.0,0.00048,1.0,0.00048,1.0,6.1000000000000005e-05,1.0,0.00063,1.0,4.5784e-05,1.0,5.310000000000001e-05,1.0,1.18e-05,1.0,3.54e-05,1.0,4.72e-05
mmlu-machine-learning.val.63,claude-v1,0.0,0.000752,0.0,2.79e-05,1.0,7.52e-05,0.0,0.000752,0.0,0.000752,0.0,9.3e-05,0.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,0.0,5.58e-05,0.0,7.439999999999999e-05
winogrande.dev.825,claude-v1,1.0,0.000504,1.0,1.83e-05,1.0,5.04e-05,1.0,0.000504,1.0,0.000504,1.0,6.2e-05,1.0,0.00066,1.0,4.8112e-05,1.0,5.58e-05,1.0,1.24e-05,1.0,3.72e-05,1.0,4.88e-05
mmlu-professional-law.val.594,meta/llama-2-70b-chat,0.0,0.0002664,0.0,8.88e-05,1.0,0.0002376,1.0,0.002376,1.0,0.002376,1.0,0.000296,1.0,0.00297,0.0,0.000229696,0.0,0.0002664,0.0,5.920000000000001e-05,1.0,0.0001776,1.0,0.000236
mmlu-high-school-macroeconomics.val.154,gpt-4-1106-preview,1.0,0.0015,1.0,4.47e-05,0.0,0.00012,1.0,0.0012,0.0,0.0012,1.0,0.000149,1.0,0.0015,0.0,0.000115624,0.0,0.0001341,1.0,2.9800000000000003e-05,1.0,8.94e-05,1.0,0.0001192
mmlu-high-school-government-and-politics.val.140,meta/llama-2-70b-chat,0.0,8.91e-05,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,1.0,1.98e-05,1.0,5.94e-05,1.0,7.840000000000001e-05
grade-school-math.dev.6902,meta/llama-2-70b-chat,0.75,0.0002997,0.25,0.0001338,0.75,0.0004751999999999,0.75,0.0042959999999999,0.75,0.0048959999999999,0.5,0.0003909999999999,0.5,0.00636,0.5,0.000245216,0.75,0.0002997,0.75,6.14e-05,0.75,0.0001877999999999,0.75,0.000316
mmlu-professional-law.val.372,claude-v1,1.0,0.001648,0.0,6.149999999999999e-05,1.0,0.0001648,1.0,0.001648,1.0,0.001648,1.0,0.000205,1.0,0.00206,0.0,0.0001590799999999,0.0,0.0001845,0.0,4.100000000000001e-05,1.0,0.0001229999999999,1.0,0.0001632
mmlu-professional-medicine.val.90,WizardLM/WizardLM-13B-V1.2,0.0,8.49e-05,0.0,8.49e-05,1.0,0.0002272,1.0,0.002272,1.0,0.002272,1.0,0.000283,1.0,0.00284,0.0,0.000219608,0.0,0.0002547,0.0,5.660000000000001e-05,1.0,0.0001698,1.0,0.0002256
mbpp.dev.276,gpt-4-1106-preview,1.0,0.01562,1.0,0.0001038,1.0,0.0005392,1.0,0.002632,1.0,0.00604,1.0,0.000382,1.0,0.01562,1.0,0.000131144,1.0,0.0003546,1.0,2.9000000000000004e-05,1.0,0.0001056,1.0,0.0001968
hellaswag.val.4538,claude-v1,1.0,0.001992,0.0,7.439999999999999e-05,1.0,0.0001992,1.0,0.001992,1.0,0.001992,1.0,0.00025,1.0,0.00249,0.0,0.000192448,1.0,0.0002223,0.0,4.9600000000000006e-05,1.0,0.0001487999999999,1.0,0.0001976
mmlu-sociology.val.120,meta/llama-2-70b-chat,0.0,6.03e-05,1.0,2.01e-05,1.0,5.44e-05,1.0,0.000544,1.0,0.000544,1.0,6.699999999999999e-05,1.0,0.0006799999999999,0.0,5.1992000000000006e-05,0.0,6.03e-05,1.0,1.34e-05,1.0,4.02e-05,1.0,5.36e-05
consensus_summary.dev.266,meta/code-llama-instruct-34b-chat,0.75,0.000223488,0.75,0.0001446,0.75,0.000348,0.75,0.002832,0.75,0.0036959999999999,0.75,0.0003,0.75,0.00438,0.75,0.000223488,0.75,0.0003546,0.75,4.8200000000000006e-05,0.75,0.0001541999999999,0.75,0.0001968
hellaswag.val.6634,claude-v1,1.0,0.002104,0.0,7.86e-05,0.0,0.0002104,1.0,0.002104,1.0,0.002104,0.0,0.0002639999999999,1.0,0.00266,0.0,0.000203312,1.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
hellaswag.val.4809,claude-v1,1.0,0.002128,1.0,7.919999999999999e-05,1.0,0.0002152,1.0,0.002128,1.0,0.002128,0.0,0.000267,1.0,0.00266,1.0,0.00020564,1.0,0.0002376,1.0,5.300000000000001e-05,1.0,0.000159,1.0,0.0002112
bias_detection.dev.191,meta/llama-2-70b-chat,0.0,0.000315,0.0,9.45e-05,1.0,0.000292,1.0,0.0037359999999999,1.0,0.006952,1.0,0.000254,1.0,0.00827,0.0,0.000241336,0.0,0.000315,0.0,5.06e-05,1.0,0.0001727999999999,0.0,0.0002296
mmlu-professional-law.val.1159,gpt-4-1106-preview,0.0,0.00183,1.0,5.46e-05,0.0,0.0001464,0.0,0.001464,0.0,0.001464,0.0,0.000182,0.0,0.00183,0.0,0.000141232,0.0,0.0001638,0.0,3.64e-05,0.0,0.0001092,0.0,0.0001456
mmlu-miscellaneous.val.391,zero-one-ai/Yi-34B-Chat,0.0,6.640000000000001e-05,0.0,2.52e-05,0.0,6.800000000000001e-05,0.0,0.00068,0.0,0.00068,0.0,8.6e-05,0.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,0.0,1.6800000000000002e-05,0.0,5.04e-05,0.0,6.640000000000001e-05
mmlu-professional-law.val.798,mistralai/mixtral-8x7b-chat,0.0,0.0003257999999999,1.0,0.0001628999999999,1.0,0.0004352,1.0,0.004352,1.0,0.004352,1.0,0.000543,1.0,0.0054399999999999,0.0,0.000421368,0.0,0.0004887,0.0,0.0001086,0.0,0.0003257999999999,1.0,0.0004336
grade-school-math.dev.4979,gpt-4-1106-preview,0.25,0.00777,0.25,0.0001563,0.25,0.0006024,0.25,0.004704,0.25,0.007584,0.25,0.0005679999999999,0.25,0.00777,0.25,0.000346872,0.25,0.0003699,0.25,9.74e-05,1.0,0.0002082,0.25,0.0002448
mmlu-professional-law.val.417,claude-v1,1.0,0.001984,1.0,7.379999999999999e-05,1.0,0.0001984,1.0,0.001984,1.0,0.001984,1.0,0.000247,1.0,0.00248,0.0,0.000191672,0.0,0.0002222999999999,0.0,4.94e-05,1.0,0.0001482,1.0,0.0001967999999999
mmlu-sociology.val.85,claude-v1,1.0,0.001096,0.0,4.08e-05,1.0,0.0001096,1.0,0.001096,0.0,0.001096,0.0,0.000136,1.0,0.00137,0.0,0.000105536,0.0,0.0001224,0.0,2.72e-05,0.0,8.159999999999999e-05,1.0,0.0001088
mmlu-professional-law.val.185,claude-v1,0.0,0.0015999999999999,0.0,5.97e-05,0.0,0.00016,0.0,0.0015999999999999,0.0,0.0015999999999999,0.0,0.0001989999999999,1.0,0.002,0.0,0.000154424,0.0,0.0001791,0.0,3.980000000000001e-05,0.0,0.0001193999999999,0.0,0.0001592
grade-school-math.dev.3263,gpt-4-1106-preview,0.25,0.01187,0.25,0.0001779,0.25,0.0005775999999999,0.25,0.00592,0.25,0.0067119999999999,0.25,0.000611,0.25,0.01187,0.25,0.000296432,0.25,0.0004383,0.25,0.000114,0.25,0.0003768,0.25,0.0004592
hellaswag.val.9260,claude-v1,1.0,0.001864,0.0,6.96e-05,1.0,0.0001864,1.0,0.001864,1.0,0.001864,1.0,0.000234,1.0,0.00233,0.0,0.000180032,1.0,0.0002087999999999,0.0,4.64e-05,0.0,0.0001392,1.0,0.0001848
mmlu-professional-law.val.1076,gpt-4-1106-preview,1.0,0.0029,1.0,8.669999999999999e-05,1.0,0.000232,1.0,0.00232,1.0,0.00232,1.0,0.000289,1.0,0.0029,0.0,0.000224264,0.0,0.0002601,0.0,5.780000000000001e-05,1.0,0.0001733999999999,1.0,0.0002304
mmlu-professional-psychology.val.489,WizardLM/WizardLM-13B-V1.2,1.0,2.67e-05,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,1.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
mmlu-computer-security.val.48,WizardLM/WizardLM-13B-V1.2,1.0,3.75e-05,1.0,3.75e-05,1.0,0.0001008,1.0,0.001008,1.0,0.001008,1.0,0.000125,1.0,0.00126,0.0,9.7e-05,0.0,0.0001125,0.0,2.5e-05,1.0,7.5e-05,1.0,0.0001
mmlu-high-school-european-history.val.151,claude-v1,0.0,0.002368,0.0,8.85e-05,0.0,0.0002368,0.0,0.002368,0.0,0.002368,1.0,0.000295,1.0,0.00299,0.0,0.0002289199999999,0.0,0.0002655,1.0,5.9e-05,1.0,0.000177,1.0,0.0002352
hellaswag.val.3327,claude-v1,0.0,0.002312,0.0,8.609999999999999e-05,0.0,0.0002312,0.0,0.002312,0.0,0.002312,1.0,0.000288,1.0,0.00289,0.0,0.0002234879999999,1.0,0.0002583,0.0,5.76e-05,0.0,0.0001728,1.0,0.0002296
arc-challenge.test.1012,gpt-4-1106-preview,1.0,0.00103,1.0,3.06e-05,0.0,8.240000000000001e-05,1.0,0.000824,1.0,0.000824,1.0,0.000104,1.0,0.00103,0.0,7.9152e-05,1.0,9.09e-05,0.0,2.04e-05,1.0,6.12e-05,1.0,8.080000000000001e-05
hellaswag.val.8506,claude-v1,0.0,0.00236,0.0,8.819999999999999e-05,0.0,0.000236,0.0,0.00236,1.0,0.00236,0.0,0.000296,1.0,0.00298,0.0,0.000228144,1.0,0.0002646,0.0,5.8800000000000006e-05,0.0,0.0001763999999999,1.0,0.0002344
grade-school-math.dev.2904,gpt-4-1106-preview,0.75,0.00713,0.25,0.0001352999999999,0.75,0.000556,0.75,0.004072,0.75,0.006616,0.75,0.00051,0.75,0.00713,0.25,0.000342992,0.25,0.0003744,0.25,8.5e-05,0.75,0.0002873999999999,0.0,0.000496
grade-school-math.dev.5860,gpt-4-1106-preview,0.75,0.01044,0.25,0.0002081999999999,0.5,0.000624,0.75,0.004776,0.5,0.007272,0.25,0.0009,0.75,0.01044,0.25,0.000366272,0.25,0.0004391999999999,0.25,9.6e-05,0.25,0.0003035999999999,0.25,0.0002584
hellaswag.val.7221,claude-v1,0.0,0.001936,0.0,7.230000000000001e-05,0.0,0.0001936,0.0,0.001936,0.0,0.00196,1.0,0.000243,0.0,0.00242,0.0,0.000187016,1.0,0.0002169,0.0,4.8200000000000006e-05,0.0,0.0001446,1.0,0.000192
mmlu-marketing.val.163,meta/llama-2-70b-chat,0.0,7.83e-05,0.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.900000000000001e-05,1.0,0.00091,0.0,6.751200000000001e-05,0.0,7.83e-05,0.0,1.74e-05,1.0,5.22e-05,1.0,6.88e-05
mmlu-professional-law.val.148,gpt-4-1106-preview,0.0,0.00246,0.0,7.35e-05,1.0,0.0001968,0.0,0.001968,0.0,0.001968,0.0,0.000247,0.0,0.00246,0.0,0.00019012,0.0,0.0002205,1.0,4.9000000000000005e-05,1.0,0.000147,0.0,0.0001952
mmlu-professional-law.val.80,claude-v1,0.0,0.003328,1.0,0.0001245,0.0,0.0003328,0.0,0.003328,1.0,0.003328,1.0,0.000415,0.0,0.00416,0.0,0.00032204,0.0,0.0003734999999999,0.0,8.3e-05,0.0,0.0002484,0.0,0.0003312
hellaswag.val.3954,mistralai/mistral-7b-chat,0.0,5.380000000000001e-05,0.0,8.07e-05,0.0,0.000216,0.0,0.00216,0.0,0.00216,0.0,0.000269,1.0,0.0027,0.0,0.000208744,0.0,0.0002412,0.0,5.380000000000001e-05,0.0,0.0001614,1.0,0.0002144
mmlu-professional-law.val.65,gpt-4-1106-preview,1.0,0.00482,0.0,0.0001442999999999,0.0,0.0003856,0.0,0.003856,0.0,0.003856,1.0,0.000481,1.0,0.00482,0.0,0.000373256,0.0,0.0004329,1.0,9.62e-05,0.0,0.0002885999999999,0.0,0.000384
hellaswag.val.4906,claude-v1,0.0,0.002088,0.0,7.8e-05,0.0,0.0002088,0.0,0.002088,0.0,0.002088,0.0,0.0002619999999999,1.0,0.00264,0.0,0.00020176,0.0,0.0002331,0.0,5.2e-05,0.0,0.000156,1.0,0.0002072
grade-school-math.dev.2799,gpt-4-1106-preview,0.75,0.00575,0.75,0.0001251,0.75,0.0004744,0.75,0.003712,0.75,0.004744,0.75,0.000371,0.75,0.00575,0.75,0.00028712,0.5,0.0003114,0.25,7.960000000000001e-05,0.75,0.0002316,0.75,0.0003312
mmlu-marketing.val.134,meta/code-llama-instruct-34b-chat,0.0,6.285600000000001e-05,0.0,2.43e-05,0.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
hellaswag.val.9497,meta/llama-2-70b-chat,0.0,0.0002619,0.0,8.730000000000001e-05,0.0,0.000236,0.0,0.0023599999999999,0.0,0.002336,1.0,0.0002929999999999,1.0,0.00295,0.0,0.000225816,0.0,0.0002619,0.0,5.8200000000000005e-05,0.0,0.0001746,1.0,0.000232
hellaswag.val.8171,claude-v1,1.0,0.0018239999999999,1.0,6.81e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.0018239999999999,0.0,0.0002289999999999,1.0,0.00231,1.0,0.0001761519999999,0.0,0.0002033999999999,1.0,4.5400000000000006e-05,1.0,0.0001362,1.0,0.0001808
mmlu-nutrition.val.225,WizardLM/WizardLM-13B-V1.2,1.0,3.15e-05,1.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,0.0,0.000848,1.0,0.0001049999999999,1.0,0.00109,0.0,8.148e-05,0.0,9.45e-05,1.0,2.1e-05,1.0,6.3e-05,0.0,8.400000000000001e-05
grade-school-math.dev.1805,meta/llama-2-70b-chat,0.75,0.0003392999999999,0.5,0.0001512,0.75,0.000592,0.5,0.004504,0.5,0.0058,0.5,0.000514,0.5,0.0077,0.25,0.000315832,0.75,0.0003392999999999,0.25,0.0001016,0.75,0.0002376,0.75,0.0004144
hellaswag.val.5381,claude-v1,1.0,0.002416,0.0,8.999999999999999e-05,0.0,0.0002416,1.0,0.002416,1.0,0.002416,1.0,0.000303,1.0,0.00305,0.0,0.000233576,0.0,0.0002709,0.0,6.0200000000000006e-05,0.0,0.0001806,1.0,0.00024
mmlu-professional-law.val.1016,gpt-4-1106-preview,1.0,0.00106,0.0,3.15e-05,0.0,8.480000000000001e-05,0.0,0.000848,1.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,0.0,2.1e-05,1.0,6.3e-05,1.0,8.400000000000001e-05
hellaswag.val.1363,mistralai/mistral-7b-chat,0.0,2.78e-05,0.0,4.17e-05,1.0,0.000112,0.0,0.00112,1.0,0.00112,1.0,0.000139,0.0,0.0014299999999999,0.0,0.000107864,0.0,0.0001241999999999,0.0,2.78e-05,0.0,8.340000000000001e-05,0.0,0.0001104
grade-school-math.dev.5413,gpt-4-1106-preview,0.75,0.00771,0.75,0.000138,0.25,0.0005736,0.75,0.0036959999999999,0.75,0.00468,0.75,0.000522,0.75,0.00771,0.75,0.000278584,0.25,0.0004032,0.75,9.4e-05,0.75,0.0001931999999999,0.75,0.0003568
hellaswag.val.8950,claude-v1,1.0,0.001848,0.0,6.9e-05,0.0,0.0001872,1.0,0.001848,1.0,0.001872,1.0,0.000232,1.0,0.00234,0.0,0.0001784799999999,1.0,0.000207,0.0,4.600000000000001e-05,0.0,0.000138,0.0,0.0001832
hellaswag.val.8835,claude-v1,1.0,0.00228,0.0,8.52e-05,1.0,0.0002304,1.0,0.00228,1.0,0.00228,0.0,0.000286,1.0,0.00285,0.0,0.0002203839999999,0.0,0.0002556,0.0,5.680000000000001e-05,0.0,0.0001704,0.0,0.0002264
mmlu-prehistory.val.134,meta/llama-2-70b-chat,0.0,9.18e-05,0.0,3.06e-05,1.0,8.240000000000001e-05,1.0,0.000824,1.0,0.000824,1.0,0.000102,1.0,0.00103,0.0,7.9152e-05,0.0,9.18e-05,0.0,2.04e-05,1.0,6.12e-05,1.0,8.080000000000001e-05
grade-school-math.dev.5353,gpt-4-1106-preview,0.75,0.00763,0.25,0.0001311,0.75,0.0006032,0.75,0.004472,0.75,0.006656,0.75,0.0004759999999999,0.75,0.00763,0.75,0.000294104,0.75,0.0003618,0.25,8.78e-05,0.25,0.0002286,0.75,0.0004064
hellaswag.val.6666,claude-v1,0.0,0.00236,0.0,8.819999999999999e-05,0.0,0.0002384,0.0,0.00236,0.0,0.00236,1.0,0.000294,1.0,0.00295,0.0,0.000228144,1.0,0.0002637,0.0,5.8800000000000006e-05,0.0,0.0001763999999999,0.0,0.0002344
mbpp.dev.63,meta/llama-2-70b-chat,0.0,0.0003051,0.0,6.27e-05,1.0,0.0005384,1.0,0.003704,0.0,0.0077599999999999,1.0,0.000333,0.0,0.01573,0.0,0.000175376,0.0,0.0003051,0.0,3.64e-05,0.0,0.0002214,0.0,0.0002296
grade-school-math.dev.3322,gpt-4-1106-preview,0.75,0.01222,0.75,0.0001662,0.75,0.0007016,0.25,0.007424,0.75,0.01124,0.25,0.000734,0.75,0.01222,0.25,0.00031816,0.25,0.0005022,0.25,9.68e-05,0.75,0.000285,0.25,0.0007288
grade-school-math.dev.4060,meta/code-llama-instruct-34b-chat,0.5,0.00029876,0.75,0.0001581,0.75,0.0006736,0.75,0.005128,0.75,0.005728,0.75,0.000441,0.75,0.0092,0.5,0.00029876,0.5,0.0004761,0.75,0.0001102,0.75,0.0002484,0.5,0.000384
mmlu-high-school-biology.val.153,claude-v1,1.0,0.001032,0.0,3.84e-05,0.0,0.0001032,1.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,0.0,2.56e-05,1.0,7.68e-05,1.0,0.0001024
grade-school-math.dev.3275,gpt-4-1106-preview,0.75,0.0051699999999999,0.75,0.0001314,0.75,0.0004256,0.75,0.004352,0.75,0.004472,0.75,0.000362,0.75,0.0051699999999999,0.75,0.000260736,0.75,0.0002979,0.25,9.02e-05,0.5,0.0002172,0.75,0.0002984
grade-school-math.dev.1767,meta/llama-2-70b-chat,0.75,0.0003789,0.5,0.0001727999999999,0.75,0.00062,0.75,0.004592,0.75,0.006632,0.5,0.00039,0.75,0.0091,0.25,0.000253752,0.75,0.0003789,0.25,6.04e-05,0.5,0.0002766,0.75,0.000424
accounting_audit.dev.25,gpt-4-1106-preview,1.0,0.00185,0.0,5.67e-05,1.0,0.000148,1.0,0.00148,1.0,0.00148,1.0,0.000184,1.0,0.00185,0.0,0.000146664,0.0,0.0001701,0.0,5.92e-05,0.0,0.0001133999999999,0.0,0.0001472
hellaswag.val.2325,gpt-4-1106-preview,1.0,0.00095,1.0,2.82e-05,1.0,7.600000000000002e-05,1.0,0.00076,1.0,0.00076,1.0,9.6e-05,1.0,0.00095,0.0,7.2944e-05,1.0,8.46e-05,0.0,1.8800000000000003e-05,1.0,5.64e-05,1.0,7.440000000000002e-05
winogrande.dev.1092,claude-v1,0.0,0.000488,0.0,1.77e-05,0.0,4.88e-05,0.0,0.000488,0.0,0.000488,0.0,6.2e-05,0.0,0.00064,0.0,4.5784e-05,0.0,5.31e-05,1.0,1.2e-05,1.0,3.6e-05,0.0,4.8e-05
mmlu-security-studies.val.113,gpt-4-1106-preview,1.0,0.0021,1.0,6.269999999999999e-05,1.0,0.000168,1.0,0.0016799999999999,0.0,0.0016799999999999,0.0,0.0002089999999999,1.0,0.0021,0.0,0.000162184,0.0,0.0001881,0.0,4.1800000000000006e-05,0.0,0.0001253999999999,1.0,0.0001672
mmlu-miscellaneous.val.538,claude-v1,1.0,0.000752,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.3e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.439999999999999e-05
hellaswag.val.4707,claude-v1,0.0,0.0022,0.0,8.219999999999999e-05,0.0,0.00022,0.0,0.0022,1.0,0.0022,0.0,0.000276,0.0,0.00275,0.0,0.000212624,0.0,0.0002457,0.0,5.480000000000001e-05,0.0,0.0001643999999999,1.0,0.0002184
hellaswag.val.2271,mistralai/mistral-7b-chat,0.0,2.18e-05,0.0,3.27e-05,1.0,8.800000000000001e-05,1.0,0.00088,1.0,0.00088,1.0,0.0001089999999999,1.0,0.0011,0.0,8.4584e-05,0.0,9.72e-05,0.0,2.18e-05,1.0,6.54e-05,1.0,8.640000000000001e-05
mmlu-professional-law.val.15,claude-v1,1.0,0.001584,0.0,5.91e-05,1.0,0.0001584,1.0,0.001584,1.0,0.001584,1.0,0.000197,1.0,0.00198,0.0,0.0001528719999999,0.0,0.0001773,0.0,3.94e-05,1.0,0.0001182,1.0,0.0001568
mmlu-moral-scenarios.val.720,claude-v1,1.0,0.001112,0.0,4.14e-05,1.0,0.0001112,1.0,0.001112,1.0,0.001112,1.0,0.000138,1.0,0.00139,0.0,0.000107088,0.0,0.0001241999999999,0.0,2.7600000000000003e-05,1.0,8.28e-05,1.0,0.0001096
hellaswag.val.3092,mistralai/mistral-7b-chat,1.0,2.52e-05,1.0,3.78e-05,0.0,0.0001016,0.0,0.001016,1.0,0.001016,0.0,0.000126,0.0,0.00127,0.0,9.7776e-05,0.0,0.0001125,1.0,2.52e-05,1.0,7.56e-05,0.0,0.0001
mmlu-high-school-statistics.val.104,mistralai/mixtral-8x7b-chat,1.0,0.0001038,1.0,5.19e-05,1.0,0.0001392,1.0,0.001392,1.0,0.001392,1.0,0.000173,1.0,0.00174,0.0,0.0001342479999999,0.0,0.0001557,0.0,3.460000000000001e-05,1.0,0.0001038,1.0,0.0001376
mmlu-nutrition.val.130,meta/llama-2-70b-chat,0.0,0.0001215,1.0,4.05e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,1.0,0.000135,1.0,0.00139,0.0,0.0001047599999999,0.0,0.0001215,1.0,2.7e-05,1.0,8.1e-05,1.0,0.000108
mmlu-professional-psychology.val.276,meta/llama-2-70b-chat,0.0,0.0001044,1.0,3.45e-05,1.0,9.36e-05,1.0,0.000936,1.0,0.000936,1.0,0.000116,1.0,0.00117,0.0,9.0016e-05,0.0,0.0001044,0.0,2.32e-05,1.0,6.96e-05,1.0,9.2e-05
hellaswag.val.3818,claude-v1,1.0,0.00216,0.0,8.07e-05,0.0,0.0002184,1.0,0.00216,0.0,0.00216,0.0,0.000271,1.0,0.0027,0.0,0.000208744,1.0,0.0002412,0.0,5.380000000000001e-05,1.0,0.0001614,0.0,0.0002144
mmlu-international-law.val.27,gpt-4-1106-preview,1.0,0.00112,0.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,0.0,0.000108,1.0,0.00112,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.640000000000001e-05
mmlu-moral-disputes.val.6,WizardLM/WizardLM-13B-V1.2,1.0,2.85e-05,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,0.0,0.00099,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,0.0,5.7e-05,1.0,7.6e-05
mmlu-prehistory.val.307,meta/llama-2-70b-chat,0.0,9.63e-05,1.0,3.21e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.0011099999999999,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.56e-05
mmlu-professional-law.val.254,claude-v1,1.0,0.002144,1.0,8.01e-05,1.0,0.0002144,1.0,0.002144,1.0,0.002144,1.0,0.000267,1.0,0.00268,0.0,0.000207192,0.0,0.0002402999999999,1.0,5.34e-05,1.0,0.0001602,1.0,0.0002128
grade-school-math.dev.3124,meta/llama-2-70b-chat,0.75,0.0004608,0.25,0.000174,0.5,0.0007744,0.5,0.005752,0.5,0.008848,0.75,0.000665,0.5,0.0115699999999999,0.25,0.0003685999999999,0.75,0.0004608,0.25,8.48e-05,0.25,0.0002724,0.25,0.0002696
consensus_summary.dev.216,gpt-4-1106-preview,1.0,0.00267,0.75,7.62e-05,0.5,0.000228,1.0,0.001896,0.75,0.003504,0.75,0.000275,1.0,0.00267,0.75,0.000243664,0.75,0.0002952,1.0,5.0400000000000005e-05,0.75,0.0001661999999999,0.75,0.0002016
mmlu-professional-law.val.744,gpt-4-1106-preview,0.0,0.00229,0.0,6.84e-05,0.0,0.0001832,0.0,0.0018319999999999,0.0,0.0018319999999999,0.0,0.0002279999999999,0.0,0.00229,0.0,0.000176928,0.0,0.0002052,1.0,4.56e-05,0.0,0.0001368,0.0,0.0001824
grade-school-math.dev.5577,gpt-4-1106-preview,0.5,0.01013,0.25,0.0001446,0.25,0.0006136,0.25,0.0046,0.5,0.008008,0.25,0.000641,0.5,0.01013,0.25,0.000421368,0.25,0.0003798,0.25,0.0001014,0.25,0.0002472,0.25,0.0005168
hellaswag.val.9039,claude-v1,0.0,0.002232,0.0,8.309999999999999e-05,0.0,0.0002256,0.0,0.002232,1.0,0.002232,0.0,0.00028,1.0,0.00282,0.0,0.000215728,0.0,0.0002502,0.0,5.56e-05,0.0,0.0001668,1.0,0.0002216
grade-school-math.dev.2482,gpt-4-1106-preview,0.75,0.009,0.75,0.0001674,0.75,0.0005688,0.75,0.0050399999999999,0.75,0.006936,0.75,0.000567,0.75,0.009,0.75,0.000308072,0.75,0.0003987,0.75,8.76e-05,0.75,0.0002802,0.75,0.0003744
consensus_summary.dev.179,WizardLM/WizardLM-13B-V1.2,0.75,9.24e-05,0.75,9.24e-05,0.75,0.0003088,1.0,0.0021999999999999,0.75,0.004912,0.75,0.0003509999999999,0.75,0.00392,0.75,0.000242888,0.75,0.000279,0.75,6.2e-05,0.75,0.0001842,0.75,0.0002496
winogrande.dev.528,claude-v1,1.0,0.000384,1.0,1.41e-05,1.0,3.84e-05,1.0,0.000384,1.0,0.000384,1.0,4.9000000000000005e-05,1.0,0.00051,1.0,3.6472000000000006e-05,0.0,4.2300000000000005e-05,1.0,9.4e-06,1.0,2.82e-05,1.0,3.68e-05
grade-school-math.dev.4571,gpt-4-1106-preview,0.5,0.00932,0.75,0.0001581,0.75,0.0005632,0.5,0.004744,0.75,0.005104,0.5,0.00058,0.5,0.00932,0.25,0.000293328,0.25,0.0003411,0.25,8.840000000000001e-05,0.75,0.0002747999999999,0.75,0.0003736
hellaswag.val.8600,meta/llama-2-70b-chat,0.0,0.0002466,0.0,8.219999999999999e-05,1.0,0.0002224,1.0,0.0022,1.0,0.0022,1.0,0.000274,1.0,0.00275,0.0,0.000212624,0.0,0.0002466,0.0,5.480000000000001e-05,0.0,0.0001643999999999,1.0,0.0002184
mmlu-professional-law.val.281,meta/llama-2-70b-chat,0.0,0.0003834,0.0,0.0001278,1.0,0.0003416,1.0,0.003416,1.0,0.003416,1.0,0.000426,1.0,0.0042699999999999,0.0,0.000330576,0.0,0.0003834,0.0,8.520000000000001e-05,1.0,0.0002556,1.0,0.0003408
hellaswag.val.7893,claude-v1,0.0,0.002024,0.0,7.529999999999999e-05,1.0,0.0002048,0.0,0.002024,0.0,0.002024,1.0,0.000254,1.0,0.00253,0.0,0.000195552,0.0,0.0002259,0.0,5.0400000000000005e-05,0.0,0.0001512,1.0,0.0002008
hellaswag.val.2763,WizardLM/WizardLM-13B-V1.2,0.0,3.12e-05,0.0,3.12e-05,0.0,8.480000000000001e-05,0.0,0.000848,1.0,0.000848,0.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,1.0,9.45e-05,0.0,2.1e-05,0.0,6.3e-05,0.0,8.320000000000002e-05
grade-school-math.dev.6413,gpt-4-1106-preview,0.75,0.00799,0.25,0.0001677,0.5,0.0005984,0.75,0.004568,0.5,0.00608,0.75,0.0004799999999999,0.75,0.00799,0.75,0.000301088,0.5,0.0004581,0.25,0.0001006,0.75,0.0003114,0.75,0.0003656
mmlu-us-foreign-policy.val.7,meta/llama-2-70b-chat,0.0,8.1e-05,0.0,2.7e-05,0.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,0.0,8.999999999999999e-05,0.0,0.00091,0.0,6.984e-05,0.0,8.1e-05,0.0,1.8e-05,0.0,5.4e-05,0.0,7.120000000000001e-05
grade-school-math.dev.5824,meta/llama-2-70b-chat,0.75,0.0003429,0.75,0.0001428,0.75,0.0004984,0.75,0.004432,0.75,0.005104,0.5,0.0004179999999999,0.75,0.00752,0.75,0.000262288,0.75,0.0003429,0.75,6.780000000000001e-05,0.75,0.0002399999999999,0.75,0.0003576
winogrande.dev.82,claude-v1,1.0,0.000416,1.0,1.53e-05,0.0,4.16e-05,1.0,0.000416,1.0,0.000416,0.0,5.3e-05,0.0,0.00055,0.0,3.9576e-05,1.0,4.59e-05,0.0,1.02e-05,0.0,3.06e-05,0.0,4.08e-05
mmlu-miscellaneous.val.476,meta/llama-2-70b-chat,0.0,6.66e-05,0.0,2.22e-05,0.0,6e-05,0.0,0.0006,0.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,0.0,4.44e-05,0.0,5.92e-05
grade-school-math.dev.5041,gpt-4-1106-preview,0.75,0.01055,0.25,0.0002028,0.75,0.0006904,0.75,0.00544,0.5,0.007144,0.75,0.0006839999999999,0.75,0.01055,0.25,0.000326696,0.75,0.0004977,0.25,0.0001134,0.25,0.0003,0.75,0.0004664
mmlu-high-school-us-history.val.28,meta/llama-2-70b-chat,0.0,0.0003537,1.0,0.0001178999999999,1.0,0.0003152,1.0,0.003152,1.0,0.003152,1.0,0.000393,1.0,0.00394,0.0,0.0003049679999999,0.0,0.0003537,1.0,7.86e-05,1.0,0.0002357999999999,1.0,0.0003136
grade-school-math.dev.6754,gpt-4-1106-preview,0.75,0.00882,0.75,0.0001454999999999,0.75,0.0005088,0.75,0.004032,0.5,0.004608,0.75,0.000509,0.75,0.00882,0.75,0.0003104,0.75,0.0003123,0.25,7.38e-05,0.25,0.0002153999999999,0.25,0.0003463999999999
hellaswag.val.7190,claude-v1,1.0,0.001952,1.0,7.29e-05,0.0,0.0001952,1.0,0.001952,1.0,0.001952,0.0,0.000245,1.0,0.00247,1.0,0.000188568,1.0,0.0002187,1.0,4.860000000000001e-05,1.0,0.0001458,1.0,0.0001936
mmlu-nutrition.val.222,claude-v1,0.0,0.000824,0.0,3.06e-05,0.0,8.240000000000001e-05,0.0,0.000824,0.0,0.000824,0.0,0.000102,0.0,0.00103,0.0,7.9152e-05,0.0,9.18e-05,0.0,2.04e-05,0.0,6.12e-05,0.0,8.16e-05
mmlu-miscellaneous.val.70,gpt-4-1106-preview,1.0,0.00086,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.8e-05
consensus_summary.dev.225,meta/llama-2-70b-chat,0.75,0.0002799,0.75,8.97e-05,1.0,0.0003448,1.0,0.00364,0.25,0.004096,0.75,0.000451,0.25,0.00422,0.25,0.000312728,0.75,0.0002799,0.25,6.879999999999999e-05,1.0,0.0001662,0.75,0.0002152
mmlu-professional-psychology.val.234,gpt-4-1106-preview,1.0,0.00087,1.0,2.58e-05,1.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,1.0,5.16e-05,1.0,6.800000000000001e-05
hellaswag.val.7707,claude-v1,1.0,0.002304,1.0,8.61e-05,1.0,0.0002304,1.0,0.002304,1.0,0.002304,0.0,0.000289,1.0,0.00291,1.0,0.000222712,1.0,0.0002583,1.0,5.7400000000000006e-05,1.0,0.0001722,1.0,0.0002288
hellaswag.val.6260,meta/llama-2-70b-chat,0.0,0.0002259,0.0,7.529999999999999e-05,0.0,0.0002048,0.0,0.002024,0.0,0.002024,0.0,0.000254,1.0,0.00256,0.0,0.000195552,0.0,0.0002259,0.0,5.0400000000000005e-05,0.0,0.0001512,1.0,0.0002008
grade-school-math.dev.4389,gpt-4-1106-preview,0.75,0.00844,0.5,0.0001605,0.25,0.0009176,0.75,0.005048,0.75,0.0073519999999999,0.0,0.0009499999999999,0.75,0.00844,0.25,0.000467928,0.75,0.0004238999999999,0.25,0.0001564,0.25,0.0004362,0.25,0.0005088
mmlu-elementary-mathematics.val.7,gpt-4-1106-preview,1.0,0.00082,0.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,0.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,0.0,4.86e-05,0.0,6.48e-05
mmlu-miscellaneous.val.742,mistralai/mixtral-8x7b-chat,1.0,4.14e-05,1.0,2.07e-05,1.0,5.6e-05,1.0,0.00056,1.0,0.00056,1.0,6.9e-05,1.0,0.0007,0.0,5.354400000000001e-05,1.0,6.21e-05,0.0,1.38e-05,1.0,4.14e-05,1.0,5.44e-05
mmlu-professional-law.val.1170,gpt-4-1106-preview,1.0,0.00329,1.0,9.84e-05,1.0,0.0002632,1.0,0.002632,1.0,0.002632,1.0,0.000328,1.0,0.00329,0.0,0.000254528,0.0,0.0002952,0.0,6.56e-05,1.0,0.0001967999999999,1.0,0.0002616
mmlu-moral-scenarios.val.760,claude-v1,1.0,0.001176,1.0,4.38e-05,1.0,0.0001176,1.0,0.001176,1.0,0.001176,1.0,0.000146,1.0,0.00147,0.0,0.000113296,0.0,0.0001314,0.0,2.92e-05,0.0,8.759999999999999e-05,1.0,0.000116
grade-school-math.dev.2701,gpt-4-1106-preview,0.75,0.00761,0.75,0.0001977,0.25,0.000736,0.5,0.005104,0.75,0.005464,0.25,0.00068,0.75,0.00761,0.25,0.000369376,0.25,0.0003789,0.25,0.0001112,0.75,0.000267,0.25,0.0004576
mmlu-professional-law.val.1486,claude-v1,0.0,0.002736,0.0,0.0001023,0.0,0.0002736,0.0,0.002736,0.0,0.002736,0.0,0.000341,1.0,0.00342,0.0,0.000264616,0.0,0.0003069,0.0,6.819999999999999e-05,1.0,0.0002046,1.0,0.000272
mmlu-professional-law.val.565,gpt-4-1106-preview,1.0,0.00323,0.0,9.66e-05,1.0,0.0002584,1.0,0.002584,1.0,0.002584,0.0,0.000322,1.0,0.00323,0.0,0.000249872,0.0,0.0002898,0.0,6.44e-05,1.0,0.0001932,1.0,0.0002568
hellaswag.val.650,mistralai/mistral-7b-chat,0.0,2.54e-05,1.0,3.81e-05,1.0,0.0001024,1.0,0.001024,1.0,0.001024,1.0,0.000127,1.0,0.00128,0.0,9.8552e-05,1.0,0.0001133999999999,0.0,2.54e-05,1.0,7.62e-05,1.0,0.0001008
grade-school-math.dev.6352,gpt-4-1106-preview,0.75,0.00966,0.75,0.000195,0.75,0.0005808,0.75,0.004704,0.75,0.0063599999999999,0.75,0.000659,0.75,0.00966,0.75,0.000357736,0.25,0.0004131,0.25,9.82e-05,0.75,0.0002946,0.75,0.000404
grade-school-math.dev.375,gpt-4-1106-preview,0.75,0.00653,0.75,0.0001442999999999,0.75,0.0006136,0.25,0.004696,0.75,0.00616,0.75,0.00052,0.75,0.00653,0.25,0.000315832,0.75,0.0003429,0.75,8.520000000000001e-05,0.75,0.0002652,0.75,0.0003832
hellaswag.val.8001,claude-v1,1.0,0.00188,1.0,7.02e-05,1.0,0.000188,1.0,0.00188,1.0,0.00188,0.0,0.000236,1.0,0.00238,1.0,0.000181584,0.0,0.0002097,1.0,4.6800000000000006e-05,1.0,0.0001404,1.0,0.0001864
grade-school-math.dev.7042,gpt-4-1106-preview,0.75,0.00703,0.75,0.0001209,0.75,0.000524,0.5,0.004088,0.75,0.005,0.75,0.000385,0.75,0.00703,0.75,0.0002948799999999,0.75,0.0003249,0.75,7.520000000000001e-05,0.75,0.0002334,0.25,0.0003456
mmlu-professional-medicine.val.218,WizardLM/WizardLM-13B-V1.2,1.0,4.5e-05,1.0,4.5e-05,1.0,0.0001208,1.0,0.0012079999999999,1.0,0.0012079999999999,1.0,0.00015,1.0,0.00151,0.0,0.0001164,0.0,0.000135,1.0,3e-05,1.0,9e-05,1.0,0.0001192
mmlu-security-studies.val.6,claude-v2,1.0,0.001072,1.0,3.99e-05,0.0,0.0001072,1.0,0.001072,1.0,0.001072,1.0,0.000133,1.0,0.00134,0.0,0.000103208,0.0,0.0001197,0.0,2.6600000000000003e-05,1.0,7.98e-05,1.0,0.0001064
hellaswag.val.9903,claude-v1,1.0,0.002464,1.0,9.18e-05,1.0,0.0002463999999999,1.0,0.002464,1.0,0.002464,1.0,0.000307,1.0,0.00308,0.0,0.000238232,1.0,0.0002754,0.0,6.14e-05,0.0,0.0001842,1.0,0.0002448
mtbench-math.dev.17,gpt-4-1106-preview,1.0,0.01193,0.1,0.0004098,1.0,0.001,1.0,0.006904,1.0,0.006256,1.0,0.000427,1.0,0.01193,1.0,0.000173048,0.1,0.0002277,0.4,5.640000000000001e-05,0.4,0.0003396,1.0,0.0003848
consensus_summary.dev.4,gpt-4-1106-preview,1.0,0.0026999999999999,0.75,9.03e-05,1.0,0.0007296,1.0,0.001968,0.75,0.005064,0.0,0.000308,1.0,0.0026999999999999,0.75,0.000276256,0.75,0.0002763,0.75,6.58e-05,0.75,0.0001824,1.0,0.0002216
grade-school-math.dev.2764,meta/llama-2-70b-chat,0.25,0.0003789,0.5,0.0001758,0.75,0.0005912,0.75,0.005048,0.75,0.0056,0.75,0.000739,0.5,0.00898,0.25,0.000339112,0.25,0.0003789,0.75,0.0001018,0.75,0.0002592,0.5,0.0004192
mmlu-high-school-chemistry.val.120,gpt-4-1106-preview,1.0,0.00067,0.0,1.98e-05,0.0,5.36e-05,0.0,0.000536,0.0,0.000536,0.0,6.599999999999999e-05,1.0,0.00067,0.0,5.1216000000000006e-05,0.0,5.940000000000001e-05,0.0,1.32e-05,0.0,3.96e-05,0.0,5.2e-05
winogrande.dev.569,claude-v1,0.0,0.000408,0.0,1.5e-05,0.0,4.08e-05,0.0,0.000408,1.0,0.000408,1.0,5.2e-05,1.0,0.00054,1.0,3.880000000000001e-05,0.0,4.5e-05,1.0,1e-05,1.0,3e-05,1.0,3.92e-05
grade-school-math.dev.4207,gpt-4-1106-preview,0.75,0.00748,0.25,0.0001509,0.75,0.0004784,0.75,0.003944,0.75,0.00524,0.75,0.000583,0.75,0.00748,0.25,0.000295656,0.25,0.0003123,0.25,7.520000000000001e-05,0.75,0.0002508,0.75,0.0003336
mmlu-college-physics.val.70,gpt-4-1106-preview,1.0,0.00147,0.0,4.29e-05,0.0,0.0001152,0.0,0.001152,0.0,0.001152,1.0,0.000143,1.0,0.00147,0.0,0.000110968,0.0,0.0001286999999999,0.0,2.8600000000000004e-05,0.0,8.58e-05,0.0,0.0001144
hellaswag.val.9251,claude-v1,1.0,0.00196,0.0,7.319999999999999e-05,1.0,0.000196,1.0,0.00196,1.0,0.00196,1.0,0.000246,1.0,0.00245,0.0,0.0001893439999999,0.0,0.0002196,0.0,4.880000000000001e-05,0.0,0.0001463999999999,0.0,0.0001944
hellaswag.val.1324,mistralai/mistral-7b-chat,1.0,2.9e-05,0.0,4.35e-05,0.0,0.0001168,0.0,0.001168,0.0,0.001168,1.0,0.000147,1.0,0.00149,0.0,0.00011252,0.0,0.0001296,1.0,2.9e-05,1.0,8.7e-05,0.0,0.0001152
arc-challenge.val.4,meta/llama-2-70b-chat,1.0,9.09e-05,1.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.000103,1.0,0.00102,0.0,7.8376e-05,1.0,9.09e-05,1.0,2.02e-05,1.0,6.06e-05,1.0,8e-05
mmlu-professional-medicine.val.156,WizardLM/WizardLM-13B-V1.2,1.0,0.0001092,1.0,0.0001092,0.0,0.000292,0.0,0.00292,0.0,0.00292,0.0,0.000364,1.0,0.00368,0.0,0.000282464,0.0,0.0003276,0.0,7.280000000000001e-05,0.0,0.0002184,0.0,0.0002904
hellaswag.val.9051,meta/llama-2-70b-chat,0.0,0.0001952999999999,0.0,6.51e-05,0.0,0.0001768,0.0,0.0017439999999999,0.0,0.0017439999999999,0.0,0.0002189999999999,0.0,0.00221,0.0,0.0001683919999999,0.0,0.0001952999999999,0.0,4.340000000000001e-05,0.0,0.0001302,0.0,0.0001728
mmlu-elementary-mathematics.val.252,mistralai/mixtral-8x7b-chat,1.0,5.7e-05,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,1.0,7.520000000000001e-05
mmlu-professional-law.val.1118,gpt-4-1106-preview,0.0,0.00208,0.0,6.21e-05,0.0,0.0001664,0.0,0.001664,0.0,0.001664,0.0,0.0002089999999999,0.0,0.00208,0.0,0.000160632,0.0,0.0001863,1.0,4.14e-05,1.0,0.0001242,0.0,0.0001656
hellaswag.val.2979,gpt-4-1106-preview,1.0,0.00111,1.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,1.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,1.0,9.81e-05,0.0,2.2e-05,1.0,6.6e-05,1.0,8.72e-05
grade-school-math.dev.6403,gpt-4-1106-preview,0.75,0.01031,0.25,0.0002007,0.75,0.0007,0.5,0.006136,0.5,0.007144,0.75,0.000664,0.75,0.01031,0.25,0.00039964,0.25,0.0004437,0.25,0.000102,0.5,0.0003509999999999,0.25,0.00054
grade-school-math.dev.4083,gpt-4-1106-preview,0.75,0.01023,0.25,0.000153,0.25,0.0005688,0.25,0.004464,0.75,0.006216,0.25,0.000523,0.75,0.01023,0.25,0.000332904,0.25,0.0003771,0.25,9.9e-05,0.25,0.000279,0.5,0.0004608
mmlu-high-school-mathematics.val.25,claude-v1,0.0,0.000816,0.0,3.03e-05,0.0,8.16e-05,0.0,0.000816,0.0,0.000816,0.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,0.0,6.06e-05,0.0,8.08e-05
mmlu-professional-law.val.373,claude-v1,0.0,0.002296,0.0,8.58e-05,0.0,0.0002296,0.0,0.002296,0.0,0.002296,0.0,0.000286,0.0,0.00287,0.0,0.000221936,0.0,0.0002573999999999,0.0,5.720000000000001e-05,0.0,0.0001716,0.0,0.000228
mbpp.dev.329,WizardLM/WizardLM-13B-V1.2,0.0,7.649999999999999e-05,0.0,7.649999999999999e-05,1.0,0.0005352,1.0,0.006408,0.0,0.008088,0.0,0.0002619999999999,0.0,0.0091799999999999,1.0,0.0002095199999999,0.0,0.0002331,0.0,5.06e-05,0.0,0.0002993999999999,0.0,0.0002064
mtbench.dev.11,mistralai/mistral-7b-chat,0.5,4.22e-05,0.7,8.639999999999999e-05,0.8,0.0004847999999999,0.8,0.004008,0.8,0.005784,0.9,0.0003019999999999,0.5,0.00876,0.5,0.000231248,0.7,0.0005157,0.5,4.22e-05,0.9,0.0002832,0.3,0.0004928
mmlu-nutrition.val.74,claude-v1,0.0,0.000632,0.0,2.3400000000000003e-05,0.0,6.32e-05,0.0,0.000632,1.0,0.000632,1.0,7.8e-05,1.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.240000000000001e-05
mmlu-professional-law.val.135,gpt-4-1106-preview,1.0,0.00235,0.0,7.02e-05,0.0,0.000188,0.0,0.00188,0.0,0.00188,0.0,0.000234,1.0,0.00235,0.0,0.000181584,0.0,0.0002106,1.0,4.6800000000000006e-05,0.0,0.0001404,0.0,0.0001864
mmlu-nutrition.val.6,meta/llama-2-70b-chat,0.0,7.83e-05,1.0,2.61e-05,1.0,7.04e-05,1.0,0.000704,1.0,0.000704,1.0,8.7e-05,1.0,0.00091,0.0,6.751200000000001e-05,0.0,7.83e-05,1.0,1.74e-05,1.0,5.22e-05,1.0,6.96e-05
mmlu-professional-law.val.536,gpt-4-1106-preview,1.0,0.00301,1.0,8.999999999999999e-05,0.0,0.0002408,1.0,0.002408,1.0,0.002408,0.0,0.0003,1.0,0.00301,0.0,0.0002328,0.0,0.00027,0.0,6e-05,0.0,0.0001799999999999,0.0,0.0002392
mmlu-nutrition.val.293,gpt-4-1106-preview,1.0,0.00115,0.0,3.4200000000000005e-05,0.0,9.2e-05,1.0,0.00092,1.0,0.00092,1.0,0.0001139999999999,1.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001026,0.0,2.28e-05,1.0,6.840000000000001e-05,0.0,9.12e-05
grade-school-math.dev.7036,meta/llama-2-70b-chat,0.5,0.0003222,0.5,0.0001629,0.5,0.0005376,0.5,0.005016,0.75,0.005256,0.5,0.000369,0.5,0.00726,0.5,0.000277032,0.5,0.0003222,0.75,7.48e-05,0.5,0.0002676,0.75,0.000396
hellaswag.val.8764,meta/llama-2-70b-chat,0.0,0.000243,0.0,8.13e-05,1.0,0.0002176,0.0,0.002176,0.0,0.002176,0.0,0.000271,1.0,0.00272,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
grade-school-math.dev.7434,gpt-4-1106-preview,0.75,0.00755,0.75,0.0001305,0.75,0.0005152,0.75,0.004384,0.75,0.00448,0.75,0.000441,0.75,0.00755,0.75,0.000326696,0.5,0.0003429,0.75,8.42e-05,0.75,0.0002538,0.75,0.0003784
hellaswag.val.6211,meta/code-llama-instruct-34b-chat,0.0,0.00019788,0.0,7.649999999999999e-05,0.0,0.0002072,1.0,0.002048,1.0,0.002048,1.0,0.000257,1.0,0.00256,0.0,0.00019788,1.0,0.0002295,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-high-school-european-history.val.5,WizardLM/WizardLM-13B-V1.2,0.0,0.0001532999999999,0.0,0.0001532999999999,1.0,0.0004096,1.0,0.004096,0.0,0.004096,1.0,0.000511,1.0,0.0051199999999999,0.0,0.000396536,0.0,0.0004599,1.0,0.0001022,1.0,0.000306,1.0,0.000408
hellaswag.val.5883,claude-v1,1.0,0.002448,0.0,9.15e-05,1.0,0.0002448,1.0,0.002448,1.0,0.002448,0.0,0.000307,1.0,0.00309,0.0,0.00023668,1.0,0.0002736,0.0,6.1000000000000005e-05,0.0,0.0001829999999999,1.0,0.0002432
mmlu-high-school-statistics.val.156,gpt-4-1106-preview,1.0,0.00119,0.0,3.54e-05,0.0,9.52e-05,1.0,0.000952,1.0,0.000952,1.0,0.000118,1.0,0.00119,0.0,9.1568e-05,0.0,0.0001062,0.0,2.36e-05,1.0,7.08e-05,1.0,9.44e-05
arc-challenge.val.147,gpt-4-1106-preview,1.0,0.00078,1.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,0.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,1.0,6.93e-05,0.0,1.54e-05,0.0,4.6200000000000005e-05,1.0,6.08e-05
mmlu-electrical-engineering.val.48,WizardLM/WizardLM-13B-V1.2,0.0,2.25e-05,0.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,1.0,7.5e-05,0.0,0.00079,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,0.0,6e-05
mmlu-jurisprudence.val.72,meta/llama-2-70b-chat,0.0,6.840000000000001e-05,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,0.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
hellaswag.val.263,mistralai/mixtral-8x7b-chat,1.0,8.1e-05,1.0,4.05e-05,1.0,0.0001088,1.0,0.001088,1.0,0.001088,1.0,0.000135,1.0,0.00136,0.0,0.0001047599999999,1.0,0.0001206,0.0,2.7e-05,1.0,8.1e-05,1.0,0.0001072
hellaswag.val.6188,claude-v1,1.0,0.002192,1.0,8.159999999999999e-05,1.0,0.0002216,1.0,0.002192,1.0,0.002192,1.0,0.0002749999999999,1.0,0.00277,1.0,0.0002118479999999,1.0,0.0002457,1.0,5.4600000000000006e-05,1.0,0.0001638,1.0,0.0002176
mmlu-professional-law.val.463,gpt-4-1106-preview,1.0,0.00324,0.0,9.69e-05,1.0,0.0002592,0.0,0.002592,0.0,0.002592,0.0,0.000323,1.0,0.00324,0.0,0.000250648,0.0,0.0002907,1.0,6.46e-05,0.0,0.0001938,0.0,0.0002576
grade-school-math.dev.5044,meta/llama-2-70b-chat,0.75,0.0003123,0.75,0.0001467,0.75,0.0004464,0.75,0.0038399999999999,0.75,0.005592,0.75,0.000542,0.5,0.00711,0.75,0.00029876,0.75,0.0003123,0.75,8.560000000000001e-05,0.75,0.0002399999999999,0.75,0.0003472
grade-school-math.dev.1935,gpt-4-1106-preview,0.75,0.0108,0.25,0.0002034,0.25,0.0008472,0.25,0.009768,0.5,0.0106559999999999,0.25,0.000715,0.75,0.0108,0.25,0.00041128,0.25,0.0004752,0.25,9.82e-05,0.75,0.0002826,0.25,0.0005248
grade-school-math.dev.6199,gpt-4-1106-preview,0.75,0.00695,0.25,0.0001442999999999,0.5,0.000436,0.75,0.004312,0.5,0.005464,0.75,0.00041,0.75,0.00695,0.25,0.000305744,0.5,0.0003411,0.5,6.16e-05,0.25,0.0003006,0.25,0.0005288
mmlu-human-sexuality.val.52,meta/llama-2-70b-chat,0.0,6.57e-05,0.0,2.19e-05,1.0,5.92e-05,0.0,0.000592,0.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,0.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,0.0,5.84e-05
hellaswag.val.1188,mistralai/mistral-7b-chat,0.0,2.28e-05,0.0,3.4200000000000005e-05,1.0,9.2e-05,0.0,0.00092,0.0,0.00092,1.0,0.0001139999999999,1.0,0.00115,0.0,8.846400000000001e-05,1.0,0.0001026,0.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
mmlu-high-school-microeconomics.val.63,meta/llama-2-70b-chat,0.0,7.920000000000001e-05,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,1.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,7.039999999999999e-05
mmlu-prehistory.val.309,meta/llama-2-70b-chat,0.0,9.63e-05,1.0,3.21e-05,1.0,8.64e-05,0.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.56e-05
grade-school-math.dev.7184,gpt-4-1106-preview,0.75,0.01001,0.25,0.000156,0.75,0.0006544,0.25,0.005272,0.75,0.00568,0.25,0.000651,0.75,0.01001,0.25,0.000374808,0.25,0.000369,0.25,9.34e-05,0.25,0.0003558,0.75,0.0006336
mmlu-business-ethics.val.6,meta/llama-2-70b-chat,0.0,8.370000000000002e-05,1.0,2.79e-05,0.0,7.52e-05,1.0,0.000752,0.0,0.000752,1.0,9.3e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,0.0,7.439999999999999e-05
hellaswag.val.790,meta/llama-2-70b-chat,0.0,0.0001016999999999,1.0,3.4200000000000005e-05,0.0,9.2e-05,1.0,0.00092,0.0,0.00092,1.0,0.000116,1.0,0.00115,0.0,8.846400000000001e-05,0.0,0.0001016999999999,1.0,2.28e-05,1.0,6.840000000000001e-05,1.0,9.040000000000002e-05
winogrande.dev.14,claude-v1,1.0,0.000464,1.0,1.7100000000000002e-05,0.0,4.64e-05,1.0,0.000464,1.0,0.000464,1.0,5.9e-05,1.0,0.00061,1.0,4.4232e-05,0.0,5.04e-05,1.0,1.14e-05,1.0,3.3600000000000004e-05,1.0,4.56e-05
hellaswag.val.4973,claude-v1,1.0,0.0016879999999999,1.0,6.269999999999999e-05,0.0,0.0001712,1.0,0.0016879999999999,1.0,0.0016879999999999,0.0,0.0002119999999999,1.0,0.00214,1.0,0.00016296,1.0,0.0001881,1.0,4.2e-05,0.0,0.000126,1.0,0.0001672
arc-challenge.val.286,gpt-4-1106-preview,1.0,0.00085,1.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.6e-05,1.0,0.00085,0.0,6.5184e-05,1.0,7.56e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
arc-challenge.val.229,meta/llama-2-70b-chat,0.0,8.01e-05,0.0,2.67e-05,0.0,7.200000000000002e-05,0.0,0.00072,0.0,0.00072,0.0,9.1e-05,0.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,1.0,1.7800000000000002e-05,1.0,5.34e-05,0.0,7.040000000000002e-05
hellaswag.val.5617,claude-v1,0.0,0.002152,0.0,8.04e-05,0.0,0.0002176,0.0,0.002152,0.0,0.002152,1.0,0.00027,1.0,0.00272,0.0,0.0002079679999999,0.0,0.0002412,0.0,5.360000000000001e-05,0.0,0.0001608,0.0,0.0002136
grade-school-math.dev.2309,gpt-4-1106-preview,0.5,0.01234,0.25,0.000192,0.25,0.000824,0.5,0.00692,0.5,0.008936,0.75,0.000739,0.5,0.01234,0.25,0.000385672,0.25,0.0005454,0.25,0.0001218,0.25,0.000345,0.25,0.0004344
hellaswag.val.2550,claude-v1,1.0,0.00096,0.0,3.54e-05,1.0,9.600000000000002e-05,1.0,0.00096,0.0,0.00096,0.0,0.000121,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001061999999999,0.0,2.3800000000000003e-05,0.0,7.14e-05,1.0,9.440000000000002e-05
mmlu-marketing.val.6,meta/code-llama-instruct-34b-chat,0.0,6.285600000000001e-05,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00085,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
hellaswag.val.5231,claude-v1,1.0,0.001928,0.0,7.199999999999999e-05,1.0,0.0001928,1.0,0.001928,1.0,0.001928,1.0,0.000242,1.0,0.00244,0.0,0.00018624,1.0,0.0002151,0.0,4.8e-05,0.0,0.0001433999999999,1.0,0.0001911999999999
mmlu-high-school-microeconomics.val.173,gpt-4-1106-preview,1.0,0.00086,0.0,2.55e-05,1.0,6.88e-05,0.0,0.000688,1.0,0.000688,0.0,8.499999999999999e-05,1.0,0.00086,0.0,6.596e-05,0.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,0.0,6.8e-05
mmlu-professional-law.val.1081,gpt-4-1106-preview,1.0,0.00267,1.0,7.98e-05,1.0,0.0002136,1.0,0.002136,0.0,0.002136,1.0,0.000268,1.0,0.00267,0.0,0.0002064159999999,0.0,0.0002394,0.0,5.3200000000000006e-05,1.0,0.0001596,0.0,0.0002128
arc-challenge.test.923,WizardLM/WizardLM-13B-V1.2,0.0,3.06e-05,0.0,3.06e-05,0.0,8.240000000000001e-05,0.0,0.000824,0.0,0.000824,0.0,0.000104,1.0,0.00106,1.0,7.9152e-05,0.0,9.18e-05,1.0,2.04e-05,1.0,6.12e-05,0.0,8.080000000000001e-05
hellaswag.val.2275,gpt-4-1106-preview,0.0,0.00105,1.0,3.09e-05,1.0,8.400000000000001e-05,0.0,0.00084,1.0,0.00084,0.0,0.000104,0.0,0.00105,0.0,8.0704e-05,0.0,9.27e-05,0.0,2.08e-05,0.0,6.24e-05,0.0,8.240000000000001e-05
hellaswag.val.1847,claude-v1,1.0,0.000688,1.0,2.55e-05,1.0,6.88e-05,1.0,0.000688,1.0,0.000688,1.0,8.499999999999999e-05,1.0,0.00089,0.0,6.596e-05,1.0,7.65e-05,0.0,1.7e-05,1.0,5.1e-05,1.0,6.8e-05
grade-school-math.dev.3933,gpt-4-1106-preview,0.75,0.00881,0.5,0.0001722,0.75,0.0005704,0.75,0.004984,0.75,0.005608,0.75,0.000614,0.75,0.00881,0.5,0.000343768,0.5,0.0003834,0.5,8.76e-05,0.5,0.0002634,0.75,0.000436
grade-school-math.dev.4666,claude-v1,0.75,0.005592,0.75,0.0001598999999999,0.75,0.0004848,0.75,0.005592,0.75,0.004824,0.75,0.0004179999999999,0.75,0.00711,0.25,0.000305744,0.75,0.0003186,0.25,0.0001094,0.75,0.0002496,0.25,0.0002232
arc-challenge.test.75,meta/code-llama-instruct-34b-chat,0.0,9.0792e-05,1.0,3.51e-05,1.0,9.44e-05,1.0,0.000944,1.0,0.000944,1.0,0.000119,1.0,0.00118,0.0,9.0792e-05,1.0,0.0001053,0.0,2.34e-05,1.0,7.02e-05,1.0,9.28e-05
mbpp.dev.259,meta/llama-2-70b-chat,0.0,0.0003176999999999,0.0,7.05e-05,0.0,0.0004807999999999,0.0,0.003008,0.0,0.005624,0.0,0.000188,0.0,0.00988,0.0,0.000156752,0.0,0.0003176999999999,0.0,4.4e-05,0.0,0.0002142,0.0,0.0002872
hellaswag.val.8816,claude-v1,1.0,0.002224,1.0,8.31e-05,1.0,0.0002248,1.0,0.002224,1.0,0.002224,0.0,0.000277,1.0,0.00278,1.0,0.000214952,1.0,0.0002484,1.0,5.5400000000000005e-05,1.0,0.0001662,1.0,0.0002208
mmlu-electrical-engineering.val.10,gpt-4-1106-preview,1.0,0.0009699999999999,1.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.68e-05
hellaswag.val.863,claude-v1,1.0,0.000864,0.0,3.18e-05,1.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000109,1.0,0.00108,0.0,8.3032e-05,0.0,9.54e-05,0.0,2.14e-05,0.0,6.42e-05,1.0,8.48e-05
grade-school-math.dev.3733,meta/llama-2-70b-chat,0.75,0.0004401,0.5,0.0001938,0.75,0.000656,0.75,0.0064399999999999,0.75,0.007856,0.75,0.000736,0.75,0.01102,0.75,0.000385672,0.75,0.0004401,0.75,0.0001214,0.25,0.000312,0.75,0.0004816
mmlu-miscellaneous.val.523,meta/llama-2-70b-chat,0.0,6.66e-05,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,1.0,1.48e-05,1.0,4.44e-05,1.0,5.84e-05
arc-challenge.val.37,meta/llama-2-70b-chat,1.0,0.0001305,0.0,4.35e-05,1.0,0.0001168,1.0,0.001168,1.0,0.001168,1.0,0.000147,1.0,0.00149,0.0,0.00011252,1.0,0.0001305,0.0,2.9e-05,1.0,8.7e-05,1.0,0.0001152
hellaswag.val.7254,claude-v1,0.0,0.002032,0.0,7.56e-05,1.0,0.0002032,0.0,0.002032,0.0,0.002032,0.0,0.0002549999999999,1.0,0.00257,0.0,0.000196328,0.0,0.0002268,0.0,5.06e-05,0.0,0.0001518,0.0,0.0002016
grade-school-math.dev.4043,gpt-4-1106-preview,0.75,0.00913,0.25,0.0001557,0.75,0.000548,0.75,0.0062239999999999,0.75,0.006752,0.75,0.0004759999999999,0.75,0.00913,0.25,0.00039964,0.75,0.0003231,0.25,7.6e-05,0.25,0.0001931999999999,0.75,0.0004904
hellaswag.val.9382,claude-v1,1.0,0.0018239999999999,1.0,6.81e-05,1.0,0.0001824,1.0,0.0018239999999999,1.0,0.0018239999999999,0.0,0.0002289999999999,1.0,0.00231,1.0,0.0001761519999999,1.0,0.0002042999999999,1.0,4.5400000000000006e-05,1.0,0.0001362,1.0,0.0001808
hellaswag.val.7332,claude-v1,0.0,0.002152,0.0,8.01e-05,1.0,0.0002152,0.0,0.002152,0.0,0.002152,0.0,0.00027,1.0,0.00269,0.0,0.0002079679999999,0.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
hellaswag.val.8211,meta/code-llama-instruct-34b-chat,0.0,0.00021728,0.0,8.369999999999999e-05,0.0,0.0002296,0.0,0.002248,0.0,0.002248,1.0,0.0002819999999999,1.0,0.00281,0.0,0.00021728,0.0,0.000252,0.0,5.6000000000000006e-05,1.0,0.000168,0.0,0.0002232
mmlu-high-school-psychology.val.212,meta/llama-2-70b-chat,0.0,0.0001116,1.0,3.72e-05,1.0,0.0001,1.0,0.001,1.0,0.001,1.0,0.000124,1.0,0.00125,0.0,9.6224e-05,0.0,0.0001116,1.0,2.4800000000000003e-05,1.0,7.44e-05,1.0,9.840000000000002e-05
grade-school-math.dev.216,gpt-4-1106-preview,0.75,0.01012,0.75,0.0001983,0.75,0.0005672,0.25,0.010184,0.75,0.00788,0.75,0.000728,0.75,0.01012,0.75,0.000389552,0.25,0.0004310999999999,0.75,0.0001104,0.25,0.00033,0.75,0.0005256
mmlu-professional-law.val.1203,gpt-4-1106-preview,1.0,0.0042,0.0,0.0001257,1.0,0.000336,1.0,0.00336,1.0,0.00336,0.0,0.000419,1.0,0.0042,0.0,0.000325144,0.0,0.0003771,0.0,8.38e-05,0.0,0.0002514,1.0,0.0003344
mmlu-moral-disputes.val.252,gpt-4-1106-preview,1.0,0.00127,0.0,3.78e-05,0.0,0.0001016,0.0,0.001016,0.0,0.001016,0.0,0.000126,1.0,0.00127,0.0,9.7776e-05,0.0,0.0001134,0.0,2.52e-05,0.0,7.56e-05,0.0,0.0001008
hellaswag.val.3480,meta/llama-2-70b-chat,0.0,0.0002583,0.0,8.64e-05,0.0,0.0002312,0.0,0.002312,0.0,0.002312,0.0,0.000288,0.0,0.00289,0.0,0.0002234879999999,0.0,0.0002583,0.0,5.74e-05,0.0,0.0001728,0.0,0.0002296
mmlu-college-biology.val.8,gpt-4-1106-preview,1.0,0.00099,0.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00099,0.0,7.604800000000001e-05,0.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
grade-school-math.dev.972,gpt-4-1106-preview,0.75,0.00701,0.75,0.0001074,0.25,0.0002296,0.75,0.005608,0.75,0.0047919999999999,0.75,0.000403,0.75,0.00701,0.75,0.000277808,0.75,0.0003231,0.25,0.000106,0.75,0.0002544,0.75,0.0003328
mmlu-high-school-chemistry.val.59,gpt-4-1106-preview,1.0,0.0011799999999999,0.0,3.4200000000000005e-05,0.0,9.2e-05,0.0,0.00092,0.0,0.00092,1.0,0.0001139999999999,1.0,0.0011799999999999,0.0,8.846400000000001e-05,0.0,0.0001026,1.0,2.28e-05,1.0,6.840000000000001e-05,0.0,9.12e-05
grade-school-math.dev.3228,gpt-4-1106-preview,0.75,0.00826,0.75,0.0001674,0.75,0.0005624,0.75,0.00476,0.75,0.005672,0.75,0.000504,0.75,0.00826,0.75,0.0003127279999999,0.25,0.0003915,0.25,7.060000000000001e-05,0.75,0.0002466,0.75,0.0003192
mmlu-machine-learning.val.50,gpt-4-1106-preview,1.0,0.0009,1.0,2.58e-05,0.0,6.960000000000001e-05,1.0,0.000696,1.0,0.000696,1.0,8.599999999999999e-05,1.0,0.0009,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,0.0,5.16e-05,1.0,6.88e-05
mmlu-professional-medicine.val.154,WizardLM/WizardLM-13B-V1.2,1.0,5.31e-05,1.0,5.31e-05,1.0,0.0001424,1.0,0.001424,1.0,0.001424,1.0,0.000177,1.0,0.00181,0.0,0.000137352,0.0,0.0001593,1.0,3.54e-05,1.0,0.0001062,1.0,0.0001407999999999
mmlu-moral-scenarios.val.66,claude-v1,0.0,0.0010559999999999,0.0,3.93e-05,0.0,0.0001056,0.0,0.0010559999999999,0.0,0.0010559999999999,0.0,0.0001309999999999,0.0,0.00132,0.0,0.000101656,0.0,0.0001179,0.0,2.62e-05,0.0,7.86e-05,0.0,0.000104
mmlu-high-school-government-and-politics.val.59,meta/llama-2-70b-chat,0.0,0.0001071,1.0,3.57e-05,0.0,9.600000000000002e-05,0.0,0.00096,0.0,0.00096,1.0,0.0001189999999999,1.0,0.0012,0.0,9.2344e-05,0.0,0.0001071,0.0,2.3800000000000003e-05,1.0,7.14e-05,0.0,9.52e-05
grade-school-math.dev.1113,gpt-4-1106-preview,0.75,0.01413,0.25,0.000165,0.25,0.0006528,0.75,0.0056159999999999,0.75,0.007464,0.25,0.000472,0.75,0.01413,0.25,0.0003429919999999,0.25,0.0003834,0.25,7.6e-05,0.25,0.000207,0.25,0.0004272
grade-school-math.dev.4814,gpt-4-1106-preview,0.75,0.00907,0.25,0.0001827,0.75,0.0006776,0.75,0.007232,0.75,0.007088,0.75,0.000626,0.75,0.00907,0.25,0.000418264,0.25,0.0003672,0.25,8.760000000000002e-05,0.25,0.0002232,0.75,0.0004464
mmlu-clinical-knowledge.val.65,meta/llama-2-70b-chat,0.0,7.740000000000001e-05,1.0,2.58e-05,0.0,6.960000000000001e-05,0.0,0.000696,0.0,0.000696,0.0,8.8e-05,0.0,0.0009,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,0.0,1.72e-05,0.0,5.16e-05,0.0,6.88e-05
mmlu-professional-law.val.653,gpt-4-1106-preview,0.0,0.0050199999999999,0.0,0.0001502999999999,0.0,0.0004016,0.0,0.004016,0.0,0.004016,0.0,0.000501,0.0,0.0050199999999999,0.0,0.0003887759999999,0.0,0.0004509,1.0,0.0001002,0.0,0.0003005999999999,0.0,0.0004
mbpp.dev.122,WizardLM/WizardLM-13B-V1.2,1.0,0.0001010999999999,1.0,0.0001010999999999,1.0,0.0006183999999999,1.0,0.004216,0.0,0.006232,1.0,0.000378,1.0,0.0143299999999999,0.0,0.000132696,0.0,0.000297,0.0,7.400000000000001e-05,1.0,0.0001974,0.0,0.0002696
hellaswag.val.6276,claude-v1,1.0,0.002264,1.0,8.37e-05,0.0,0.000224,1.0,0.002264,0.0,0.00224,0.0,0.000279,1.0,0.00283,1.0,0.0002165039999999,0.0,0.0002502,1.0,5.580000000000001e-05,1.0,0.0001674,1.0,0.0002224
grade-school-math.dev.5657,gpt-4-1106-preview,0.75,0.00765,0.75,0.0001281,0.75,0.0004056,0.75,0.003864,0.75,0.00492,0.75,0.000401,0.75,0.00765,0.25,0.000266168,0.75,0.0003096,0.75,7.66e-05,0.5,0.0002304,0.75,0.0003456
hellaswag.val.3013,mistralai/mistral-7b-chat,0.0,2.36e-05,0.0,3.54e-05,0.0,9.52e-05,0.0,0.000952,0.0,0.000952,1.0,0.000118,1.0,0.00119,0.0,9.1568e-05,0.0,0.0001062,0.0,2.36e-05,0.0,7.08e-05,1.0,9.44e-05
mmlu-professional-law.val.496,WizardLM/WizardLM-13B-V1.2,0.0,7.92e-05,0.0,7.92e-05,1.0,0.000212,1.0,0.00212,1.0,0.00212,1.0,0.000264,1.0,0.00265,0.0,0.000204864,0.0,0.0002376,0.0,5.280000000000001e-05,1.0,0.0001584,1.0,0.0002104
mmlu-miscellaneous.val.777,gpt-4-1106-preview,1.0,0.00081,0.0,2.4e-05,0.0,6.480000000000002e-05,0.0,0.000648,0.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,0.0,4.8e-05,1.0,6.400000000000001e-05
hellaswag.val.1740,mistralai/mistral-7b-chat,0.0,2.1e-05,0.0,3.12e-05,0.0,8.480000000000001e-05,0.0,0.000848,1.0,0.000848,1.0,0.0001049999999999,0.0,0.00106,0.0,8.148e-05,1.0,9.45e-05,0.0,2.1e-05,0.0,6.3e-05,0.0,8.320000000000002e-05
hellaswag.val.6582,meta/llama-2-70b-chat,1.0,0.000225,0.0,7.53e-05,0.0,0.0002016,0.0,0.002016,1.0,0.002016,1.0,0.000251,0.0,0.00255,0.0,0.000194776,1.0,0.000225,0.0,5.020000000000001e-05,0.0,0.0001506,1.0,0.0002
hellaswag.val.2211,mistralai/mistral-7b-chat,0.0,2.58e-05,0.0,3.8700000000000006e-05,0.0,0.000104,0.0,0.00104,0.0,0.00104,0.0,0.000129,0.0,0.0013,0.0,0.000100104,0.0,0.0001152,0.0,2.58e-05,0.0,7.740000000000001e-05,0.0,0.0001024
hellaswag.val.2767,mistralai/mistral-7b-chat,0.0,2.74e-05,0.0,4.08e-05,0.0,0.0001104,0.0,0.001104,0.0,0.001104,0.0,0.000137,0.0,0.00138,0.0,0.000106312,0.0,0.0001224,0.0,2.74e-05,0.0,8.22e-05,0.0,0.0001088
hellaswag.val.4667,claude-v1,1.0,0.001784,0.0,6.66e-05,0.0,0.0001808,1.0,0.001784,0.0,0.001784,0.0,0.000222,1.0,0.00223,0.0,0.0001722719999999,0.0,0.0001997999999999,0.0,4.44e-05,0.0,0.0001332,1.0,0.0001768
mmlu-high-school-world-history.val.105,meta/llama-2-70b-chat,0.0,0.000387,1.0,0.000129,0.0,0.0003448,0.0,0.003448,1.0,0.003448,0.0,0.00043,1.0,0.00431,0.0,0.00033368,0.0,0.000387,1.0,8.6e-05,1.0,0.000258,1.0,0.0003432
mmlu-security-studies.val.133,zero-one-ai/Yi-34B-Chat,1.0,8.48e-05,1.0,3.21e-05,0.0,8.64e-05,1.0,0.000864,1.0,0.000864,1.0,0.000107,1.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,1.0,6.42e-05,1.0,8.48e-05
hellaswag.val.3392,claude-v1,1.0,0.002248,0.0,8.369999999999999e-05,1.0,0.0002248,1.0,0.002248,1.0,0.002248,0.0,0.0002819999999999,0.0,0.00284,0.0,0.00021728,0.0,0.000252,0.0,5.6000000000000006e-05,0.0,0.000168,1.0,0.0002232
mmlu-astronomy.val.82,gpt-4-1106-preview,1.0,0.00129,1.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.8552e-05,0.0,0.0001152,1.0,2.56e-05,1.0,7.68e-05,1.0,0.0001016
hellaswag.val.9435,claude-v1,1.0,0.001896,0.0,7.08e-05,1.0,0.0001896,1.0,0.001896,1.0,0.001896,1.0,0.000236,1.0,0.0024,0.0,0.000183136,1.0,0.0002115,0.0,4.720000000000001e-05,0.0,0.0001416,1.0,0.000188
hellaswag.val.7064,claude-v1,0.0,0.002368,1.0,8.85e-05,1.0,0.0002392,0.0,0.002368,0.0,0.002368,0.0,0.000297,1.0,0.00299,1.0,0.0002289199999999,0.0,0.0002646,1.0,5.9e-05,1.0,0.000177,1.0,0.0002352
hellaswag.val.3232,mistralai/mistral-7b-chat,0.0,2.58e-05,0.0,3.8400000000000005e-05,0.0,0.000104,0.0,0.00104,0.0,0.00104,1.0,0.000129,1.0,0.0013,0.0,0.000100104,0.0,0.0001161,0.0,2.58e-05,0.0,7.740000000000001e-05,0.0,0.0001024
mmlu-high-school-geography.val.81,gpt-4-1106-preview,1.0,0.00096,1.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,1.0,1.9e-05,1.0,5.7e-05,1.0,7.520000000000001e-05
hellaswag.val.4450,claude-v1,0.0,0.002064,0.0,7.68e-05,0.0,0.0002088,0.0,0.002064,0.0,0.002064,1.0,0.000259,1.0,0.00258,0.0,0.0001994319999999,1.0,0.0002304,0.0,5.14e-05,0.0,0.0001542,1.0,0.0002048
grade-school-math.dev.798,meta/llama-2-70b-chat,0.25,0.0004608,0.25,0.0001998,0.75,0.0006872,0.75,0.006392,0.75,0.0062959999999999,0.5,0.000719,0.5,0.01126,0.25,0.000363168,0.25,0.0004608,0.25,9.38e-05,0.75,0.0003336,0.25,0.0005016
mmlu-elementary-mathematics.val.222,claude-v1,0.0,0.000904,0.0,3.3600000000000004e-05,1.0,9.04e-05,0.0,0.000904,0.0,0.000904,1.0,0.000112,0.0,0.00113,0.0,8.6912e-05,0.0,0.0001008,0.0,2.24e-05,1.0,6.720000000000001e-05,0.0,8.96e-05
mmlu-professional-law.val.1316,gpt-4-1106-preview,1.0,0.00206,0.0,6.149999999999999e-05,1.0,0.0001648,1.0,0.001648,1.0,0.001648,1.0,0.000205,1.0,0.00206,0.0,0.0001590799999999,0.0,0.0001845,0.0,4.100000000000001e-05,1.0,0.0001229999999999,0.0,0.000164
grade-school-math.dev.1491,gpt-4-1106-preview,0.75,0.0066999999999999,0.75,0.0001308,0.75,0.0003488,0.75,0.003752,0.75,0.004472,0.75,0.000376,0.75,0.0066999999999999,0.75,0.000242112,0.75,0.0003195,0.75,8.560000000000001e-05,0.75,0.0002112,0.75,0.00038
mmlu-medical-genetics.val.61,claude-v1,1.0,0.000736,1.0,2.73e-05,0.0,7.360000000000001e-05,1.0,0.000736,0.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,1.0,1.82e-05,1.0,5.46e-05,1.0,7.280000000000001e-05
hellaswag.val.8426,claude-v1,0.0,0.00216,0.0,8.07e-05,1.0,0.000216,0.0,0.00216,0.0,0.00216,0.0,0.000271,0.0,0.00273,0.0,0.000208744,0.0,0.0002421,0.0,5.380000000000001e-05,0.0,0.0001614,0.0,0.0002144
mmlu-high-school-chemistry.val.54,claude-v1,0.0,0.000864,0.0,3.21e-05,0.0,8.64e-05,0.0,0.000864,0.0,0.000864,0.0,0.000107,0.0,0.00108,0.0,8.3032e-05,0.0,9.63e-05,0.0,2.14e-05,0.0,6.42e-05,0.0,8.48e-05
mmlu-prehistory.val.90,claude-v1,1.0,0.000752,1.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.3e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.36e-05
grade-school-math.dev.2688,claude-v1,0.75,0.005624,0.75,0.0001532999999999,0.25,0.000848,0.75,0.005624,0.75,0.006656,0.75,0.00054,0.75,0.00865,0.5,0.000303416,0.25,0.0003465,0.25,0.0001154,0.25,0.0003017999999999,0.75,0.0004152
grade-school-math.dev.5533,gpt-4-1106-preview,0.5,0.00739,0.5,0.0001437,0.25,0.000608,0.25,0.004616,0.5,0.005336,0.5,0.000501,0.5,0.00739,0.25,0.000346872,0.5,0.0003429,0.0,7.620000000000001e-05,0.75,0.0002184,0.75,0.0003808
grade-school-math.dev.5776,gpt-4-1106-preview,0.75,0.01038,0.75,0.0001668,0.75,0.0006743999999999,0.75,0.0059039999999999,0.75,0.008568,0.75,0.000568,0.75,0.01038,0.25,0.000332128,0.75,0.0004122,0.75,8.319999999999999e-05,0.75,0.0002951999999999,0.75,0.0004144
hellaswag.val.6632,claude-v1,0.0,0.00224,1.0,8.37e-05,0.0,0.0002264,0.0,0.00224,0.0,0.00224,0.0,0.000279,1.0,0.0028,1.0,0.0002165039999999,1.0,0.0002511,1.0,5.580000000000001e-05,1.0,0.0001674,1.0,0.0002224
mmlu-miscellaneous.val.86,WizardLM/WizardLM-13B-V1.2,1.0,1.83e-05,1.0,1.83e-05,1.0,5.04e-05,1.0,0.000504,1.0,0.000504,1.0,6.2e-05,1.0,0.0006299999999999,0.0,4.8112e-05,0.0,5.58e-05,1.0,1.24e-05,1.0,3.72e-05,1.0,4.96e-05
mmlu-professional-law.val.481,meta/llama-2-70b-chat,0.0,0.0003887999999999,0.0,0.0001295999999999,1.0,0.0003464,0.0,0.003464,1.0,0.003464,0.0,0.000432,0.0,0.00433,0.0,0.000335232,0.0,0.0003887999999999,0.0,8.64e-05,0.0,0.0002591999999999,0.0,0.0003456
grade-school-math.dev.5160,gpt-4-1106-preview,0.75,0.0101,0.5,0.0001598999999999,0.75,0.000652,0.75,0.0055359999999999,0.75,0.00556,0.75,0.000595,0.75,0.0101,0.25,0.000471032,0.5,0.0003968999999999,0.75,9.06e-05,0.25,0.0002195999999999,0.5,0.0004392
mmlu-clinical-knowledge.val.49,WizardLM/WizardLM-13B-V1.2,1.0,2.4e-05,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,1.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
grade-school-math.dev.1842,meta/llama-2-70b-chat,0.25,0.0004275,0.25,0.0001865999999999,0.25,0.000596,0.25,0.005312,0.75,0.0056,0.75,0.000725,0.75,0.0113499999999999,0.25,0.000425248,0.25,0.0004275,0.25,9.74e-05,0.75,0.0003053999999999,0.25,0.0004272
mmlu-high-school-physics.val.67,claude-v1,0.0,0.000736,1.0,2.73e-05,0.0,7.360000000000001e-05,0.0,0.000736,0.0,0.000736,0.0,9.1e-05,0.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.46e-05,0.0,7.200000000000002e-05
grade-school-math.dev.5193,claude-v1,0.75,0.006416,0.75,0.0001983,0.75,0.0007375999999999,0.75,0.006416,0.75,0.007112,0.75,0.000607,0.75,0.00928,0.5,0.000467152,0.75,0.0005193,0.25,9.9e-05,0.75,0.0003006,0.25,0.0004424
hellaswag.val.8699,claude-v1,0.0,0.002144,0.0,8.01e-05,0.0,0.0002144,0.0,0.002144,0.0,0.002144,0.0,0.000269,0.0,0.00271,0.0,0.000207192,0.0,0.0002402999999999,0.0,5.34e-05,0.0,0.0001602,0.0,0.0002128
hellaswag.val.5211,claude-v1,1.0,0.0018319999999999,0.0,6.84e-05,1.0,0.0001856,1.0,0.0018319999999999,1.0,0.0018319999999999,1.0,0.0002299999999999,1.0,0.00229,0.0,0.000176928,1.0,0.0002052,0.0,4.56e-05,0.0,0.0001368,0.0,0.0001816
grade-school-math.dev.1009,meta/llama-2-70b-chat,0.25,0.0006462,0.25,0.0002538,0.75,0.0005143999999999,0.25,0.005816,0.75,0.0074719999999999,0.75,0.000464,0.75,0.00796,0.25,0.000338336,0.25,0.0006462,0.25,9.98e-05,0.75,0.0004169999999999,0.25,0.0004776
mmlu-professional-accounting.val.59,gpt-4-1106-preview,1.0,0.00112,0.0,3.33e-05,1.0,8.960000000000001e-05,1.0,0.000896,1.0,0.000896,1.0,0.000111,1.0,0.00112,0.0,8.6136e-05,0.0,9.990000000000002e-05,0.0,2.22e-05,1.0,6.659999999999999e-05,1.0,8.800000000000001e-05
mmlu-moral-disputes.val.153,meta/llama-2-70b-chat,0.0,9.45e-05,1.0,3.15e-05,0.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,0.0,2.1e-05,1.0,6.3e-05,1.0,8.400000000000001e-05
mmlu-professional-psychology.val.144,meta/code-llama-instruct-34b-chat,0.0,7.992800000000001e-05,1.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.160000000000002e-05
abstract2title.test.104,gpt-4-1106-preview,1.0,0.00406,1.0,0.000102,1.0,0.0003224,1.0,0.003704,1.0,0.00332,1.0,0.00036,1.0,0.00406,1.0,0.000276256,1.0,0.0004662,1.0,6.68e-05,1.0,0.0002058,1.0,0.000276
hellaswag.val.8980,claude-v1,0.0,0.002176,0.0,8.13e-05,1.0,0.0002176,0.0,0.002176,0.0,0.0021999999999999,1.0,0.0002729999999999,1.0,0.00272,0.0,0.0002102959999999,0.0,0.000243,0.0,5.420000000000001e-05,0.0,0.0001626,1.0,0.000216
mmlu-public-relations.val.85,claude-v1,1.0,0.001256,1.0,4.68e-05,1.0,0.0001256,1.0,0.001256,1.0,0.001256,1.0,0.000158,1.0,0.0015999999999999,0.0,0.000121056,0.0,0.0001403999999999,0.0,3.1200000000000006e-05,1.0,9.36e-05,1.0,0.000124
mmlu-medical-genetics.val.52,meta/code-llama-instruct-34b-chat,0.0,8.380800000000001e-05,1.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,0.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
mmlu-high-school-government-and-politics.val.11,meta/llama-2-70b-chat,0.0,7.290000000000001e-05,0.0,2.43e-05,0.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
bias_detection.dev.284,meta/llama-2-70b-chat,0.0,0.0003141,0.0,9.21e-05,0.0,0.000368,1.0,0.00344,0.0,0.004136,0.0,0.00035,0.0,0.00772,0.0,0.000241336,0.0,0.0003141,0.0,5.52e-05,0.0,0.0001914,0.0,0.0002528
bias_detection.dev.161,meta/llama-2-70b-chat,1.0,0.0002907,1.0,9.749999999999998e-05,1.0,0.0003208,1.0,0.0037359999999999,0.0,0.005536,1.0,0.000317,1.0,0.00875,0.0,0.0002374559999999,1.0,0.0002907,0.0,5.5400000000000005e-05,1.0,0.0001925999999999,1.0,0.0002616
hellaswag.val.10010,claude-v1,0.0,0.00164,0.0,6.12e-05,0.0,0.0001664,0.0,0.00164,0.0,0.00164,0.0,0.000206,1.0,0.00208,1.0,0.000158304,0.0,0.0001836,1.0,4.080000000000001e-05,1.0,0.0001224,1.0,0.0001632
grade-school-math.dev.4245,gpt-4-1106-preview,0.75,0.0092199999999999,0.25,0.0002031,0.75,0.0006799999999999,0.75,0.006704,0.75,0.00656,0.5,0.000546,0.75,0.0092199999999999,0.25,0.000340664,0.25,0.000387,0.25,7.92e-05,0.5,0.0002519999999999,0.5,0.000428
hellaswag.val.2557,meta/llama-2-70b-chat,1.0,9.18e-05,1.0,3.06e-05,1.0,8.240000000000001e-05,1.0,0.000824,1.0,0.000824,1.0,0.000102,1.0,0.00103,0.0,7.9152e-05,1.0,9.18e-05,0.0,2.04e-05,1.0,6.12e-05,1.0,8.080000000000001e-05
mmlu-professional-psychology.val.186,meta/llama-2-70b-chat,0.0,0.000108,0.0,3.6e-05,1.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.0001199999999999,1.0,0.00121,0.0,9.312e-05,0.0,0.000108,0.0,2.4e-05,1.0,7.2e-05,1.0,9.6e-05
grade-school-math.dev.6509,gpt-4-1106-preview,0.75,0.01023,0.25,0.0001725,0.25,0.0006743999999999,0.75,0.005136,0.75,0.005304,0.75,0.000597,0.75,0.01023,0.25,0.000353856,0.75,0.0003951,0.75,0.000103,0.75,0.0002676,0.75,0.0004352
mmlu-jurisprudence.val.107,WizardLM/WizardLM-13B-V1.2,1.0,3.51e-05,1.0,3.51e-05,1.0,9.44e-05,0.0,0.000944,1.0,0.000944,1.0,0.000117,1.0,0.00121,0.0,9.0792e-05,0.0,0.0001053,1.0,2.34e-05,1.0,7.02e-05,1.0,9.28e-05
mmlu-high-school-european-history.val.11,gpt-4-1106-preview,1.0,0.00322,0.0,9.63e-05,0.0,0.0002576,0.0,0.002576,1.0,0.002576,0.0,0.000321,1.0,0.00322,0.0,0.000249096,0.0,0.0002889,0.0,6.42e-05,0.0,0.0001926,0.0,0.0002568
hellaswag.val.7994,claude-v1,0.0,0.0022,0.0,8.219999999999999e-05,0.0,0.00022,0.0,0.0022,1.0,0.0022,1.0,0.000274,1.0,0.00275,0.0,0.000212624,0.0,0.0002466,0.0,5.480000000000001e-05,0.0,0.0001643999999999,0.0,0.0002184
mbpp.dev.384,WizardLM/WizardLM-13B-V1.2,1.0,8.13e-05,1.0,8.13e-05,1.0,0.0006752,1.0,0.0046879999999999,1.0,0.005624,1.0,0.00042,1.0,0.00913,1.0,0.00018624,1.0,0.0003816,0.0,5.14e-05,1.0,0.0001686,0.0,0.0002256
mmlu-professional-law.val.1272,zero-one-ai/Yi-34B-Chat,1.0,0.0002512,0.0,9.42e-05,1.0,0.000252,1.0,0.00252,1.0,0.00252,1.0,0.000314,1.0,0.00315,0.0,0.000243664,0.0,0.0002826,0.0,6.280000000000001e-05,1.0,0.0001884,1.0,0.0002512
mmlu-international-law.val.12,claude-v1,1.0,0.0012959999999999,1.0,4.83e-05,1.0,0.0001296,1.0,0.0012959999999999,1.0,0.0012959999999999,1.0,0.0001609999999999,1.0,0.00162,0.0,0.000124936,0.0,0.0001448999999999,0.0,3.2200000000000003e-05,1.0,9.66e-05,1.0,0.000128
mmlu-professional-law.val.403,meta/llama-2-70b-chat,0.0,9e-05,0.0,3e-05,1.0,8.080000000000001e-05,0.0,0.000808,1.0,0.000808,0.0,0.0001,0.0,0.00101,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,1.0,6e-05,0.0,8e-05
hellaswag.val.489,mistralai/mistral-7b-chat,0.0,2.3e-05,0.0,3.45e-05,1.0,9.28e-05,1.0,0.000928,1.0,0.000928,1.0,0.0001149999999999,1.0,0.0011899999999999,0.0,8.924e-05,1.0,0.0001026,0.0,2.3e-05,0.0,6.9e-05,1.0,9.12e-05
mmlu-professional-law.val.234,gpt-4-1106-preview,1.0,0.00306,0.0,9.15e-05,1.0,0.0002448,1.0,0.002448,1.0,0.002448,0.0,0.000305,1.0,0.00306,0.0,0.00023668,0.0,0.0002745,0.0,6.1000000000000005e-05,1.0,0.0001829999999999,1.0,0.0002432
hellaswag.val.4466,claude-v1,1.0,0.002104,0.0,7.86e-05,0.0,0.0002128,1.0,0.002104,1.0,0.002104,1.0,0.0002639999999999,1.0,0.00266,0.0,0.000203312,1.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
mmlu-high-school-macroeconomics.val.316,gpt-4-1106-preview,1.0,0.00104,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.240000000000001e-05
bias_detection.dev.27,meta/code-llama-instruct-34b-chat,0.0,0.0002312479999999,0.0,9.42e-05,0.0,0.000276,0.0,0.004464,0.0,0.004464,0.0,0.000319,0.0,0.00489,0.0,0.0002312479999999,0.0,0.000279,0.0,6.88e-05,0.0,0.000174,0.0,0.0001936
mmlu-moral-scenarios.val.391,meta/llama-2-70b-chat,0.0,0.0001286999999999,0.0,4.29e-05,0.0,0.0001152,0.0,0.001152,0.0,0.001152,0.0,0.000143,1.0,0.00147,0.0,0.000110968,0.0,0.0001286999999999,0.0,2.8600000000000004e-05,0.0,8.58e-05,1.0,0.0001136
mmlu-professional-psychology.val.123,gpt-4-1106-preview,1.0,0.00124,1.0,3.69e-05,0.0,9.92e-05,1.0,0.000992,0.0,0.000992,0.0,0.000123,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001107,0.0,2.46e-05,0.0,7.379999999999999e-05,0.0,9.84e-05
mmlu-professional-law.val.390,gpt-4-1106-preview,1.0,0.00277,0.0,8.280000000000001e-05,1.0,0.0002216,0.0,0.002216,1.0,0.002216,0.0,0.000276,1.0,0.00277,0.0,0.0002141759999999,0.0,0.0002483999999999,0.0,5.520000000000001e-05,0.0,0.0001656,1.0,0.00022
grade-school-math.dev.6830,meta/llama-2-70b-chat,0.25,0.000504,0.25,0.000195,0.75,0.0006432,0.25,0.006168,0.25,0.007464,0.75,0.000647,0.75,0.01155,0.75,0.000366272,0.25,0.000504,0.75,8.24e-05,0.75,0.0002856,0.75,0.0005231999999999
mmlu-us-foreign-policy.val.62,WizardLM/WizardLM-13B-V1.2,0.0,3e-05,0.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
winogrande.dev.95,claude-v1,1.0,0.000368,0.0,1.35e-05,1.0,3.679999999999999e-05,1.0,0.000368,1.0,0.000368,0.0,4.5e-05,1.0,0.00049,0.0,3.4920000000000004e-05,1.0,4.050000000000001e-05,0.0,8.999999999999999e-06,0.0,2.7e-05,0.0,3.5199999999999995e-05
grade-school-math.dev.3870,gpt-4-1106-preview,0.75,0.00858,0.75,0.0001619999999999,0.75,0.0005495999999999,0.75,0.005208,0.5,0.00564,0.5,0.000753,0.75,0.00858,0.25,0.00029488,0.25,0.0003987,0.75,8.38e-05,1.0,0.0001877999999999,0.75,0.0003272
hellaswag.val.870,mistralai/mistral-7b-chat,0.0,1.8e-05,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,9.2e-05,1.0,0.00091,0.0,6.984e-05,1.0,8.1e-05,0.0,1.8e-05,1.0,5.4e-05,1.0,7.120000000000001e-05
mmlu-clinical-knowledge.val.8,meta/code-llama-instruct-34b-chat,0.0,7.682400000000001e-05,1.0,2.97e-05,1.0,8e-05,1.0,0.0008,1.0,0.0008,1.0,9.9e-05,1.0,0.001,0.0,7.682400000000001e-05,0.0,8.91e-05,1.0,1.98e-05,1.0,5.94e-05,1.0,7.840000000000001e-05
mmlu-high-school-geography.val.98,meta/llama-2-70b-chat,0.0,7.38e-05,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00086,0.0,6.3632e-05,0.0,7.38e-05,1.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
bias_detection.dev.140,meta/llama-2-70b-chat,0.0,0.0005112,1.0,0.0001083,0.0,0.0003551999999999,0.0,0.004128,0.0,0.0054719999999999,0.0,0.000381,0.0,0.00852,0.0,0.000252976,0.0,0.0005112,0.0,5.6800000000000005e-05,0.0,0.0001794,0.0,0.0002928
hellaswag.val.9123,meta/code-llama-instruct-34b-chat,0.0,0.00018624,0.0,7.199999999999999e-05,0.0,0.0001928,0.0,0.001928,1.0,0.001928,1.0,0.00024,1.0,0.00241,0.0,0.00018624,0.0,0.0002151,0.0,4.8e-05,0.0,0.0001439999999999,1.0,0.0001911999999999
grade-school-math.dev.24,gpt-4-1106-preview,0.75,0.00616,0.25,0.0001487999999999,0.75,0.0003176,0.75,0.004856,0.75,0.00596,0.5,0.000462,0.75,0.00616,0.5,0.000285568,0.75,0.0003483,0.5,7.780000000000001e-05,0.5,0.0002148,0.75,0.0003896
mmlu-high-school-us-history.val.54,meta/llama-2-70b-chat,0.0,0.0002097,0.0,6.989999999999999e-05,0.0,0.0001872,1.0,0.001872,0.0,0.001872,1.0,0.000233,1.0,0.00234,0.0,0.000180808,0.0,0.0002097,1.0,4.660000000000001e-05,1.0,0.0001397999999999,1.0,0.0001856
mmlu-miscellaneous.val.551,WizardLM/WizardLM-13B-V1.2,1.0,2.16e-05,1.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,1.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,1.0,1.44e-05,1.0,4.32e-05,1.0,5.68e-05
mmlu-formal-logic.val.61,meta/llama-2-70b-chat,0.0,0.0002078999999999,0.0,6.93e-05,1.0,0.0001856,0.0,0.001856,1.0,0.001856,1.0,0.000231,1.0,0.00232,0.0,0.000179256,0.0,0.0002078999999999,0.0,4.6200000000000005e-05,1.0,0.0001386,1.0,0.000184
mmlu-management.val.48,claude-v1,1.0,0.000624,1.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,0.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.16e-05
abstract2title.test.133,gpt-4-1106-preview,1.0,0.00167,1.0,3.15e-05,1.0,0.000136,1.0,0.001264,1.0,0.00172,1.0,0.000128,1.0,0.00167,1.0,8.3032e-05,1.0,0.0002106,1.0,1.96e-05,1.0,6.66e-05,1.0,8e-05
hellaswag.val.857,claude-v1,0.0,0.00088,0.0,3.27e-05,1.0,8.800000000000001e-05,0.0,0.00088,0.0,0.00088,1.0,0.000111,0.0,0.0011,0.0,8.4584e-05,0.0,9.72e-05,1.0,2.18e-05,1.0,6.54e-05,1.0,8.640000000000001e-05
mmlu-logical-fallacies.val.88,meta/llama-2-70b-chat,0.0,8.640000000000001e-05,0.0,2.88e-05,0.0,7.76e-05,0.0,0.000776,0.0,0.000776,0.0,9.6e-05,0.0,0.001,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,0.0,5.76e-05,0.0,7.6e-05
grade-school-math.dev.5394,claude-v1,0.75,0.003456,0.75,0.0001251,0.75,0.0004512,0.75,0.003456,0.75,0.0041519999999999,0.75,0.000383,0.75,0.00618,0.75,0.000258408,0.75,0.000315,0.75,6.02e-05,0.75,0.0002351999999999,0.75,0.0003432
mmlu-high-school-us-history.val.26,meta/llama-2-70b-chat,1.0,0.0002834999999999,1.0,9.45e-05,1.0,0.0002528,1.0,0.002528,1.0,0.002528,1.0,0.000317,1.0,0.00316,0.0,0.00024444,1.0,0.0002834999999999,1.0,6.3e-05,1.0,0.0001889999999999,1.0,0.0002512
arc-challenge.test.698,meta/llama-2-70b-chat,1.0,8.640000000000001e-05,0.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,0.0,9.6e-05,1.0,0.001,0.0,7.4496e-05,1.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
mmlu-miscellaneous.val.73,WizardLM/WizardLM-13B-V1.2,1.0,2.04e-05,1.0,2.04e-05,1.0,5.52e-05,1.0,0.000552,1.0,0.000552,1.0,6.8e-05,1.0,0.00069,0.0,5.2768e-05,1.0,6.12e-05,0.0,1.36e-05,1.0,4.08e-05,1.0,5.36e-05
mmlu-electrical-engineering.val.105,meta/llama-2-70b-chat,0.0,8.01e-05,0.0,2.67e-05,0.0,7.200000000000002e-05,0.0,0.00072,0.0,0.00072,0.0,8.9e-05,0.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,0.0,1.7800000000000002e-05,0.0,5.34e-05,0.0,7.120000000000001e-05
mmlu-professional-law.val.1446,gpt-4-1106-preview,0.0,0.00247,0.0,7.38e-05,0.0,0.0001976,0.0,0.001976,0.0,0.001976,1.0,0.000246,0.0,0.00247,0.0,0.0001908959999999,0.0,0.0002214,1.0,4.920000000000001e-05,1.0,0.0001476,1.0,0.000196
grade-school-math.dev.4297,gpt-4-1106-preview,0.75,0.01027,0.75,0.0001965,0.75,0.0006728,0.75,0.004784,0.75,0.006344,0.75,0.000644,0.75,0.01027,0.75,0.000327472,0.75,0.0004122,0.75,9.96e-05,0.25,0.0002663999999999,0.75,0.0004128
mmlu-high-school-macroeconomics.val.138,gpt-4-1106-preview,1.0,0.00102,1.0,3.03e-05,1.0,8.16e-05,1.0,0.000816,1.0,0.000816,1.0,0.0001009999999999,1.0,0.00102,0.0,7.8376e-05,0.0,9.09e-05,0.0,2.02e-05,0.0,6.06e-05,1.0,8e-05
mmlu-human-aging.val.179,meta/llama-2-70b-chat,0.0,7.920000000000001e-05,0.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,1.0,5.28e-05,1.0,6.96e-05
mmlu-moral-scenarios.val.371,WizardLM/WizardLM-13B-V1.2,0.0,4.32e-05,0.0,4.32e-05,0.0,0.000116,0.0,0.00116,0.0,0.00116,0.0,0.000144,1.0,0.00148,0.0,0.000111744,0.0,0.0001295999999999,0.0,2.88e-05,0.0,8.64e-05,0.0,0.0001144
grade-school-math.dev.155,meta/llama-2-70b-chat,0.25,0.0003672,0.25,0.0001683,0.5,0.0005656,0.75,0.005248,0.75,0.005896,0.5,0.000457,0.5,0.00719,0.25,0.000331352,0.25,0.0003672,0.75,7.300000000000001e-05,0.25,0.0002874,0.25,0.0002352
mmlu-high-school-physics.val.132,meta/llama-2-70b-chat,0.0,8.640000000000001e-05,0.0,2.88e-05,1.0,7.76e-05,0.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
mmlu-professional-law.val.164,mistralai/mixtral-8x7b-chat,1.0,0.0001733999999999,1.0,8.669999999999999e-05,1.0,0.000232,1.0,0.00232,1.0,0.00232,1.0,0.000289,1.0,0.0029,0.0,0.000224264,0.0,0.0002601,0.0,5.780000000000001e-05,1.0,0.0001733999999999,1.0,0.0002312
bias_detection.dev.209,meta/code-llama-instruct-34b-chat,0.0,0.0002134,0.0,0.0001035,0.0,0.0003608,0.0,0.0030559999999999,0.0,0.00608,0.0,0.0003819999999999,0.0,0.00868,0.0,0.0002134,0.0,0.0002961,0.0,6.440000000000001e-05,0.0,0.0001889999999999,0.0,0.000208
mmlu-professional-law.val.1061,claude-v1,1.0,0.00288,1.0,0.0001076999999999,0.0,0.000288,1.0,0.00288,0.0,0.00288,1.0,0.000359,1.0,0.0036,0.0,0.0002785839999999,0.0,0.0003231,0.0,7.18e-05,0.0,0.0002153999999999,1.0,0.0002872
mmlu-college-mathematics.val.52,gpt-4-1106-preview,1.0,0.00122,0.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000121,1.0,0.00122,0.0,9.3896e-05,0.0,0.0001089,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.68e-05
mmlu-moral-scenarios.val.444,claude-v1,0.0,0.0010479999999999,1.0,3.9e-05,0.0,0.0001048,0.0,0.0010479999999999,0.0,0.0010479999999999,0.0,0.00013,1.0,0.00134,0.0,0.00010088,0.0,0.000117,0.0,2.6e-05,0.0,7.8e-05,0.0,0.0001032
mmlu-miscellaneous.val.105,gpt-4-1106-preview,1.0,0.00124,1.0,3.69e-05,1.0,9.92e-05,1.0,0.000992,1.0,0.000992,1.0,0.000123,1.0,0.00124,0.0,9.5448e-05,0.0,0.0001107,0.0,2.46e-05,1.0,7.379999999999999e-05,1.0,9.84e-05
grade-school-math.dev.923,gpt-4-1106-preview,0.75,0.00611,0.75,0.0001227,0.75,0.0005656,0.75,0.00472,0.75,0.00532,0.5,0.000417,0.75,0.00611,0.75,0.000293328,0.75,0.0003429,0.5,8.54e-05,0.75,0.0002352,0.75,0.000328
grade-school-math.dev.5486,gpt-4-1106-preview,0.75,0.0077,0.5,0.0001539,0.75,0.0005296,0.75,0.0050799999999999,0.75,0.0052239999999999,0.75,0.000543,0.75,0.0077,0.75,0.0003298,0.25,0.0003689999999999,0.75,9.18e-05,0.75,0.0002676,0.25,0.0002392
hellaswag.val.8591,claude-v1,1.0,0.00204,1.0,7.62e-05,0.0,0.000204,1.0,0.00204,1.0,0.00204,0.0,0.000256,1.0,0.00258,1.0,0.0001971039999999,1.0,0.0002286,1.0,5.080000000000001e-05,0.0,0.0001524,1.0,0.0002024
grade-school-math.dev.2128,gpt-4-1106-preview,0.75,0.00806,0.75,0.0001442999999999,0.75,0.0004456,0.5,0.004216,0.75,0.005488,0.25,0.000464,0.75,0.00806,0.75,0.000339112,0.5,0.0003942,0.25,0.0001032,0.75,0.0002639999999999,0.75,0.0003752
grade-school-math.dev.3783,gpt-4-1106-preview,0.75,0.00774,0.75,0.0001532999999999,0.75,0.0005136,0.75,0.0042,0.75,0.005304,0.75,0.000407,0.75,0.00774,0.25,0.000340664,0.75,0.0003276,0.25,9.5e-05,0.25,0.0002352,0.75,0.0003776
grade-school-math.dev.2924,meta/llama-2-70b-chat,0.75,0.0003132,0.75,0.0001452,0.75,0.0004696,0.75,0.004312,0.75,0.005056,0.75,0.000448,0.75,0.00839,0.25,0.00030652,0.75,0.0003132,0.75,8.26e-05,0.75,0.0002352,0.75,0.0003352
mtbench.dev.37,gpt-4-1106-preview,1.0,0.03378,1.0,0.0002697,1.0,0.0014952,1.0,0.015072,1.0,0.01464,1.0,0.0018059999999999,1.0,0.03378,0.9,0.000708488,1.0,0.0010683,1.0,0.000166,1.0,0.0005436,0.9,0.00092
mmlu-professional-law.val.515,claude-v1,0.0,0.00156,0.0,5.82e-05,0.0,0.000156,0.0,0.00156,0.0,0.00156,0.0,0.000196,0.0,0.00195,0.0,0.000150544,0.0,0.0001746,1.0,3.880000000000001e-05,0.0,0.0001164,0.0,0.0001544
mmlu-jurisprudence.val.95,WizardLM/WizardLM-13B-V1.2,1.0,3.09e-05,1.0,3.09e-05,0.0,8.320000000000002e-05,0.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00107,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,0.0,8.240000000000001e-05
hellaswag.val.2467,mistralai/mistral-7b-chat,0.0,2.5e-05,0.0,3.75e-05,1.0,0.0001008,1.0,0.001008,1.0,0.001008,1.0,0.000125,1.0,0.00126,0.0,9.7e-05,1.0,0.0001116,0.0,2.5e-05,1.0,7.5e-05,1.0,9.92e-05
mmlu-miscellaneous.val.446,mistralai/mistral-7b-chat,0.0,1.42e-05,1.0,2.13e-05,0.0,5.76e-05,1.0,0.000576,1.0,0.000576,1.0,7.099999999999999e-05,1.0,0.0007199999999999,0.0,5.5096e-05,0.0,6.390000000000001e-05,0.0,1.42e-05,1.0,4.26e-05,1.0,5.68e-05
hellaswag.val.1087,claude-v1,1.0,0.001272,1.0,4.71e-05,1.0,0.0001272,1.0,0.001272,1.0,0.001272,1.0,0.0001599999999999,1.0,0.00162,0.0,0.000122608,1.0,0.0001413,1.0,3.160000000000001e-05,1.0,9.48e-05,1.0,0.0001256
mmlu-professional-medicine.val.104,WizardLM/WizardLM-13B-V1.2,0.0,4.38e-05,0.0,4.38e-05,1.0,0.0001176,1.0,0.001176,1.0,0.001176,0.0,0.000146,1.0,0.0015,0.0,0.000113296,0.0,0.0001314,0.0,2.92e-05,1.0,8.759999999999999e-05,1.0,0.000116
grade-school-math.dev.5064,gpt-4-1106-preview,0.75,0.01261,0.25,0.0002055,0.75,0.0007183999999999,0.75,0.006128,0.5,0.008216,0.75,0.000663,0.75,0.01261,0.5,0.000426024,0.25,0.0004284,0.25,0.0001336,0.25,0.0003689999999999,0.5,0.0005064
hellaswag.val.4387,claude-v1,1.0,0.001888,0.0,7.019999999999999e-05,0.0,0.0001888,1.0,0.001888,1.0,0.001888,1.0,0.000237,1.0,0.00236,0.0,0.0001823599999999,1.0,0.0002115,0.0,4.7e-05,0.0,0.0001409999999999,1.0,0.0001872
mmlu-high-school-mathematics.val.46,meta/llama-2-70b-chat,0.0,6.48e-05,0.0,2.16e-05,0.0,5.84e-05,0.0,0.000584,0.0,0.000584,0.0,7.199999999999999e-05,1.0,0.00073,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,0.0,4.32e-05,0.0,5.76e-05
arc-challenge.test.598,meta/llama-2-70b-chat,1.0,9.72e-05,0.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,1.0,9.72e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
hellaswag.val.4724,claude-v1,1.0,0.002432,1.0,9.09e-05,1.0,0.0002432,1.0,0.002432,1.0,0.002432,1.0,0.000305,1.0,0.00304,0.0,0.0002351279999999,1.0,0.0002718,0.0,6.06e-05,0.0,0.0001818,1.0,0.0002416
grade-school-math.dev.7164,meta/llama-2-70b-chat,0.75,0.0003239999999999,0.75,0.0001299,0.75,0.0004592,0.75,0.003848,0.75,0.00488,0.75,0.0005319999999999,0.75,0.00484,0.75,0.0002747039999999,0.75,0.0003239999999999,0.75,8.24e-05,0.25,0.0002808,0.75,0.000352
grade-school-math.dev.1868,gpt-4-1106-preview,0.25,0.00918,0.25,0.0001487999999999,0.25,0.0006839999999999,0.25,0.005592,0.25,0.007152,0.5,0.0007589999999999,0.25,0.00918,0.25,0.000379464,0.25,0.0004347,0.25,0.000104,0.25,0.0002808,0.25,0.0004888
mmlu-high-school-european-history.val.13,WizardLM/WizardLM-13B-V1.2,0.0,0.000144,0.0,0.000144,0.0,0.0003848,0.0,0.003848,0.0,0.003848,0.0,0.00048,0.0,0.00484,0.0,0.00037248,0.0,0.000432,0.0,9.6e-05,0.0,0.000288,0.0,0.0003832
mmlu-computer-security.val.50,WizardLM/WizardLM-13B-V1.2,0.0,2.4e-05,0.0,2.4e-05,1.0,6.720000000000001e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
mmlu-professional-law.val.1377,claude-v1,0.0,0.002,0.0,7.47e-05,0.0,0.0002,0.0,0.002,0.0,0.002,0.0,0.000249,0.0,0.0025,0.0,0.0001932239999999,0.0,0.0002241,0.0,4.980000000000001e-05,1.0,0.0001494,0.0,0.0001992
hellaswag.val.3481,claude-v1,1.0,0.002104,0.0,7.86e-05,0.0,0.0002104,1.0,0.002104,1.0,0.002104,1.0,0.0002639999999999,0.0,0.00266,0.0,0.000203312,1.0,0.0002349,0.0,5.24e-05,0.0,0.0001572,1.0,0.0002087999999999
mmlu-business-ethics.val.62,claude-v1,1.0,0.000768,0.0,2.85e-05,1.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,0.0,9.5e-05,1.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,1.0,5.7e-05,1.0,7.6e-05
hellaswag.val.8999,claude-v1,0.0,0.002088,0.0,7.8e-05,0.0,0.0002112,0.0,0.002088,0.0,0.002088,0.0,0.00026,0.0,0.00264,0.0,0.00020176,0.0,0.000234,0.0,5.2e-05,0.0,0.000156,0.0,0.0002072
arc-challenge.val.48,gpt-4-1106-preview,0.0,0.00129,0.0,3.84e-05,1.0,0.0001032,1.0,0.001032,0.0,0.001032,0.0,0.00013,0.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,0.0,2.56e-05,0.0,7.68e-05,1.0,0.0001016
mmlu-professional-medicine.val.161,WizardLM/WizardLM-13B-V1.2,1.0,3.57e-05,1.0,3.57e-05,1.0,9.600000000000002e-05,1.0,0.00096,0.0,0.00096,1.0,0.0001189999999999,1.0,0.00123,0.0,9.2344e-05,0.0,0.0001071,1.0,2.3800000000000003e-05,1.0,7.14e-05,1.0,9.440000000000002e-05
mmlu-college-biology.val.97,meta/llama-2-70b-chat,0.0,7.290000000000001e-05,0.0,2.43e-05,0.0,6.560000000000001e-05,0.0,0.000656,0.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,1.0,1.62e-05,1.0,4.86e-05,0.0,6.48e-05
grade-school-math.dev.7010,gpt-4-1106-preview,0.75,0.00985,0.5,0.0001748999999999,0.25,0.000728,0.75,0.004664,0.75,0.006704,0.75,0.000508,0.75,0.00985,0.25,0.000384896,0.75,0.0004122,0.75,0.0001036,0.25,0.0002339999999999,0.75,0.0003936
abstract2title.test.28,gpt-4-1106-preview,1.0,0.00356,1.0,9.42e-05,1.0,0.000304,1.0,0.003064,1.0,0.003328,1.0,0.0003309999999999,1.0,0.00356,1.0,0.00024056,1.0,0.000288,1.0,6.220000000000001e-05,1.0,0.0001872,1.0,0.0002544
hellaswag.val.214,mistralai/mistral-7b-chat,0.0,3.04e-05,0.0,4.53e-05,1.0,0.0001224,1.0,0.0012239999999999,1.0,0.0012239999999999,1.0,0.0001519999999999,0.0,0.00156,0.0,0.000117952,0.0,0.0001359,0.0,3.04e-05,0.0,9.12e-05,1.0,0.0001208
mmlu-professional-law.val.442,zero-one-ai/Yi-34B-Chat,0.0,0.0002664,0.0,0.0001002,1.0,0.000268,0.0,0.00268,1.0,0.00268,0.0,0.000334,0.0,0.00335,0.0,0.000259184,0.0,0.0003006,0.0,6.68e-05,1.0,0.0002004,0.0,0.0002664
grade-school-math.dev.3284,meta/llama-2-70b-chat,0.25,0.0004842,0.25,0.0001886999999999,0.25,0.0008416,0.25,0.0067119999999999,0.75,0.009856,0.75,0.000722,0.75,0.0098,0.75,0.000305744,0.25,0.0004842,0.25,0.0001012,0.25,0.0002862,0.25,0.0007264
mmlu-moral-scenarios.val.810,claude-v1,1.0,0.0010559999999999,1.0,3.93e-05,0.0,0.0001056,1.0,0.0010559999999999,1.0,0.0010559999999999,1.0,0.0001309999999999,1.0,0.00132,0.0,0.000101656,0.0,0.0001179,0.0,2.62e-05,1.0,7.86e-05,1.0,0.000104
hellaswag.val.1696,mistralai/mistral-7b-chat,1.0,2.72e-05,1.0,4.08e-05,0.0,0.0001096,0.0,0.001096,0.0,0.001096,1.0,0.000136,1.0,0.00137,1.0,0.000105536,0.0,0.0001215,1.0,2.72e-05,1.0,8.159999999999999e-05,0.0,0.000108
grade-school-math.dev.6477,gpt-4-1106-preview,0.5,0.0082,0.25,0.000144,0.75,0.0006824,0.75,0.005552,0.75,0.00824,0.25,0.0007379999999999,0.5,0.0082,0.25,0.000383344,0.25,0.0004131,0.25,0.0001058,0.75,0.000312,0.75,0.000568
mmlu-high-school-statistics.val.50,WizardLM/WizardLM-13B-V1.2,0.0,4.86e-05,0.0,4.86e-05,0.0,0.0001304,1.0,0.001304,0.0,0.001304,1.0,0.000162,1.0,0.00166,0.0,0.000125712,0.0,0.0001458,1.0,3.24e-05,1.0,9.72e-05,1.0,0.0001296
hellaswag.val.7806,claude-v1,1.0,0.002176,1.0,8.099999999999999e-05,1.0,0.0002176,1.0,0.002176,1.0,0.002176,1.0,0.0002729999999999,1.0,0.00275,1.0,0.0002102959999999,1.0,0.000243,1.0,5.420000000000001e-05,1.0,0.0001626,1.0,0.000216
grade-school-math.dev.1218,gpt-4-1106-preview,0.75,0.00875,0.75,0.000147,0.75,0.0004623999999999,0.75,0.004312,0.75,0.00556,0.75,0.000401,0.75,0.00875,0.5,0.000303416,0.75,0.0003195,0.25,8.22e-05,0.75,0.0002454,0.75,0.0003488
hellaswag.val.6534,claude-v1,0.0,0.002072,0.0,7.74e-05,1.0,0.0002096,0.0,0.002072,1.0,0.002072,1.0,0.00026,0.0,0.00262,0.0,0.000200208,1.0,0.0002322,0.0,5.160000000000001e-05,0.0,0.0001548,1.0,0.0002056
mmlu-professional-law.val.465,gpt-4-1106-preview,0.0,0.0032,0.0,9.57e-05,0.0,0.000256,0.0,0.00256,0.0,0.00256,0.0,0.000321,0.0,0.0032,0.0,0.000247544,0.0,0.0002871,0.0,6.38e-05,0.0,0.0001914,0.0,0.0002552
hellaswag.val.4161,gpt-4-1106-preview,1.0,0.0028,0.0,8.37e-05,1.0,0.000224,0.0,0.00224,0.0,0.00224,0.0,0.000281,1.0,0.0028,0.0,0.0002165039999999,1.0,0.0002502,0.0,5.580000000000001e-05,0.0,0.0001674,1.0,0.0002224
mmlu-college-biology.val.40,claude-v1,1.0,0.0019119999999999,0.0,7.14e-05,1.0,0.0001912,1.0,0.0019119999999999,0.0,0.0019119999999999,1.0,0.0002379999999999,1.0,0.00239,0.0,0.000184688,0.0,0.0002142,0.0,4.7600000000000005e-05,1.0,0.0001428,0.0,0.0001904
hellaswag.val.2026,mistralai/mistral-7b-chat,0.0,1.6800000000000002e-05,1.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,1.0,7.469999999999999e-05,0.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.640000000000001e-05
mmlu-professional-law.val.1067,claude-v1,0.0,0.002672,0.0,9.99e-05,0.0,0.0002672,0.0,0.002672,0.0,0.002672,0.0,0.000333,0.0,0.00334,0.0,0.0002584079999999,0.0,0.0002997,0.0,6.659999999999999e-05,0.0,0.0001998,0.0,0.0002656
mmlu-miscellaneous.val.131,meta/llama-2-70b-chat,0.0,6.12e-05,0.0,2.04e-05,0.0,5.52e-05,1.0,0.000552,0.0,0.000552,1.0,6.8e-05,1.0,0.00069,0.0,5.2768e-05,0.0,6.12e-05,0.0,1.36e-05,1.0,4.08e-05,1.0,5.36e-05
hellaswag.val.8944,claude-v1,1.0,0.002184,0.0,8.16e-05,1.0,0.0002184,1.0,0.002184,1.0,0.002184,0.0,0.000274,1.0,0.00276,0.0,0.000211072,1.0,0.0002448,0.0,5.44e-05,0.0,0.0001632,1.0,0.0002167999999999
hellaswag.val.3283,claude-v1,0.0,0.00216,0.0,8.07e-05,0.0,0.000216,0.0,0.00216,0.0,0.00216,0.0,0.000271,1.0,0.0027,0.0,0.000208744,1.0,0.0002421,0.0,5.380000000000001e-05,0.0,0.0001614,0.0,0.0002144
mmlu-professional-law.val.1521,gpt-4-1106-preview,0.0,0.0034,0.0,0.0001016999999999,0.0,0.000272,1.0,0.00272,1.0,0.00272,1.0,0.000339,0.0,0.0034,0.0,0.000263064,0.0,0.0003051,0.0,6.780000000000001e-05,0.0,0.0002033999999999,1.0,0.0002704
grade-school-math.dev.2789,gpt-4-1106-preview,0.75,0.00849,0.75,0.0001505999999999,0.75,0.0005591999999999,0.75,0.005328,0.75,0.005952,0.75,0.000639,0.75,0.00849,0.25,0.000344544,0.5,0.0003078,0.75,9.92e-05,0.75,0.0002795999999999,0.75,0.0004552
hellaswag.val.5947,claude-v1,1.0,0.002,1.0,7.439999999999999e-05,1.0,0.0002,1.0,0.002,1.0,0.002,0.0,0.000251,1.0,0.0025,1.0,0.0001932239999999,1.0,0.0002241,1.0,4.980000000000001e-05,1.0,0.0001494,1.0,0.0001984
mmlu-elementary-mathematics.val.298,claude-v1,1.0,0.000752,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.5e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,0.0,7.439999999999999e-05
grade-school-math.dev.4366,gpt-4-1106-preview,0.75,0.0067899999999999,0.5,0.0001422,0.75,0.0006056,0.75,0.0048319999999999,0.5,0.006056,0.75,0.000439,0.75,0.0067899999999999,0.25,0.000269272,0.75,0.0005840999999999,0.5,7.6e-05,0.5,0.0002657999999999,0.5,0.0003528
mmlu-international-law.val.92,meta/llama-2-70b-chat,0.0,0.0001116,1.0,3.72e-05,1.0,0.0001,1.0,0.001,1.0,0.001,1.0,0.000124,1.0,0.00125,0.0,9.6224e-05,0.0,0.0001116,0.0,2.4800000000000003e-05,1.0,7.44e-05,1.0,9.840000000000002e-05
hellaswag.val.5668,claude-v1,1.0,0.002008,1.0,7.5e-05,1.0,0.0002032,1.0,0.002008,1.0,0.002008,0.0,0.000252,1.0,0.00251,1.0,0.000194,1.0,0.000225,1.0,5e-05,1.0,0.00015,1.0,0.0001992
mmlu-anatomy.val.127,WizardLM/WizardLM-13B-V1.2,1.0,2.22e-05,1.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,1.0,1.48e-05,1.0,4.44e-05,1.0,5.92e-05
arc-challenge.test.408,claude-v1,0.0,0.000888,0.0,3.3e-05,0.0,8.88e-05,0.0,0.000888,0.0,0.000888,0.0,0.000112,1.0,0.00111,1.0,8.536000000000001e-05,0.0,9.9e-05,1.0,2.2e-05,0.0,6.6e-05,0.0,8.72e-05
mmlu-anatomy.val.57,WizardLM/WizardLM-13B-V1.2,1.0,2.49e-05,1.0,2.49e-05,1.0,6.720000000000001e-05,1.0,0.000672,1.0,0.000672,1.0,8.3e-05,1.0,0.0008399999999999,0.0,6.4408e-05,1.0,7.470000000000001e-05,1.0,1.66e-05,1.0,4.98e-05,1.0,6.560000000000001e-05
mmlu-electrical-engineering.val.23,WizardLM/WizardLM-13B-V1.2,1.0,2.25e-05,1.0,2.25e-05,0.0,6.08e-05,0.0,0.000608,0.0,0.000608,0.0,7.5e-05,1.0,0.00079,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,0.0,4.5e-05,1.0,6e-05
mmlu-high-school-world-history.val.156,meta/llama-2-70b-chat,0.0,0.0001593,1.0,5.31e-05,1.0,0.0001424,1.0,0.001424,1.0,0.001424,1.0,0.000177,1.0,0.00178,0.0,0.000137352,0.0,0.0001593,1.0,3.54e-05,1.0,0.0001062,1.0,0.0001407999999999
hellaswag.val.807,meta/llama-2-70b-chat,1.0,9e-05,0.0,3.03e-05,0.0,8.16e-05,1.0,0.000816,0.0,0.000816,1.0,0.000103,1.0,0.00102,0.0,7.8376e-05,1.0,9e-05,0.0,2.02e-05,0.0,6.06e-05,1.0,8e-05
mmlu-professional-law.val.265,zero-one-ai/Yi-34B-Chat,0.0,0.0001792,0.0,6.72e-05,0.0,0.00018,0.0,0.0018,0.0,0.0018,0.0,0.000224,1.0,0.00225,0.0,0.0001738239999999,0.0,0.0002016,0.0,4.480000000000001e-05,0.0,0.0001344,0.0,0.0001792
mmlu-high-school-macroeconomics.val.285,gpt-4-1106-preview,1.0,0.00078,1.0,2.28e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,1.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
grade-school-math.dev.1916,gpt-4-1106-preview,0.75,0.00714,0.75,0.0001746,0.75,0.0004296,0.75,0.005256,0.5,0.006696,0.75,0.0004109999999999,0.75,0.00714,0.5,0.000367048,0.75,0.0003987,0.5,9.38e-05,0.75,0.0002772,0.75,0.0003696
mmlu-high-school-mathematics.val.133,gpt-4-1106-preview,1.0,0.00128,0.0,3.81e-05,1.0,0.0001024,1.0,0.001024,1.0,0.001024,0.0,0.000127,1.0,0.00128,0.0,9.8552e-05,0.0,0.0001143,0.0,2.54e-05,0.0,7.62e-05,1.0,0.0001016
mmlu-international-law.val.81,meta/llama-2-70b-chat,0.0,0.0001152,0.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,1.0,0.000128,1.0,0.00129,0.0,9.9328e-05,0.0,0.0001152,0.0,2.56e-05,1.0,7.68e-05,1.0,0.0001016
mmlu-professional-law.val.1130,WizardLM/WizardLM-13B-V1.2,1.0,4.86e-05,1.0,4.86e-05,1.0,0.0001304,1.0,0.001304,0.0,0.001304,1.0,0.000162,1.0,0.00166,0.0,0.000125712,0.0,0.0001458,1.0,3.24e-05,1.0,9.72e-05,1.0,0.0001288
mmlu-machine-learning.val.41,meta/llama-2-70b-chat,0.0,0.0001628999999999,0.0,5.43e-05,1.0,0.0001456,1.0,0.0014559999999999,1.0,0.0014559999999999,0.0,0.0001809999999999,0.0,0.00182,0.0,0.0001404559999999,0.0,0.0001628999999999,0.0,3.6200000000000006e-05,1.0,0.0001086,0.0,0.0001448
grade-school-math.dev.3784,gpt-4-1106-preview,0.75,0.01133,0.5,0.000159,0.25,0.0005824,0.75,0.004168,0.75,0.006352,0.75,0.000598,0.75,0.01133,0.25,0.000317384,0.75,0.000396,0.25,9.84e-05,0.75,0.0003281999999999,0.75,0.0004736
hellaswag.val.3353,claude-v1,1.0,0.002072,1.0,7.74e-05,1.0,0.0002072,1.0,0.002072,1.0,0.002072,0.0,0.00026,1.0,0.00262,1.0,0.000200208,1.0,0.0002322,1.0,5.160000000000001e-05,1.0,0.0001548,1.0,0.0002056
grade-school-math.dev.5036,claude-v1,0.75,0.0046479999999999,0.75,0.0001706999999999,0.75,0.0005416,0.75,0.0046479999999999,0.5,0.005728,0.75,0.000472,0.75,0.0070999999999999,0.75,0.000308848,0.75,0.0003717,0.25,8.400000000000001e-05,0.25,0.0002868,0.25,0.0004792
grade-school-math.dev.5183,meta/llama-2-70b-chat,0.75,0.0003447,0.75,0.0001722,0.75,0.0005768,0.75,0.005048,0.75,0.005312,0.75,0.000664,0.75,0.01057,0.75,0.000342216,0.75,0.0003447,0.25,7.7e-05,0.75,0.0002724,0.75,0.0004456
grade-school-math.dev.4728,meta/llama-2-70b-chat,0.75,0.0003888,0.25,0.0001479,0.75,0.0005104,0.75,0.00508,0.75,0.005944,0.75,0.000518,0.75,0.00761,0.5,0.00030652,0.75,0.0003888,0.25,7.82e-05,0.25,0.0002441999999999,0.75,0.0003456
mmlu-abstract-algebra.val.50,gpt-4-1106-preview,0.0,0.0009199999999999,1.0,2.73e-05,0.0,7.360000000000001e-05,1.0,0.000736,0.0,0.000736,0.0,9.3e-05,0.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,0.0,5.46e-05,1.0,7.280000000000001e-05
mmlu-nutrition.val.288,meta/llama-2-70b-chat,0.0,8.370000000000002e-05,0.0,2.79e-05,1.0,7.52e-05,1.0,0.000752,1.0,0.000752,1.0,9.3e-05,1.0,0.00094,0.0,7.2168e-05,0.0,8.370000000000002e-05,0.0,1.86e-05,1.0,5.58e-05,1.0,7.439999999999999e-05
grade-school-math.dev.4811,meta/llama-2-70b-chat,0.25,0.0003654,0.25,0.0001491,0.25,0.0006296,0.75,0.00428,0.75,0.00596,0.75,0.0005499999999999,0.75,0.00898,0.25,0.000303416,0.25,0.0003654,0.75,8.740000000000001e-05,0.25,0.000249,0.75,0.0003736
mbpp.dev.43,WizardLM/WizardLM-13B-V1.2,0.0,6.269999999999999e-05,0.0,6.269999999999999e-05,1.0,0.000504,0.0,0.0067919999999999,1.0,0.005016,1.0,0.0001619999999999,1.0,0.00894,1.0,0.00010864,0.0,0.0003681,1.0,3.5e-05,1.0,0.0002459999999999,0.0,0.0002512
mmlu-moral-scenarios.val.252,WizardLM/WizardLM-13B-V1.2,0.0,3.8700000000000006e-05,0.0,3.8700000000000006e-05,0.0,0.000104,0.0,0.00104,0.0,0.00104,0.0,0.000129,1.0,0.00133,0.0,0.000100104,0.0,0.0001161,0.0,2.58e-05,0.0,7.740000000000001e-05,1.0,0.0001032
accounting_audit.dev.5,WizardLM/WizardLM-13B-V1.2,0.0,4.2600000000000005e-05,0.0,4.2600000000000005e-05,0.0,0.000112,0.0,0.00112,0.0,0.00112,0.0,0.000139,0.0,0.0014,0.0,0.0001101919999999,0.0,0.0001278,0.0,4.12e-05,1.0,0.0001296,1.0,0.0001104
hellaswag.val.1952,mistralai/mistral-7b-chat,0.0,2.24e-05,0.0,3.3600000000000004e-05,0.0,9.04e-05,1.0,0.000904,0.0,0.000904,0.0,0.000112,0.0,0.00113,0.0,8.6912e-05,0.0,9.99e-05,0.0,2.24e-05,0.0,6.720000000000001e-05,0.0,8.88e-05
grade-school-math.dev.5514,meta/llama-2-70b-chat,0.75,0.0003393,0.75,0.0001613999999999,0.75,0.0005495999999999,0.75,0.004416,0.75,0.005064,0.75,0.000449,0.75,0.0075,0.25,0.000274704,0.75,0.0003393,0.75,8.56e-05,0.75,0.0002429999999999,0.75,0.0002248
hellaswag.val.4792,claude-v1,1.0,0.001888,0.0,7.049999999999999e-05,0.0,0.0001888,1.0,0.001888,1.0,0.001888,1.0,0.000237,1.0,0.00236,0.0,0.0001823599999999,1.0,0.0002115,0.0,4.7e-05,0.0,0.0001409999999999,1.0,0.0001872
mmlu-high-school-european-history.val.65,gpt-4-1106-preview,1.0,0.00378,0.0,0.0001131,0.0,0.0003024,1.0,0.003024,1.0,0.003024,1.0,0.000377,1.0,0.00378,0.0,0.000292552,0.0,0.0003393,0.0,7.539999999999999e-05,1.0,0.0002262,1.0,0.0003008
mmlu-high-school-mathematics.val.140,gpt-4-1106-preview,1.0,0.00111,0.0,3.3e-05,1.0,8.88e-05,1.0,0.000888,1.0,0.000888,0.0,0.0001099999999999,1.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.54e-05,0.0,8.8e-05
arc-challenge.test.367,WizardLM/WizardLM-13B-V1.2,1.0,2.19e-05,1.0,2.19e-05,0.0,5.92e-05,1.0,0.000592,0.0,0.000592,0.0,7.3e-05,0.0,0.00074,1.0,5.6648e-05,1.0,6.57e-05,1.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
mmlu-miscellaneous.val.384,gpt-4-1106-preview,1.0,0.00086,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00086,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
mmlu-high-school-psychology.val.430,gpt-4-1106-preview,1.0,0.00083,1.0,2.46e-05,1.0,6.64e-05,1.0,0.000664,1.0,0.000664,1.0,8.2e-05,1.0,0.00083,0.0,6.3632e-05,0.0,7.38e-05,0.0,1.64e-05,1.0,4.92e-05,1.0,6.48e-05
mmlu-marketing.val.180,gpt-4-1106-preview,1.0,0.001,0.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.001,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
mmlu-miscellaneous.val.61,gpt-4-1106-preview,1.0,0.00077,0.0,2.28e-05,1.0,6.16e-05,1.0,0.000616,1.0,0.000616,0.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6e-05
grade-school-math.dev.3744,gpt-4-1106-preview,0.75,0.00789,0.25,0.0001428,0.25,0.0005544,0.25,0.004176,0.25,0.004632,0.25,0.0004379999999999,0.75,0.00789,0.25,0.000291,0.25,0.0004122,0.25,7.34e-05,0.25,0.0002748,0.25,0.000256
grade-school-math.dev.2702,gpt-4-1106-preview,0.75,0.00758,0.5,0.0001494,0.75,0.0006544,0.75,0.0050799999999999,0.75,0.0056799999999999,0.75,0.000538,0.75,0.00758,0.25,0.000294104,0.75,0.0004653,0.75,8.72e-05,0.75,0.0002172,0.5,0.0003599999999999
grade-school-math.dev.3386,gpt-4-1106-preview,0.75,0.0066599999999999,0.25,0.0001521,0.75,0.0006048,0.25,0.00396,0.25,0.0044639999999999,0.75,0.000553,0.75,0.0066599999999999,0.25,0.000342216,0.25,0.000414,0.25,0.0001022,0.25,0.0003474,0.25,0.0003944
mmlu-moral-scenarios.val.112,WizardLM/WizardLM-13B-V1.2,1.0,4.23e-05,1.0,4.23e-05,0.0,0.0001136,0.0,0.0011359999999999,0.0,0.0011359999999999,1.0,0.0001409999999999,0.0,0.00145,0.0,0.000109416,0.0,0.0001269,0.0,2.82e-05,0.0,8.46e-05,0.0,0.0001128
hellaswag.val.9644,claude-v1,0.0,0.0022,1.0,8.189999999999998e-05,1.0,0.0002224,0.0,0.0022,1.0,0.0022,0.0,0.000276,1.0,0.00278,1.0,0.000212624,1.0,0.0002466,1.0,5.480000000000001e-05,1.0,0.0001643999999999,1.0,0.0002184
winogrande.dev.513,claude-v1,0.0,0.000448,0.0,1.65e-05,0.0,4.4800000000000005e-05,0.0,0.000448,0.0,0.000448,0.0,5.7e-05,0.0,0.00059,1.0,4.2680000000000005e-05,0.0,4.86e-05,1.0,1.1e-05,0.0,3.3e-05,1.0,4.4000000000000006e-05
mmlu-professional-law.val.1338,gpt-4-1106-preview,1.0,0.00181,0.0,5.4e-05,1.0,0.0001448,1.0,0.0014479999999999,1.0,0.0014479999999999,0.0,0.0001799999999999,1.0,0.00181,0.0,0.00013968,0.0,0.000162,0.0,3.600000000000001e-05,1.0,0.000108,1.0,0.0001432
mmlu-abstract-algebra.val.58,gpt-4-1106-preview,0.0,0.00096,0.0,2.85e-05,0.0,7.680000000000001e-05,0.0,0.000768,0.0,0.000768,0.0,9.7e-05,0.0,0.00096,0.0,7.372e-05,0.0,8.55e-05,0.0,1.9e-05,0.0,5.7e-05,0.0,7.6e-05
mmlu-professional-psychology.val.437,gpt-4-1106-preview,1.0,0.00081,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00081,0.0,6.208e-05,0.0,7.2e-05,0.0,1.6000000000000003e-05,1.0,4.8e-05,0.0,6.320000000000002e-05
mmlu-professional-law.val.790,gpt-4-1106-preview,1.0,0.00333,1.0,9.96e-05,1.0,0.0002664,1.0,0.002664,1.0,0.002664,1.0,0.000332,1.0,0.00333,0.0,0.000257632,0.0,0.0002988,0.0,6.64e-05,1.0,0.0001992,1.0,0.0002648
arc-challenge.test.725,gpt-4-1106-preview,1.0,0.00065,1.0,1.83e-05,1.0,4.96e-05,0.0,0.000496,0.0,0.000496,0.0,6.1e-05,1.0,0.00065,1.0,4.7336e-05,0.0,5.4900000000000006e-05,1.0,1.22e-05,1.0,3.66e-05,1.0,4.8e-05
arc-challenge.test.1081,zero-one-ai/Yi-34B-Chat,1.0,7.040000000000002e-05,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,1.0,6.9064e-05,1.0,8.01e-05,1.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
grade-school-math.dev.2999,meta/llama-2-70b-chat,0.5,0.0003861,0.25,0.0001430999999999,0.75,0.0007136,0.75,0.0047119999999999,0.75,0.006392,0.75,0.00067,0.75,0.00889,0.25,0.000437664,0.5,0.0003861,0.25,9.86e-05,0.25,0.0003107999999999,0.75,0.0004432
grade-school-math.dev.2883,gpt-4-1106-preview,0.75,0.00558,0.75,0.0001241999999999,0.75,0.0005591999999999,0.75,0.003744,0.75,0.004056,0.5,0.000448,0.75,0.00558,0.75,0.000249872,0.75,0.0003321,0.25,9.04e-05,0.25,0.0001644,0.25,0.0002216
mmlu-professional-law.val.654,zero-one-ai/Yi-34B-Chat,1.0,0.0003823999999999,1.0,0.0001433999999999,0.0,0.0003832,0.0,0.003832,0.0,0.003832,1.0,0.000478,1.0,0.00479,0.0,0.000370928,0.0,0.0004302,0.0,9.56e-05,0.0,0.0002867999999999,1.0,0.0003823999999999
mmlu-high-school-world-history.val.83,meta/llama-2-70b-chat,0.0,0.0005049,0.0,0.0001682999999999,1.0,0.0004496,0.0,0.004496,1.0,0.004496,0.0,0.000561,0.0,0.00562,0.0,0.000435336,0.0,0.0005049,0.0,0.0001122,0.0,0.0003365999999999,1.0,0.000448
mmlu-world-religions.val.84,meta/llama-2-70b-chat,0.0,7.020000000000001e-05,0.0,2.3400000000000003e-05,0.0,6.32e-05,0.0,0.000632,0.0,0.000632,0.0,7.8e-05,0.0,0.00079,0.0,6.0528e-05,0.0,7.020000000000001e-05,0.0,1.5600000000000003e-05,0.0,4.6800000000000006e-05,0.0,6.16e-05
consensus_summary.dev.290,meta/llama-2-70b-chat,0.75,0.0002844,1.0,7.439999999999999e-05,0.0,0.0005016,0.5,0.002016,0.75,0.004536,0.5,0.00025,0.5,0.00252,0.75,0.000249872,0.75,0.0002844,0.5,5.380000000000001e-05,0.5,0.0001614,0.5,0.0001984
mmlu-conceptual-physics.val.83,gpt-4-1106-preview,1.0,0.00087,0.0,2.58e-05,0.0,6.960000000000001e-05,0.0,0.000696,0.0,0.000696,0.0,8.599999999999999e-05,1.0,0.00087,0.0,6.673599999999999e-05,0.0,7.740000000000001e-05,1.0,1.72e-05,0.0,5.16e-05,0.0,6.88e-05
hellaswag.val.543,gpt-4-1106-preview,1.0,0.00101,1.0,3e-05,1.0,8.080000000000001e-05,1.0,0.000808,1.0,0.000808,1.0,0.0001,1.0,0.00101,0.0,7.76e-05,0.0,9e-05,0.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
consensus_summary.dev.8,gpt-4-1106-preview,0.5,0.00402,0.0,0.0003048,0.0,0.0003432,0.0,0.00144,0.5,0.003816,0.75,0.00042,0.5,0.00402,0.75,0.000212624,1.0,0.0002547,0.25,3.540000000000001e-05,0.0,0.0001524,0.25,0.0001424
grade-school-math.dev.3865,gpt-4-1106-preview,0.75,0.0102,0.75,0.0001959,0.75,0.0006072,0.75,0.004872,0.75,0.006096,0.75,0.000444,0.75,0.0102,0.25,0.000266168,0.25,0.0003924,0.0,8.72e-05,0.75,0.0003114,0.25,0.0004864
grade-school-math.dev.7191,gpt-4-1106-preview,0.75,0.00831,0.75,0.0001314,0.75,0.0005304,0.75,0.004608,0.75,0.005184,0.5,0.000413,0.75,0.00831,0.25,0.000290224,0.75,0.0003834,0.5,7.180000000000001e-05,0.5,0.0002172,0.75,0.0004696
mmlu-high-school-psychology.val.301,claude-v1,1.0,0.000976,1.0,3.63e-05,1.0,9.76e-05,1.0,0.000976,1.0,0.000976,1.0,0.000123,1.0,0.00122,0.0,9.3896e-05,0.0,0.0001089,0.0,2.42e-05,1.0,7.259999999999999e-05,1.0,9.6e-05
mmlu-professional-medicine.val.236,gpt-4-1106-preview,1.0,0.0036,0.0,0.0001076999999999,1.0,0.000288,1.0,0.00288,1.0,0.00288,1.0,0.000361,1.0,0.0036,0.0,0.0002785839999999,0.0,0.0003231,0.0,7.18e-05,1.0,0.0002153999999999,1.0,0.0002864
arc-challenge.test.2,claude-v1,1.0,0.000792,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,9.8e-05,1.0,0.00102,1.0,7.604800000000001e-05,1.0,8.82e-05,0.0,1.96e-05,1.0,5.88e-05,1.0,7.760000000000002e-05
hellaswag.val.5832,meta/llama-2-70b-chat,0.0,0.0002304,0.0,7.68e-05,0.0,0.0002064,0.0,0.002064,0.0,0.002064,1.0,0.000259,0.0,0.00258,0.0,0.0001994319999999,0.0,0.0002304,0.0,5.14e-05,0.0,0.0001542,0.0,0.0002056
hellaswag.val.4965,gpt-4-1106-preview,1.0,0.00269,0.0,8.04e-05,1.0,0.0002176,1.0,0.002152,1.0,0.002152,1.0,0.00027,1.0,0.00269,0.0,0.0002079679999999,1.0,0.0002403,0.0,5.360000000000001e-05,0.0,0.0001608,1.0,0.0002136
hellaswag.val.2215,mistralai/mistral-7b-chat,0.0,2.44e-05,0.0,3.66e-05,1.0,9.84e-05,1.0,0.000984,1.0,0.000984,1.0,0.000124,1.0,0.00123,0.0,9.4672e-05,1.0,0.0001098,0.0,2.44e-05,0.0,7.32e-05,1.0,9.68e-05
mmlu-professional-law.val.1244,gpt-4-1106-preview,1.0,0.00265,0.0,7.92e-05,0.0,0.000212,0.0,0.00212,0.0,0.00212,0.0,0.000264,1.0,0.00265,0.0,0.000204864,0.0,0.0002376,1.0,5.280000000000001e-05,1.0,0.0001584,1.0,0.0002104
grade-school-math.dev.3376,gpt-4-1106-preview,0.5,0.00971,0.5,0.0001910999999999,0.75,0.0007191999999999,0.75,0.00604,0.75,0.006856,0.5,0.000614,0.5,0.00971,0.5,0.000390328,0.5,0.0004995,0.25,0.0001026,0.5,0.0002502,0.5,0.0004176
mmlu-miscellaneous.val.660,meta/llama-2-70b-chat,1.0,5.940000000000001e-05,1.0,1.98e-05,1.0,5.36e-05,1.0,0.000536,1.0,0.000536,1.0,6.599999999999999e-05,1.0,0.00067,0.0,5.1216000000000006e-05,1.0,5.940000000000001e-05,0.0,1.32e-05,1.0,3.96e-05,1.0,5.2e-05
grade-school-math.dev.6695,gpt-4-1106-preview,0.75,0.00859,0.25,0.000159,0.25,0.0006872,0.75,0.003896,0.75,0.005888,0.75,0.000441,0.75,0.00859,0.25,0.000320488,0.75,0.0003861,0.25,0.0001026,0.75,0.0003156,0.75,0.0004136
mmlu-moral-scenarios.val.845,claude-v1,1.0,0.00108,0.0,4.02e-05,1.0,0.000108,1.0,0.00108,1.0,0.00108,0.0,0.000134,1.0,0.00138,0.0,0.000103984,0.0,0.0001206,0.0,2.68e-05,1.0,8.04e-05,1.0,0.0001064
hellaswag.val.5829,claude-v1,0.0,0.002368,0.0,8.85e-05,0.0,0.0002368,0.0,0.002368,1.0,0.002368,1.0,0.000297,1.0,0.00296,0.0,0.0002289199999999,0.0,0.0002646,0.0,5.9e-05,0.0,0.000177,0.0,0.0002352
mmlu-miscellaneous.val.493,WizardLM/WizardLM-13B-V1.2,1.0,2.19e-05,1.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,1.0,6.57e-05,0.0,1.46e-05,1.0,4.38e-05,1.0,5.76e-05
mmlu-miscellaneous.val.63,gpt-4-1106-preview,1.0,0.00092,1.0,2.64e-05,1.0,7.12e-05,1.0,0.000712,1.0,0.000712,1.0,8.8e-05,1.0,0.00092,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,0.0,5.28e-05,1.0,6.96e-05
mmlu-miscellaneous.val.35,WizardLM/WizardLM-13B-V1.2,1.0,2.19e-05,1.0,2.19e-05,1.0,5.92e-05,1.0,0.000592,1.0,0.000592,1.0,7.3e-05,1.0,0.00074,0.0,5.6648e-05,1.0,6.57e-05,1.0,1.46e-05,1.0,4.38e-05,1.0,5.84e-05
winogrande.dev.953,claude-v1,1.0,0.0004559999999999,0.0,1.6800000000000002e-05,1.0,4.56e-05,1.0,0.0004559999999999,1.0,0.0004559999999999,1.0,5.8e-05,1.0,0.0006,0.0,4.4232000000000006e-05,0.0,5.0400000000000005e-05,1.0,1.12e-05,1.0,3.3600000000000004e-05,1.0,4.4e-05
grade-school-math.dev.6584,gpt-4-1106-preview,0.75,0.00969,0.75,0.0001733999999999,0.5,0.0006552,0.75,0.004872,0.75,0.006168,0.5,0.000679,0.75,0.00969,0.75,0.000344544,0.5,0.0003771,0.25,9.26e-05,0.5,0.0002562,0.75,0.000444
hellaswag.val.1937,mistralai/mistral-7b-chat,0.0,2.42e-05,1.0,3.63e-05,1.0,9.76e-05,0.0,0.000976,0.0,0.000976,1.0,0.000123,1.0,0.00125,0.0,9.3896e-05,0.0,0.000108,0.0,2.42e-05,0.0,7.259999999999999e-05,0.0,9.6e-05
hellaswag.val.8977,claude-v1,1.0,0.00204,0.0,7.589999999999999e-05,1.0,0.000204,1.0,0.00204,1.0,0.00204,1.0,0.000256,1.0,0.00255,0.0,0.0001971039999999,1.0,0.0002277,0.0,5.080000000000001e-05,0.0,0.0001524,1.0,0.0002024
mmlu-philosophy.val.189,WizardLM/WizardLM-13B-V1.2,1.0,2.67e-05,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.00093,0.0,6.9064e-05,0.0,8.01e-05,1.0,1.7800000000000002e-05,1.0,5.34e-05,1.0,7.040000000000002e-05
hellaswag.val.6160,claude-v1,1.0,0.002048,1.0,7.649999999999999e-05,1.0,0.0002072,1.0,0.002048,1.0,0.002048,0.0,0.000255,1.0,0.00259,1.0,0.00019788,1.0,0.0002295,1.0,5.1000000000000006e-05,1.0,0.0001529999999999,1.0,0.0002032
hellaswag.val.7074,gpt-4-1106-preview,1.0,0.00289,1.0,8.519999999999998e-05,0.0,0.0002312,0.0,0.002288,1.0,0.002288,1.0,0.000287,1.0,0.00289,0.0,0.00022116,1.0,0.0002565,0.0,5.7e-05,0.0,0.0001703999999999,1.0,0.0002272
mmlu-global-facts.val.77,meta/llama-2-70b-chat,0.0,6.75e-05,1.0,2.25e-05,1.0,6.08e-05,1.0,0.000608,1.0,0.000608,1.0,7.7e-05,0.0,0.0007599999999999,0.0,5.8200000000000005e-05,0.0,6.75e-05,0.0,1.5e-05,1.0,4.5e-05,1.0,5.92e-05
hellaswag.val.7917,claude-v1,1.0,0.002024,1.0,7.56e-05,0.0,0.0002048,1.0,0.002024,0.0,0.002024,0.0,0.000252,1.0,0.00256,1.0,0.000195552,0.0,0.0002259,1.0,5.0400000000000005e-05,1.0,0.0001512,1.0,0.0002008
mmlu-high-school-biology.val.289,WizardLM/WizardLM-13B-V1.2,0.0,4.35e-05,0.0,4.35e-05,1.0,0.0001168,1.0,0.001168,1.0,0.001168,1.0,0.000145,1.0,0.00146,0.0,0.00011252,0.0,0.0001305,1.0,2.9e-05,1.0,8.7e-05,1.0,0.0001152
mmlu-international-law.val.94,meta/llama-2-70b-chat,0.0,0.0001017,1.0,3.39e-05,1.0,9.120000000000002e-05,1.0,0.000912,1.0,0.000912,1.0,0.000113,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,0.0,2.2600000000000004e-05,1.0,6.78e-05,1.0,8.960000000000002e-05
winogrande.dev.1014,claude-v1,0.0,0.000392,0.0,1.41e-05,0.0,3.92e-05,0.0,0.000392,1.0,0.000392,1.0,5e-05,1.0,0.00052,1.0,3.7248e-05,0.0,4.32e-05,1.0,9.6e-06,0.0,2.8799999999999995e-05,1.0,3.7600000000000006e-05
hellaswag.val.6587,claude-v1,0.0,0.002144,0.0,8.01e-05,1.0,0.0002144,0.0,0.002144,1.0,0.002144,0.0,0.000267,1.0,0.00268,0.0,0.000207192,0.0,0.0002394,0.0,5.34e-05,0.0,0.0001602,0.0,0.0002128
hellaswag.val.2461,mistralai/mistral-7b-chat,0.0,2.44e-05,0.0,3.66e-05,0.0,9.84e-05,1.0,0.000984,0.0,0.000984,0.0,0.000122,0.0,0.00123,0.0,9.4672e-05,1.0,0.0001088999999999,0.0,2.44e-05,0.0,7.32e-05,0.0,9.68e-05
grade-school-math.dev.6641,gpt-4-1106-preview,0.75,0.01247,0.25,0.0001452,0.75,0.0006688,0.25,0.004888,0.75,0.008032,0.25,0.000534,0.75,0.01247,0.25,0.000400416,0.25,0.0005085,0.75,0.0001006,0.25,0.00033,0.75,0.0004544
grade-school-math.dev.1799,gpt-4-1106-preview,0.75,0.01028,0.25,0.0001779,0.75,0.000664,0.75,0.005872,0.75,0.0076,0.75,0.0007019999999999,0.75,0.01028,0.25,0.000355408,0.75,0.0004427999999999,0.25,0.0001056,0.75,0.000312,0.75,0.0004136
mmlu-high-school-computer-science.val.54,claude-v1,1.0,0.001936,0.0,7.230000000000001e-05,1.0,0.0001936,1.0,0.001936,1.0,0.001936,1.0,0.000241,1.0,0.00242,0.0,0.00018624,0.0,0.0002169,0.0,4.8200000000000006e-05,1.0,0.0001446,1.0,0.000192
grade-school-math.dev.3174,meta/llama-2-70b-chat,0.25,0.0003942,0.25,0.0002475,0.75,0.0006576,0.75,0.006096,0.25,0.0066,0.25,0.000542,0.75,0.01368,0.25,0.000369376,0.25,0.0003942,0.25,9.880000000000002e-05,0.25,0.0002742,0.25,0.000236
mmlu-high-school-microeconomics.val.59,meta/code-llama-instruct-34b-chat,0.0,0.000113296,0.0,4.38e-05,0.0,0.0001176,0.0,0.001176,0.0,0.001176,0.0,0.000146,1.0,0.0015,0.0,0.000113296,0.0,0.0001314,1.0,2.92e-05,1.0,8.759999999999999e-05,1.0,0.000116
grade-school-math.dev.5984,gpt-4-1106-preview,0.75,0.00752,0.25,0.0001491,0.75,0.0008536,0.75,0.005056,0.75,0.005488,0.75,0.000545,0.75,0.00752,0.25,0.000299536,0.75,0.0004005,0.25,8.42e-05,0.75,0.0002838,0.75,0.0003656
hellaswag.val.592,meta/llama-2-70b-chat,1.0,9.63e-05,1.0,3.24e-05,1.0,8.720000000000002e-05,1.0,0.000872,1.0,0.000872,1.0,0.000108,1.0,0.00109,0.0,8.380800000000001e-05,1.0,9.63e-05,0.0,2.1600000000000003e-05,1.0,6.48e-05,1.0,8.560000000000002e-05
mmlu-moral-disputes.val.144,WizardLM/WizardLM-13B-V1.2,1.0,3.27e-05,1.0,3.27e-05,0.0,8.800000000000001e-05,1.0,0.00088,1.0,0.00088,1.0,0.0001089999999999,1.0,0.0011,0.0,8.4584e-05,0.0,9.81e-05,1.0,2.18e-05,1.0,6.54e-05,1.0,8.640000000000001e-05
hellaswag.val.2560,meta/llama-2-70b-chat,0.0,8.64e-05,0.0,2.9100000000000003e-05,0.0,7.840000000000001e-05,1.0,0.000784,1.0,0.000784,1.0,9.7e-05,1.0,0.00098,0.0,7.5272e-05,0.0,8.64e-05,0.0,1.94e-05,1.0,5.8200000000000005e-05,1.0,7.680000000000001e-05
mmlu-high-school-macroeconomics.val.144,WizardLM/WizardLM-13B-V1.2,1.0,2.43e-05,1.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.400000000000001e-05
grade-school-math.dev.3455,gpt-4-1106-preview,0.5,0.00503,0.5,0.000123,0.75,0.0003808,0.75,0.003904,0.75,0.004312,0.75,0.000372,0.5,0.00503,0.75,0.00032592,0.75,0.0003015,0.75,7.38e-05,0.25,0.0002333999999999,0.25,0.0002272
grade-school-math.dev.5557,gpt-4-1106-preview,0.75,0.00713,0.75,0.0001386,0.75,0.0005104,0.75,0.0046479999999999,0.75,0.0041919999999999,0.5,0.000487,0.75,0.00713,0.75,0.000336008,0.75,0.0003618,0.75,8.340000000000001e-05,0.0,0.0002106,0.75,0.0003208
consensus_summary.dev.213,gpt-4-1106-preview,0.75,0.004,0.25,0.000111,0.25,0.0002408,0.25,0.001712,0.25,0.004136,0.25,0.0002819999999999,0.75,0.004,0.25,0.000264616,0.0,0.0002835,0.75,4.86e-05,0.75,0.0002021999999999,0.25,0.0001776
grade-school-math.dev.6989,gpt-4-1106-preview,0.5,0.00553,0.5,0.0001467,0.75,0.0004808,0.75,0.004496,0.75,0.004904,0.75,0.000429,0.5,0.00553,0.75,0.00025608,0.75,0.0003699,0.75,8.32e-05,0.75,0.0002417999999999,0.75,0.0003272
mmlu-professional-law.val.250,claude-v1,1.0,0.002848,1.0,0.0001064999999999,0.0,0.0002848,1.0,0.002848,1.0,0.002848,0.0,0.000355,1.0,0.00356,0.0,0.00027548,0.0,0.0003194999999999,0.0,7.1e-05,1.0,0.0002129999999999,1.0,0.0002832
mmlu-clinical-knowledge.val.104,WizardLM/WizardLM-13B-V1.2,0.0,2.28e-05,0.0,2.28e-05,0.0,6.16e-05,1.0,0.000616,1.0,0.000616,1.0,7.599999999999999e-05,1.0,0.00077,0.0,5.8976e-05,0.0,6.840000000000001e-05,0.0,1.52e-05,1.0,4.56e-05,1.0,6.08e-05
grade-school-math.dev.5450,gpt-4-1106-preview,0.75,0.00814,0.25,0.0002052,0.75,0.00062,0.75,0.005408,0.75,0.006032,0.75,0.0004849999999999,0.75,0.00814,0.25,0.000329024,0.0,0.0004293,0.5,0.000101,0.5,0.0002802,0.75,0.0004847999999999
grade-school-math.dev.2797,gpt-4-1106-preview,0.75,0.01242,0.5,0.0001782,0.75,0.0005591999999999,0.5,0.004848,0.75,0.005592,0.5,0.000584,0.75,0.01242,0.25,0.000331352,0.5,0.0004068,0.25,0.0001014,0.25,0.0003966,0.75,0.0005448
grade-school-math.dev.2760,gpt-4-1106-preview,0.75,0.00874,0.25,0.0001581,0.25,0.0008048,0.25,0.005192,0.25,0.00596,0.25,0.000455,0.75,0.00874,0.25,0.0002716,0.25,0.0003815999999999,0.25,0.0005054,0.25,0.0002274,0.5,0.0004352
grade-school-math.dev.4473,gpt-4-1106-preview,0.5,0.01042,0.25,0.0001137,0.25,0.0006152,0.25,0.0051439999999999,0.25,0.005648,0.25,0.000535,0.5,0.01042,0.25,0.00031428,0.25,0.0004968,0.25,0.0001092,0.25,0.0002508,0.25,0.0004112
hellaswag.val.5949,claude-v1,0.0,0.001896,0.0,7.08e-05,0.0,0.000192,0.0,0.001896,0.0,0.001896,0.0,0.000236,1.0,0.00237,0.0,0.000183136,0.0,0.0002115,0.0,4.720000000000001e-05,0.0,0.0001416,1.0,0.000188
grade-school-math.dev.188,gpt-4-1106-preview,0.5,0.00472,0.75,0.0001233,0.75,0.0004088,0.75,0.003944,0.5,0.00452,0.75,0.000365,0.5,0.00472,0.75,0.0003049679999999,0.75,0.0003086999999999,0.5,6.38e-05,0.75,0.0002021999999999,0.25,0.000224
mmlu-high-school-biology.val.252,meta/llama-2-70b-chat,0.0,8.190000000000001e-05,1.0,2.73e-05,1.0,7.360000000000001e-05,1.0,0.000736,1.0,0.000736,1.0,9.1e-05,1.0,0.0009199999999999,0.0,7.0616e-05,0.0,8.190000000000001e-05,0.0,1.82e-05,1.0,5.46e-05,1.0,7.200000000000002e-05
hellaswag.val.8592,claude-v1,1.0,0.00196,0.0,7.319999999999999e-05,0.0,0.0001984,1.0,0.00196,1.0,0.00196,1.0,0.000244,1.0,0.00245,0.0,0.0001893439999999,0.0,0.0002196,0.0,4.880000000000001e-05,0.0,0.0001463999999999,1.0,0.0001944
mmlu-college-physics.val.94,gpt-4-1106-preview,0.0,0.00111,0.0,3.3e-05,1.0,8.88e-05,0.0,0.000888,1.0,0.000888,0.0,0.0001099999999999,0.0,0.00111,0.0,8.536000000000001e-05,0.0,9.9e-05,0.0,2.2e-05,0.0,6.6e-05,0.0,8.8e-05
grade-school-math.dev.2318,gpt-4-1106-preview,0.75,0.0089199999999999,0.75,0.0002007,0.75,0.0006968,0.75,0.005216,0.75,0.006536,0.75,0.000598,0.75,0.0089199999999999,0.5,0.00042292,0.75,0.0004419,0.75,0.0001146,0.75,0.0002916,0.75,0.0004304
mmlu-global-facts.val.99,gpt-4-1106-preview,1.0,0.00076,0.0,2.16e-05,1.0,5.84e-05,1.0,0.000584,1.0,0.000584,0.0,7.199999999999999e-05,1.0,0.00076,0.0,5.5872e-05,0.0,6.48e-05,0.0,1.44e-05,0.0,4.32e-05,1.0,5.68e-05
hellaswag.val.9608,claude-v1,0.0,0.002256,1.0,8.43e-05,0.0,0.000228,0.0,0.002256,0.0,0.002256,0.0,0.000283,1.0,0.00285,1.0,0.000218056,0.0,0.0002528999999999,1.0,5.62e-05,0.0,0.0001686,1.0,0.000224
hellaswag.val.886,mistralai/mistral-7b-chat,0.0,3.24e-05,0.0,4.86e-05,1.0,0.0001304,0.0,0.001304,0.0,0.001328,1.0,0.000162,1.0,0.00163,0.0,0.000125712,0.0,0.0001449,0.0,3.24e-05,0.0,9.72e-05,0.0,0.0001288
grade-school-math.dev.617,gpt-4-1106-preview,0.5,0.0113,0.5,0.0002013,0.75,0.0008032,0.25,0.005992,0.5,0.007504,0.5,0.000636,0.5,0.0113,0.25,0.000467152,0.25,0.0005364,0.25,0.000101,0.25,0.0003558,0.25,0.0004296
grade-school-math.dev.1245,meta/llama-2-70b-chat,0.75,0.0004095,0.25,0.0001886999999999,0.75,0.0005232,0.75,0.00528,0.75,0.006408,0.5,0.000501,0.5,0.00927,0.75,0.000326696,0.75,0.0004095,0.5,7.999999999999999e-05,0.25,0.0002909999999999,0.75,0.0003599999999999
grade-school-math.dev.136,meta/llama-2-70b-chat,0.25,0.0004185,0.25,0.0001539,0.75,0.0008623999999999,0.75,0.005288,0.75,0.0062,0.75,0.000817,0.5,0.01189,0.75,0.000486552,0.25,0.0004185,0.25,0.0001006,0.25,0.0003407999999999,0.25,0.0002423999999999
mmlu-college-mathematics.val.50,gpt-4-1106-preview,0.0,0.00089,0.0,2.64e-05,1.0,7.12e-05,0.0,0.000712,0.0,0.000712,1.0,8.8e-05,0.0,0.00089,0.0,6.828800000000001e-05,0.0,7.920000000000001e-05,0.0,1.7599999999999998e-05,0.0,5.28e-05,0.0,7.039999999999999e-05
arc-challenge.test.506,WizardLM/WizardLM-13B-V1.2,1.0,2.4e-05,1.0,2.4e-05,1.0,6.480000000000002e-05,1.0,0.000648,1.0,0.000648,1.0,7.999999999999999e-05,1.0,0.00084,1.0,6.208e-05,1.0,7.2e-05,1.0,1.6000000000000003e-05,1.0,4.8e-05,1.0,6.320000000000002e-05
arc-challenge.test.816,meta/llama-2-70b-chat,0.0,5.8500000000000006e-05,1.0,1.95e-05,0.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.7e-05,1.0,0.00066,0.0,5.044e-05,0.0,5.8500000000000006e-05,0.0,1.3e-05,1.0,3.9e-05,0.0,5.12e-05
mmlu-prehistory.val.156,meta/llama-2-70b-chat,0.0,9.27e-05,0.0,3.09e-05,1.0,8.320000000000002e-05,1.0,0.000832,1.0,0.000832,1.0,0.000103,1.0,0.00104,0.0,7.992800000000001e-05,0.0,9.27e-05,0.0,2.0600000000000003e-05,1.0,6.18e-05,1.0,8.240000000000001e-05
mmlu-nutrition.val.0,meta/llama-2-70b-chat,0.0,7.290000000000001e-05,0.0,2.43e-05,1.0,6.560000000000001e-05,1.0,0.000656,1.0,0.000656,1.0,8.099999999999999e-05,1.0,0.00082,0.0,6.285600000000001e-05,0.0,7.290000000000001e-05,0.0,1.62e-05,1.0,4.86e-05,1.0,6.48e-05
mmlu-high-school-european-history.val.163,claude-v1,1.0,0.002056,1.0,7.680000000000001e-05,1.0,0.0002056,1.0,0.002056,1.0,0.002056,1.0,0.000256,1.0,0.00257,0.0,0.000198656,0.0,0.0002304,0.0,5.12e-05,1.0,0.0001536,1.0,0.000204
grade-school-math.dev.3702,gpt-4-1106-preview,0.5,0.0102,0.25,0.0001869,0.75,0.0007704,0.5,0.006168,0.75,0.007824,0.75,0.000781,0.5,0.0102,0.25,0.000415936,0.25,0.0005004,0.25,0.0001055999999999,0.25,0.0003072,0.75,0.0004328
hellaswag.val.3781,claude-v1,0.0,0.002248,0.0,8.4e-05,0.0,0.0002248,0.0,0.002248,1.0,0.002248,0.0,0.0002819999999999,1.0,0.00284,0.0,0.00021728,0.0,0.0002511,0.0,5.6000000000000006e-05,1.0,0.000168,0.0,0.0002232
mtbench-reference.dev.2,meta/llama-2-70b-chat,0.8,0.0011772,0.3,0.0001587,0.9,0.0014688,0.2,0.0113759999999999,1.0,0.011904,1.0,0.000719,0.9,0.02193,0.1,0.000576568,0.8,0.0011772,0.2,5.46e-05,0.3,0.0003282,0.5,0.0006408
mmlu-high-school-macroeconomics.val.332,gpt-4-1106-preview,1.0,0.00117,0.0,3.48e-05,1.0,9.36e-05,1.0,0.000936,1.0,0.000936,1.0,0.000116,1.0,0.00117,0.0,9.0016e-05,0.0,0.0001044,0.0,2.32e-05,1.0,6.96e-05,1.0,9.2e-05
hellaswag.val.5627,claude-v1,1.0,0.002344,1.0,8.76e-05,1.0,0.0002344,1.0,0.002344,1.0,0.002344,0.0,0.000294,1.0,0.00293,1.0,0.000226592,1.0,0.0002619,1.0,5.84e-05,1.0,0.0001752,1.0,0.0002328
grade-school-math.dev.3848,gpt-4-1106-preview,0.75,0.01025,0.25,0.000135,0.25,0.000388,0.75,0.005104,0.75,0.0065439999999999,0.75,0.000452,0.75,0.01025,0.25,0.000317384,0.25,0.0004275,0.25,8.7e-05,0.75,0.0002333999999999,0.75,0.000524
hellaswag.val.5919,claude-v1,1.0,0.001968,0.0,7.35e-05,1.0,0.0001968,1.0,0.001968,1.0,0.001968,0.0,0.000247,1.0,0.00249,0.0,0.00019012,0.0,0.0002205,0.0,4.9000000000000005e-05,0.0,0.000147,0.0,0.0001952
grade-school-math.dev.514,meta/llama-2-70b-chat,0.5,0.0003357,0.5,0.0001347,0.75,0.000504,0.75,0.004392,0.75,0.0045839999999999,0.5,0.000295,0.5,0.00573,0.75,0.000273152,0.5,0.0003357,0.25,5.74e-05,0.75,0.0002196,0.25,0.000228
mmlu-moral-disputes.val.209,WizardLM/WizardLM-13B-V1.2,0.0,3.39e-05,0.0,3.39e-05,1.0,9.120000000000002e-05,1.0,0.000912,1.0,0.000912,1.0,0.000113,1.0,0.00114,0.0,8.768799999999999e-05,0.0,0.0001017,1.0,2.2600000000000004e-05,1.0,6.78e-05,1.0,8.960000000000002e-05
hellaswag.val.7791,claude-v1,0.0,0.001864,0.0,6.96e-05,1.0,0.0001888,0.0,0.001864,1.0,0.001864,0.0,0.000234,1.0,0.00236,0.0,0.000180032,0.0,0.0002087999999999,0.0,4.64e-05,1.0,0.0001392,0.0,0.0001848
mmlu-human-sexuality.val.26,WizardLM/WizardLM-13B-V1.2,1.0,2.94e-05,1.0,2.94e-05,1.0,7.920000000000001e-05,1.0,0.000792,1.0,0.000792,1.0,0.0001,1.0,0.00102,1.0,7.604800000000001e-05,0.0,8.82e-05,1.0,1.96e-05,1.0,5.88e-05,1.0,7.840000000000001e-05
mmlu-nutrition.val.22,WizardLM/WizardLM-13B-V1.2,1.0,2.9100000000000003e-05,1.0,2.9100000000000003e-05,1.0,7.840000000000001e-05,0.0,0.000784,0.0,0.000784,1.0,9.7e-05,1.0,0.00098,0.0,7.5272e-05,0.0,8.730000000000001e-05,1.0,1.94e-05,0.0,5.8200000000000005e-05,1.0,7.76e-05
hellaswag.val.4039,claude-v1,0.0,0.00212,0.0,7.92e-05,1.0,0.000212,0.0,0.00212,1.0,0.002144,1.0,0.000264,1.0,0.00265,0.0,0.000204864,0.0,0.0002376,0.0,5.280000000000001e-05,0.0,0.0001584,0.0,0.0002104
arc-challenge.test.709,claude-v1,1.0,0.000528,1.0,1.95e-05,1.0,5.28e-05,1.0,0.000528,1.0,0.000528,1.0,6.7e-05,1.0,0.00066,0.0,5.044e-05,1.0,5.8500000000000006e-05,0.0,1.3e-05,1.0,3.9e-05,1.0,5.12e-05
hellaswag.val.2905,mistralai/mistral-7b-chat,1.0,2e-05,0.0,3e-05,0.0,8.080000000000001e-05,0.0,0.000808,0.0,0.000808,0.0,0.000102,1.0,0.00104,1.0,7.76e-05,0.0,8.91e-05,1.0,2e-05,1.0,6e-05,1.0,7.920000000000001e-05
mmlu-moral-scenarios.val.328,WizardLM/WizardLM-13B-V1.2,0.0,4.2e-05,0.0,4.2e-05,0.0,0.0001128,0.0,0.0011279999999999,0.0,0.0011279999999999,0.0,0.00014,1.0,0.0014399999999999,0.0,0.00010864,0.0,0.000126,0.0,2.8e-05,0.0,8.4e-05,1.0,0.0001112
hellaswag.val.8690,meta/code-llama-instruct-34b-chat,0.0,0.0002134,0.0,8.249999999999999e-05,1.0,0.0002232,1.0,0.002208,1.0,0.002208,1.0,0.000277,1.0,0.00276,0.0,0.0002134,1.0,0.0002475,0.0,5.5e-05,0.0,0.0001649999999999,1.0,0.0002192
mmlu-jurisprudence.val.44,meta/llama-2-70b-chat,0.0,9.45e-05,1.0,3.15e-05,1.0,8.480000000000001e-05,1.0,0.000848,1.0,0.000848,1.0,0.0001049999999999,1.0,0.00106,0.0,8.148e-05,0.0,9.45e-05,0.0,2.1e-05,1.0,6.3e-05,1.0,8.320000000000002e-05
hellaswag.val.7593,claude-v1,1.0,0.002336,1.0,8.7e-05,0.0,0.000236,1.0,0.002336,0.0,0.0023599999999999,0.0,0.0002929999999999,1.0,0.00292,0.0,0.000225816,1.0,0.0002619,0.0,5.8200000000000005e-05,1.0,0.0001746,1.0,0.000232
mmlu-professional-law.val.119,claude-v1,1.0,0.0012079999999999,0.0,4.5e-05,1.0,0.0001208,1.0,0.0012079999999999,1.0,0.0012079999999999,1.0,0.00015,0.0,0.00154,0.0,0.0001164,0.0,0.000135,0.0,3e-05,0.0,9e-05,0.0,0.0001192
bias_detection.dev.77,meta/llama-2-70b-chat,0.0,0.0003132,0.0,9.33e-05,1.0,0.0002424,0.0,0.004032,1.0,0.004584,1.0,0.00031,1.0,0.00825,1.0,0.000233576,0.0,0.0003132,0.0,4.86e-05,1.0,0.0001776,0.0,0.0002672
arc-challenge.test.635,WizardLM/WizardLM-13B-V1.2,1.0,2.7e-05,1.0,2.7e-05,1.0,7.280000000000001e-05,1.0,0.000728,1.0,0.000728,1.0,9.2e-05,1.0,0.00094,0.0,6.984e-05,1.0,8.1e-05,0.0,1.8e-05,0.0,5.4e-05,1.0,7.120000000000001e-05
mmlu-professional-law.val.210,gpt-4-1106-preview,1.0,0.00257,0.0,7.680000000000001e-05,1.0,0.0002056,1.0,0.002056,1.0,0.002056,0.0,0.000256,1.0,0.00257,0.0,0.000198656,0.0,0.0002304,0.0,5.12e-05,1.0,0.0001536,1.0,0.000204
hellaswag.val.3963,meta/llama-2-70b-chat,1.0,0.0001971,0.0,6.599999999999999e-05,1.0,0.0001768,0.0,0.001768,1.0,0.001768,0.0,0.000222,1.0,0.00221,0.0,0.00017072,1.0,0.0001971,0.0,4.4000000000000006e-05,1.0,0.0001319999999999,1.0,0.0001752
hellaswag.val.2713,claude-v1,1.0,0.000768,0.0,2.85e-05,0.0,7.680000000000001e-05,1.0,0.000768,1.0,0.000768,1.0,9.5e-05,1.0,0.00096,1.0,7.372e-05,1.0,8.55e-05,1.0,1.9e-05,1.0,5.7e-05,0.0,7.520000000000001e-05
mmlu-high-school-macroeconomics.val.35,meta/code-llama-instruct-34b-chat,0.0,5.9752000000000007e-05,1.0,2.31e-05,1.0,6.24e-05,1.0,0.000624,1.0,0.000624,1.0,7.699999999999999e-05,1.0,0.00078,0.0,5.9752000000000007e-05,0.0,6.93e-05,1.0,1.54e-05,1.0,4.6200000000000005e-05,1.0,6.08e-05
mbpp.dev.298,gpt-4-1106-preview,0.0,0.01249,0.0,8.340000000000001e-05,0.0,0.0005408,0.0,0.00728,0.0,0.006512,0.0,0.000491,0.0,0.01249,1.0,0.00017072,0.0,0.0002655,0.0,5.86e-05,0.0,0.0001656,0.0,0.0003168
grade-school-math.dev.3536,gpt-4-1106-preview,0.5,0.00755,0.5,0.0001491,0.75,0.000592,0.75,0.004768,0.75,0.005584,0.5,0.0004669999999999,0.5,0.00755,0.25,0.000315832,0.75,0.0004437,0.25,8.26e-05,0.75,0.000261,0.5,0.0003656
mmlu-college-medicine.val.168,gpt-4-1106-preview,1.0,0.0009699999999999,1.0,2.88e-05,1.0,7.76e-05,1.0,0.000776,1.0,0.000776,1.0,9.6e-05,1.0,0.0009699999999999,0.0,7.4496e-05,0.0,8.640000000000001e-05,0.0,1.92e-05,1.0,5.76e-05,1.0,7.6e-05
mmlu-professional-law.val.638,claude-v1,1.0,0.002152,0.0,8.04e-05,1.0,0.0002152,1.0,0.002152,0.0,0.002152,0.0,0.000268,1.0,0.00269,0.0,0.0002079679999999,0.0,0.0002412,0.0,5.360000000000001e-05,1.0,0.0001602,1.0,0.0002136
hellaswag.val.5922,claude-v1,0.0,0.001896,0.0,7.08e-05,0.0,0.0001896,0.0,0.001896,1.0,0.001896,1.0,0.0002379999999999,1.0,0.0024,0.0,0.000183136,1.0,0.0002123999999999,0.0,4.720000000000001e-05,0.0,0.0001416,1.0,0.000188
mmlu-nutrition.val.220,meta/llama-2-70b-chat,0.0,8.01e-05,1.0,2.67e-05,1.0,7.200000000000002e-05,1.0,0.00072,1.0,0.00072,1.0,8.9e-05,1.0,0.0009,0.0,6.9064e-05,0.0,8.01e-05,0.0,1.7800000000000002e-05,1.0,5.34e-05,0.0,7.120000000000001e-05
mmlu-human-sexuality.val.33,gpt-4-1106-preview,1.0,0.0006799999999999,1.0,2.01e-05,1.0,5.44e-05,1.0,0.000544,0.0,0.000544,1.0,6.699999999999999e-05,1.0,0.0006799999999999,0.0,5.1992000000000006e-05,0.0,6.03e-05,1.0,1.34e-05,1.0,4.02e-05,1.0,5.28e-05
winogrande.dev.129,claude-v1,0.0,0.000432,1.0,1.59e-05,1.0,4.32e-05,0.0,0.000432,1.0,0.000432,0.0,5.3e-05,0.0,0.00057,0.0,4.1128e-05,1.0,4.77e-05,0.0,1.06e-05,0.0,3.18e-05,0.0,4.16e-05
mmlu-prehistory.val.122,WizardLM/WizardLM-13B-V1.2,0.0,2.52e-05,0.0,2.52e-05,1.0,6.800000000000001e-05,1.0,0.00068,1.0,0.00068,1.0,8.4e-05,1.0,0.00085,0.0,6.5184e-05,0.0,7.56e-05,1.0,1.6800000000000002e-05,1.0,5.04e-05,1.0,6.720000000000001e-05
mmlu-high-school-physics.val.103,meta/llama-2-70b-chat,0.0,0.000108,0.0,3.6e-05,1.0,9.68e-05,1.0,0.000968,1.0,0.000968,1.0,0.000122,1.0,0.00121,0.0,9.312e-05,0.0,0.000108,0.0,2.4e-05,1.0,7.2e-05,1.0,9.6e-05
mmlu-nutrition.val.156,WizardLM/WizardLM-13B-V1.2,0.0,4.14e-05,0.0,4.14e-05,1.0,0.0001112,1.0,0.001112,1.0,0.001112,1.0,0.000138,1.0,0.00139,0.0,0.000107088,0.0,0.0001241999999999,0.0,2.7600000000000003e-05,1.0,8.28e-05,1.0,0.0001104
mmlu-moral-scenarios.val.285,claude-v1,0.0,0.001112,0.0,4.14e-05,0.0,0.0001112,0.0,0.001112,0.0,0.001112,1.0,0.00014,0.0,0.00142,0.0,0.000107088,0.0,0.0001241999999999,0.0,2.7600000000000003e-05,0.0,8.28e-05,1.0,0.0001096
winogrande.dev.361,mistralai/mistral-7b-chat,0.0,9.8e-06,0.0,1.4699999999999998e-05,1.0,4e-05,1.0,0.0003999999999999,1.0,0.0003999999999999,1.0,5.1e-05,1.0,0.0005,0.0,3.8024e-05,1.0,4.410000000000001e-05,0.0,9.8e-06,1.0,2.94e-05,1.0,3.92e-05
mmlu-high-school-mathematics.val.229,claude-v1,0.0,0.000984,0.0,3.66e-05,0.0,9.84e-05,0.0,0.000984,0.0,0.000984,0.0,0.000124,0.0,0.00123,0.0,9.4672e-05,0.0,0.0001098,0.0,2.44e-05,0.0,7.32e-05,0.0,9.68e-05
grade-school-math.dev.7065,meta/llama-2-70b-chat,0.25,0.0004112999999999,0.5,0.0001806,0.5,0.000568,0.5,0.004816,0.5,0.007432,0.5,0.000577,0.5,0.0095599999999999,0.5,0.000392656,0.25,0.0004112999999999,0.5,9.880000000000002e-05,0.5,0.0003096,0.5,0.0003856
mmlu-elementary-mathematics.val.218,meta/llama-2-70b-chat,0.0,6.66e-05,0.0,2.22e-05,1.0,6e-05,1.0,0.0006,1.0,0.0006,1.0,7.4e-05,1.0,0.00075,0.0,5.7424e-05,0.0,6.66e-05,0.0,1.48e-05,1.0,4.44e-05,1.0,5.84e-05
winogrande.dev.1012,claude-v1,0.0,0.000424,1.0,1.56e-05,1.0,4.24e-05,0.0,0.000424,1.0,0.000424,1.0,5.4000000000000005e-05,0.0,0.00056,1.0,4.0352e-05,0.0,4.59e-05,1.0,1.04e-05,1.0,3.12e-05,1.0,4.08e-05
hellaswag.val.8217,claude-v1,1.0,0.002048,0.0,7.649999999999999e-05,0.0,0.0002072,1.0,0.002048,1.0,0.002048,1.0,0.000257,1.0,0.00256,0.0,0.00019788,1.0,0.0002286,0.0,5.1000000000000006e-05,0.0,0.0001529999999999,1.0,0.0002032
mmlu-professional-law.val.209,gpt-4-1106-preview,0.0,0.00235,1.0,6.93e-05,0.0,0.0001856,0.0,0.001856,0.0,0.001856,0.0,0.000231,0.0,0.00235,0.0,0.000179256,0.0,0.0002078999999999,0.0,4.6200000000000005e-05,0.0,0.0001386,0.0,0.0001848
mmlu-high-school-mathematics.val.142,WizardLM/WizardLM-13B-V1.2,1.0,6.3e-05,1.0,6.3e-05,0.0,0.0001688,1.0,0.0016879999999999,0.0,0.0016879999999999,1.0,0.0002119999999999,1.0,0.00211,0.0,0.00016296,0.0,0.000189,1.0,4.2e-05,0.0,0.000126,1.0,0.000168
grade-school-math.dev.180,gpt-4-1106-preview,0.5,0.00621,0.5,0.0001557,0.75,0.000528,0.75,0.0042,0.75,0.005064,0.75,0.000445,0.5,0.00621,0.75,0.000256856,0.25,0.0003339,0.75,7.24e-05,0.75,0.0002304,0.75,0.0003184
mmlu-logical-fallacies.val.100,meta/llama-2-70b-chat,0.0,0.0001286999999999,0.0,4.29e-05,0.0,0.0001152,0.0,0.001152,0.0,0.001152,0.0,0.000145,1.0,0.00147,0.0,0.000110968,0.0,0.0001286999999999,1.0,2.8600000000000004e-05,0.0,8.58e-05,1.0,0.0001144
hellaswag.val.178,meta/llama-2-70b-chat,1.0,0.0001152,1.0,3.84e-05,1.0,0.0001032,1.0,0.001032,1.0,0.001032,0.0,0.000128,0.0,0.00129,0.0,9.9328e-05,1.0,0.0001152,0.0,2.56e-05,0.0,7.68e-05,0.0,0.0001016
